<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>Adaptively Secure Multiparty Computation - Ran Canetti, Uri Feige, Oded Goldreich, Moni Naor, 1996</title>
        <link rel="stylesheet" type="text/css" href="../../style/main.css">
        <link rel="stylesheet" type="text/css" href="../../style/equation.css">
        <link rel="stylesheet" type="text/css" href="../../style/ref.css">
        <link rel="stylesheet" type="text/css" href="../../style/glossary.css">
        <link rel="stylesheet" type="text/css" href="../../style/researchPaper.css">
        <link rel="icon" href="../../img/favicon.ico" type="image/x-icon">
		<script type="text/javascript" src="../../script/blockShare.js"></script>
        <script type="text/javascript" src="../../script/equation.js"></script>
        <script type="text/javascript" src="../../script/ref.js"></script>
        <script type="text/javascript" src="../../script/fnote.js"></script>
        <script type="text/javascript" src="../../script/glossary.js"></script>
        <script type="text/javascript" src="../../script/def.js"></script>
		<script type="text/javascript" src="../../script/toc.js"></script>
        <script type="text/javascript"
                src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        <script type="text/javascript">
			<!--
            function fill(box)
			{
				switch (box)
				{
					default:
						return "No info on this equation yet.";
				}
			}

			function authorLink(ref)
			{
				switch (ref)
				{
					default:
						return "#";
				}
			}

			self_def["non-general word"] = "definition";
			//-->
        </script>
    </head>
    <body>
        <div class="main_foreground">
            <div class="main_toplevel main_header">
                <h1>Multiparty Computation</h1>
            </div>
            <div class="main_toplevel main_navigation">
                <a href="../../index.html"><div class="main_navbox"><h2>home</h2></div></a>
                <a href="../../learn.html"><div class="main_navbox"><h2>learn</h2></div></a>
                <a href="../../research.html"><div class="main_navbox"><h2>research</h2></div></a>
                <a href="../../nextsteps.html"><div class="main_navbox"><h2>build</h2></div></a>
                <a href="../../resources.html"><div class="main_navbox"><h2>resources</h2></div></a>
                <a href="../../aboutus.html"><div class="main_navbox"><h2>about us</h2></div></a>
            </div>
            <div class="main_toplevel main_section main_color1" id="overview">
                <div class="main_section_nav_container">
                    <div class="main_section_nav_box"><a href="../timeline.html">Timeline</a></div>
                    <div class="main_section_nav_box"><a href="../title.html">By Title</a></div>
                    <div class="main_section_nav_box"><a href="../authors.html">By Author</a></div>
                    <div class="main_section_nav_box"><a href="../tag.html">By Category</a></div>
                </div>
                <div class="main_window main_fullwidth">
                    <div class="rp_linkbox"><a href="pdf/5.pdf"><img src="../../img/PDF.png" class="rp_link" alt="view pdf" /></a></div>
					
                    <span class="rp_title">Adaptively Secure Multiparty Computation</span>
                    <span class="rp_info">1996
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a class="rp_author" href="../authors/Ran Canetti.html">Ran Canetti</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a class="rp_author" href="../authors/Uri Feige.html">Uri Feige</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a class="rp_author" href="../authors/Oded Goldreich.html">Oded Goldreich</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a class="rp_author" href="../authors/Moni Naor.html">Moni Naor</a>
					</span>
					
					<div class="rp_snippet">
						&ldquo;&rdquo;
					</div>
					
					THIS PAGE IS UNDER CONSTRUCTION
					<h1>Overview</h1>
                    <div class="main_toc"></div>
						
                    <section id="intro">
						<h2>Introduction</h2>
						<p class="rp_analysis">
							<b>Paper Title</b> is...
						</p>
					</section>
					<section id="goals">
						<h2>Goals and Results</h2>
						<p class="rp_analysis">
							Some goals that they had.
						</p>
						<p class="rp_analysis">
							Don't forget some results, too!
						</p>
					</section>
					<section id="assumptions">
						<h2>Assumptions</h2>
					</section>
					<section id="defs">
						<h2>Definitions</h2>
						<a href="#def1"><span class="rp_definition_header">Definition 1 - </span></a>
						<p class="rp_original rp_definition">
							A definition 
						</p>
					</section>
					<section id="theorems">
						<h2>Theorems</h2>
						<p class="rp_original">Some general definitions used by several theorems</p>
						<a href="#theorem1"><span class="rp_theorem_header">Theorem 1</span></a>
						<p class="rp_original rp_theorem">
							A theorem
						</p>
					</section>
					<section id="protocol">
						<h2>Protocols</h2>
						<h3><a href="#secxx">Some Protocol Defined</a></h3>
						<ul class="rp_analysis">
							<li><b>Number of parties: </b></li>
							<li><b>Function(s): </b></li>
							<li><b>Privacy constraints: </b></li>
							<li><b>Cheating: </b></li>
							<li><b>Bits exchanged: </b></li>
							<li><b>Subprotocols: </b></li>
							<li><b>Runtime: </b></li>
							<li><b>Assumptions: </b></li>
							<li><b>Implementations: </b></li>
							<li><b>Notes: </b></li>
						</ul>
					</section>
					<section id="further">
						<h2>Further Reading</h2>
					</section>
					<section id="ref">
						<h2>Referencing This Paper</h2>
						<p class="rp_analysis">To cite this paper, simply copy and paste the below into your citation:</p>
						<p class="rp_self_reference">
							R. Canetti, U. Feige, O. Goldreich, and M. Naor. Adaptive Secure Multiparty Computation. <i>Proceedings of the Twenty-Eighth ACM Symposium on the Theory of Computing</i>, pages 639-648. ACM. 1996.
						</p>
					</section>
                </div>
            </div>
            <div class="main_toplevel main_section main_color5" id="annotated_paper">
                <div class="main_window main_fullwidth">
					<h1>Annotated Paper</h1>
					<div class="main_toc"></div>
					<section id="abstract" data-section-number="0">
						<h2>Abstract</h2>
						<p class="rp_original">
							A fundamental problem in designing secure multiparty protocols is how to deal with adaptive adversaries (i.e. adversaries that may choose the corrupted parties during the course of the computation), in a setting where the channels are insecure and secure communication is achieved by cryptographic primitives based on the computational limitations of the adversary. 
						</p>
						<p class="rp_original">
							It turns out that the power of an adaptive adversary is greatly affected by the amount of information gathered upon the corruption of a party. This amount of information models the extent to which uncorrupted parties are trusted to carry out instructions that cannot be externally verified, such as erasing records of past configurations. It has been shown that if the parties are trusted to erase such records, then adaptively secure computation can be carried out using known primitives. However, this total trust in parties may be unrealistic in many scenarios. An important question, open since 1986, is whether adaptively secure multiparty computation can be carried out in the 'insecure channel' setting, even if no party is thoroughly trusted.
						</p>
						<p class="rp_original">
							Our main result is an affirmative resolution of this question for the case where even uncorrupted parties may deviate from the protocol by keeping record of all past configurations. We first propose a novel property of encryption protocols and show that if an encryption protocol enjoying this property is used, instead of a standard encryption scheme, then known constructions become adaptively secure. Next we construct, based on the standard RSA assumption, an encryption protocol that enjoys this property. 
						</p>
						<p class="rp_original">
							We also consider parties that, even when uncorrupted, may internally deviate from their protocols in arbitrary ways, as long as no external test can detect fault behavior. We show that in this case no non-trivial protocol can be proven adaptively secure using black-box simulation. This holds even if the communication channels are totally secure.
						</p>
					</section>
					<section id="sec1">
						<h2>1. Introduction</h2>
						<p class="rp_original">
							Consider a set of parties who do not trust each other, nor the channels by which they communicate. Still, the parties wish to correctly compute some common function of their local inputs, while keeping their local data as private as possible. This, in a nutshell, is the problem of secure multiparty computation. The parties' distrust in each other and in the network is usually modeled via an adversary that corrupts some of the parties. Once a party is corrupted it follows the instructions of the adversary. In particular, all the information known to this party becomes known to the adversary.
						</p>
						<p class="rp_original">
							An important parameter, which is the focus of this work, is the way in which the corrupted parties are chosen. In the case of <i>non-adaptive</i> adversaries, the set of corrupted parties is arbitrary, but fixed before the computation starts. (Still, the uncorrupted parties do not know the identities of the corrupted parties.) A more general case is where the adversary chooses to corrupt parties during the course of the computation, based on the information gathered so far. We call such adversaries <i>adaptive</i>.
						</p>
						<p class="rp_original">
							The difference between adaptive and non-adaptive adversaries may be best demonstrated via an example. Consider the following secret sharing protocol, run in the presence of an adversary that may corrupt \(t=O(n)\) out of the \(n\) parties. <i>A dealer \(D\) chooses at random a small set \(S\) of \(m=\sqrt t\) parties, and shares its secret among the parties using an \(m\)-out-of-\(m\) sharing scheme. In addition, \(D\) publicizes the set \(S\).</i> Intuitively, this scheme lacks in security since \(S\) is public and \(|S|\ll t\). Indeed, an adaptive adversary can easily find \(D\)'s secret, <i>without corrupting \(D\)</i>, by corrupting the parties in \(S\). However, any non-adaptive adversary that does not corrupt \(D\) learns \(D\)'s secret only if \(S\) happens to be identical to the pre-defined set of corrupted parties. This happens only with exponentially small probability. Consequently, this protocol is secure in the presence of non-adaptive adversaries.
						</p>
						<p class="rp_original">
							Protocols for securely computing any function, in several computation models, have been known for a while: Goldreich, Micali, and Wigderson have shown how to securely compute any function in the <i>computational</i> setting<sup class="reference" data-citation="GMW">[?]</sup>. (In the <i>computational</i> setting, all the communication between the parties is seen by the adversary. All parties, as well as the adversary, are restricted to probabilistic polynomial time). Ben-Or, Goldwasser, and Wigderson, and independently Chuam, Cr&eacute;peau, and Damg&aring;rd, have shown how to securely compute any function in the <i>secure channels</i> setting<sup class="reference" data-citation="BGW">[?]</sup><sup class="reference" data-citation="CCD">[?]</sup>. (In the <i>secure channels</i> setting the adversary cannot eavesdrop on the communication between uncorrupted parties, and is allowd unlimited computational power.) These constructions can be shown secure in the presence of non-adaptive adversaries. In contrary to folklore beliefs, problems are encountered when attempted to prove <i>adaptive</i> security of protocols, <i>even in the secure channels setting</i>. Additional problems are encountered in the computational setting. Demonstrating, clarifying, and (partially) solving these problems is the focus of this work.
						</p>
						<p class="rp_original">
							We first pose the following question: To what extent can uncorrupted parties be trusted to carry out instructions that cannot be externally verified, such as erasing local data, or making random choices? This question is intimately related to the power of an adaptive adversary, in both of the above settings, since the adversary may gather additional information when corrupting parties that have locally deviated from the protocol (say, by not erasing data that is supposed to be erased). If uncorrupted parties are trusted to carry out even unverifiable instructions such as erasing local data, then adaptively secure computation can be carried out using known primitives<sup class="reference" data-citation="F">[?]</sup><sup class="reference" data-citation="BH">[?]</sup>. However, this trust may be unrealistic in many scenarios. We thus consider parties that, even wen uncorrupted, internally deviate slightly from their protocols. We call such parties <i>semi-honest</i>. Several degrees of internal deviation from the protocol are examined with the main focus on parties which follow their protocol with the exception that they keep record of the entire computation. We seek protocols that are secure even in the uncorrupted parties are semi-honest rather than honest.
						</p>
						<p class="rp_original">
							We discuss the problems encountered in the secure channels setting, and state the amount of internal deviation from the protocol under which adaptively secure protocols are known to exist. (In particular, under the conditions the <span class="reference" data-citation="BGW">[?]</span><sup class="reference" data-citation="CCD">[?]</sup> protocols can be proven adaptively secure.)
						</p>
						<p class="rp_original">
							Finally we concentrate on the computational setting, and on semi-honest parties that follow their protocols with the exception that no internal data is ever erased. Is adaptively secure computation possible in this scenario? This question has remained open since the result of <span class="reference" data-citation="GMW">[?]</span> (Even for the case in which the adversary only gather information from corrupted parties and does not make them deviate any further from the protocol).
						</p>
						<p class="rp_original">
							We answer this question in the affirmative. The problems encountered, and our solution, are presented via the following transformation. It is a folklore belief that any secure protocol in the secure channels setting can be transformed into a secure protocol in the computational setting, by encrypting each message using a standard semantically secure encryption scheme. This belief can indeed be turned into a proof, provided that only <i>non-adaptive</i> adversaries are considered. Trying to prove this belief in the presence of adaptive adversaries encounters major difficulties. We show how these difficulties are overcome if a novel encryption protocol is used, instead of standard encryption. We call such encryption protocols <i>non-committing</i> (Standard encryption schemes are not non-committing.)
						</p>
						<p class="rp_original">
							Non-committing encryption can be roughly described as follows. Traditional encryption schemes have the extra property that the ciphertext may serve as a <i>commitment</i> of the sender to the encrypted data. That is, suppose that after seeing the ciphertext, a third party requests the sender to <i>reveal</i> the encrypted data, and show how it was encrypted and decrypted. Using traditional encryption schemes it may be infeasible (or even impossible) for the sender to demonstrate that the encrypted data was any different than what was indeed transmitted. (In fact, many times encryption is explicitly or implicitly used for commitment.) In a <i>non-committing</i> encryption scheme, the ciphertext cannot be used to commit the sender (or the receiver) to the transmitted data. That is, a non-committing encryption protocol allows a simulator to generate <i>dummy ciphertexts</i> that look like genuine ones, and can be later "opened" as encryptions of either 1 or 0, at wish. We note that communication over absolutely secure channels is trivially non-committing, since the third party sees no "ciphertext".
						</p>
						<p class="rp_original">
							We present several constructions of non-committing encryption protocols. All constructions consist of a 'key distribution' stage which is independent of the transmitted data, followed by a single message sent from the sender to the receiver. In our most general construction, based on a primitive called common-domain trapdoor system, the key distribution stage requires participation of all parties (and is valid as long as at least <i>one</i> party remains uncorrupted). We also present two alternative constructions, based on the RSA and the Diffie-Hellman assumptions respectively, where the key distribution stage consists of one message sent fro mthe receiver to the sender.
						</p>
						<section id="sec1.1">
							<h3>1.1. Related Work</h3>
							<p class="rp_original">
								Independently of our work, Beaver has investigated the problem of converting, in the computational setting, protocols which are adaptively secure against eavesdropping adversaries into protocols adaptively secure against Byzantine adversaries<sup class="reference" data-citation="Be2">[?]</sup>. No protocols adaptively secure against eavesdropping adversaries were known prior to our work, nor are such protocols suggested in <span class="reference" data-citation="Be2">[?]</span>. We believe that the problem of adaptive security retains its difficulty even if only eavesdropping adversaries are considered. Following our work, and motivated by the "Incoercible Voting" Problem, Canetti et. al.<sup class="reference" data-citation="CDNO">[?]</sup> introduced a stronger type of non-committing encryption protocol as well as an implementation of it based on any trapdoor permutation.
							</p>
						</section>
						<section id="sec1.2">
							<h3>1.2. Organization</h3>
							<p class="rp_original">
								The rest of this paper is organized as follows. In <a href="#sec2">Section 2</a> we discuss the problem of adaptive security and our solution to it in more detail. We keep the presentation informal throughout this section. Precise definitions are given in <a href="#sec3">Section 3</a>. Our constructions for the non-erasing and honest-looking cases are presented in Sections <a href="#sec4">4</a> and <a href="#sec5">5</a>, respectively.
							</p>
						</section>
					</section>
					<section id="sec2">
						<h2>2. Semi-honesty and Adaptive Security</h2>
						<p class="rp_original">
							In this section we discuss the problem of adaptive security and our solution to it in more detail. We keep the presentation informal throughout this section. Precise definitions are given in <a href="#sec3">Section 3</a>. In <a href="#sec2.1">Subsection 2.1</a> we discuss the question of what can be expected from an honest party, and present several notions of semi-honest parties. In <a href="#sec2.2">Subsection 2.2</a> we describe the problems encountered when trying to prove adaptive security of protocols <i>in the secure channels setting</i>, and state existing solutions. In <a href="#sec2.3">Subsection 2.3</a> we present the additional problems encountered when trying to prove adaptive security of protocols <i>in the computational setting</i>, and sketch our solution.
						</p>
						<section id="sec2.1">
							<h3>2.1. Semi-honest Parties</h3>
							<p class="rp_original">
								The problem of adaptively secure computation is intimately related to the following question: To what extend can uncorrupted parties be trusted to carry out instructions that cannot be externally verified, such as erasing local data, or using randomness as instructed? Honest parties internally deviate from their protocol in many real-life scenarios, such as users that keep record of their passwords, stock-market brokers that keep records of their clients' orders, <i>operating systems</i> that "free" old memory instead of erasing it or take periodic snapshots of the memory (for error recovery purposes), and computers that use pseudorandom generators as their source of randomness instead of truly random bits. Consider for example a protocol in which party \(A\) is instructed to choose a random string \(r\) for party \(B\), hand \(r\) to \(B\), and then <i>erase \(r\)</i> from its own memory. Can \(B\) be certain that \(A\) no longer knows \(r\)? Furthermore, can \(A\) now convince a third party (or an adversary that later decides to corrupt \(A\)) that he no longer knows \(r\)?
							</p>
							<p class="rp_original">
								To address this issue we introduce the motion of a <i>semi-honest</i> party. Such a party "appears as honest" (i.e. seems to be following its protocol) from the point of view of an outside observer; however, internally, it may somewhat deviate from the protocol. For instance, a semi-honest party may fail to erase some internal data, or use randomness not as instructed. (However, semi-honest parties do <i>not</i> collaborate.) We wish to have protocols that are secure even when parties are not thoroughly trusted, or in other words when the uncorrupted parties are semi-honest rather than honest. We say that a protocol \(\pi'\) is a <i>semi-honest protocol</i> for a protocol \(\pi\) if a party running \(\pi'\) "appears as" an honest party running \(\pi\). We want the requirements from \(\pi\) to be satisfied <i>even if the uncorrupted parties are running any semi-honest protocol for \(\pi\)</i>. (In the sequel we use the terms 'semi-honest parties' and 'semi-honest protocols' interchangeably.)
							</p>
							<p class="rp_original">
								The difference between computations in the presence of totally honest parties and computations in the presence of semi-honest parties becomes evident in the presence of adaptive adversaries. Consider a party just corrupted by the adversary, during the course of the computation. If the party is totally honest, then the adversary will see exactly the data specified in the protocol (in particular, any data that was supposed to be erased will not be seen). If the party is semi-honest, then the adversary may see a great deal of other data, such as all the past random choices of the party and all the messages the party ever received and sent. Therefore, the adversary may be much more powerful in the presence of semi-honest parties. We elaborate on this crucial point in the sequel.
							</p>
							<p class="rp_original">
								We distinguish three types of semi-honest behavior. The slightest deviation from the protocol is considered to be refraining from erasing data. We call such parties <i>honest-but-non-erasing</i>, or in short <i>non-erasing</i>. Non-erasing behavior is a very simple deviation from the protocol, that is very hard to prevent. Even if the protocol is somehow protected against modifications, it is always possible to add an <i>external</i> device that copies all memory locations accessed by the protocol to a "safe" memory". This way a record of the entire execution is kept. Such an external device requires no understanding of the internal structure or of the behavior of the protocol. Furthermore, failure to erase data may occur even without intention of the honest party (e.g. the operating system examples above).
							</p>
							<p class="rp_original">
								A more severe deviation by a semi-honest party consists of executing some arbitrary protocol other than the specified one, with the restriction that no external test can distinguish between such a behavior and a truly honest behavior. We call parties that deviate in this way <i>honest-looking</i>. Honest-looking parties represent "sophisticated" parties that internally deviate from the protocol in an arbitrary way, but are not willing to take any chance that they will <i>ever</i> be uncovered (say, by an unexpected audit). Note that honest-looking parties can do other "harmful" things, on top of not erasing data. For instance, assume that some one-way permutation \(f\,:\,D\stackrel{1-1}{\mapsto}D\) is known to all parties. When instructed to choose a value \(r\) uniformly in \(D\), an honest-looking party can instead choose \(s\) uniformly in \(D\) and let \(r=f(s)\). Thus the party cannot be trusted to <i>not</i> known \(f^{-1}(r)\). (Other, more 'disturbing deviations from the protocol are possible, we elaborate in the sequel.)
							</p>
							<p class="rp_original">
								An even more permissive approach allows a semi-honest party to deviate arbitrarily from the protocol, as long as its behavior appears honest to all other parties <i>executing the protocol</i>. Other external tests, not specified in the protocol, may be able to detect such a party as cheating. We call such semi-honest parties <i>weakly-honest</i>.
							</p>
							<p class="rp_original">
								The focus of our work is mainly on adaptive security in the presence of non-erasing parties (see <a href="#sec4">Section 4</a>). This coincides with the common interpretation of the problem of adaptive security. To the best of our knowledge, honest-looking and weakly-honest parties were not considered before.
							</p>
						</section>
						<section id="sec2.2">
							<h3>2.2. Adaptive Security in the Secure Channels Setting</h3>
							<p class="rp_original">
								Although the emphasis of this paper is on the computational setting, we first present the state of knowledge, and sketch the problems involved, in the secure channels setting. We believe that understanding adaptively secure computation in the computational setting is easier when the secure channels setting is considered first.
							</p>
							<p class="rp_original">
								The state-of-the-art with respect to adaptive computation in the secure channels setting can be briefly summarized as follows. Adaptively secure protocols for computing any function exist in the presence of non-erasing parties (e.g. <span class="reference" data-citation="BGW">[?]</span><span class="reference" data-citation="CCD">[?]</span>). However, in contrast with popular belief, not every <i>non-adaptively</i> secure protocol is also <i>adaptively</i> secure in the presence of non-erasing parties. Furthermore, current techniques are insufficient for proving adaptive security of any protocol for computing a non-trivial function in the presence of honest-looking parties.
							</p>
							<p class="rp_original">
								In order to present the extra difficult in constructing <i>adaptively</i> secure protocols, we roughly sketch the standard definition of secure multiparty computation. (Full definitions appear in <a href="#sec3">Section 3</a>.) Our presentation follows<sup class="reference" data-citation="MR">[?]</sup><sup class="reference" data-citation="Be1">[?]</sup><sup class="reference" data-citation="GwL">[?]</sup><sup class="reference" data-citation="C">[?]</sup>, while incorporating the notion of semi-honest parties in the definition. The definition follows the same outline in the secure channels setting and in the computational settings.
							</p>
							<section id="sec2.2.1">
								<h4>2.2.1. Background: How is Security Defined.</h4>
								<p class="rp_original">
									First an <i>ideal model</i> for secure multiparty computation is formulated. A computation in this ideal model captures "the highest level of security we can expect from a multiparty computation". Next we require that executing a secure protocol \(\pi\) for evaluating some function \(f\) of the parties' inputs in the actual <i>real-life setting</i> is "equivalent" to evaluating \(f\) in the ideal model, where the meaning of this "equivalence" is explained below.
								</p>
								<p class="rp_original">
									A computation in the ideal model proceeds as follows. First an <i>ideal-model-adversary</i> chooses to corrupt a set of parties (either adaptively or non-adaptively), learns their input, and possibly modifies it. Next all parties hand their (possibly modified) inputs to an incorruptible <i>trusted party</i>. The trusted party then computes the expected output (i.e. the function value) and hands it back to all parties. At this stage an adaptive adversary can choose to corrupt more parties. Finally the uncorrupted parties output the value received from the trusted party whereas the corrupted parties output some arbitrary function of the information gathered during this computation.
								</p>
								<p class="rp_original">
									In the real-life model there exists no trusted party and the parties must interact with one another using some protocol in order to compute any "non-trivial" function. We say that the execution of a protocol \(\pi\) for evaluating \(f\) is "equivalent" to evaluating \(f\) in the ideal model, if for any adversary \(\mathcal A\) in the real-life model, there exists an ideal-model-adversary \(\mathcal S\) that has the same effect on the computation as \(\mathcal A\), <i>even through \(\mathcal S\) operates in the ideal model</i>. That is, on any input, the outputs of the parties after running \(\pi\) in the real-life model in the presence of \(\mathcal A\) should be distributed equally to the outputs of parties evaluating \(f\) in the ideal model in the presence of \(\mathcal S\). Furthermore, this condition should hold <i>for any semi-honest protocol \(\pi'\) for \(\pi\)</i> (according to either of the above notions of semi-honesty).
								</p>
								<p class="rp_original">
									We require that the complexity of \(\mathcal S\) be comparable to (i.e. polynomial in) the complexity of \(\mathcal A\). This requirement can be motivated as follows. Machine \(\mathcal S\) represents "what could have been learned in the ideal model". Thus, security of a protocol can be interpreted as the following statement: "whatever \(\mathcal A\) can learn in the real-life model, could have been learned in the ideal model <i>within comparable complexity</i>". A much weaker and arguably unsatisfactory notion of security emerges if the complexity of \(\mathcal S\) does not depend on that of \(\mathcal A\). (This holds even in the non-adaptive case.)<sup class="footnote" id="fref1" data-footnote="1"><a href="#footnote1">1</a></sup>
								</p>
							</section>
							<section id="sec2.2.2">
								<h4>2.2.2. Problems with Proving Adaptive Security</h4>
								<p class="rp_original">
									A standard construction of an ideal-model-adversary, \(\mathcal S\), operates via block-box interaction with the real-life adversary \(\mathcal A\). More specifically let \(\pi'\) be a semi-honest protocol for \(\pi.\;\mathcal S\) runs the black-box representing \(\mathcal A\) on a simulated interaction with a set of parties running \(\pi'.\;\mathcal S\) corrupts (in the ideal model) the same parties that \(\mathcal A\) corrupts in the simulated interaction, and outputs whatever \(\mathcal A\) outputs. From the point of view of \(\mathcal A\), the interaction simulated by \(\mathcal S\) should be distributed identically to an authentic interaction with parties running \(\pi'\). It is crucial that \(\mathcal S\) be able to run a successful simulation based only on the information available to it in the ideal model, and in particular <i>without knowing the inputs of uncorrupted parties</i>. We restrict our presentation to this methodology of proving security of protocols, where \(\mathcal S\) is restricted to probabilistic polynomial time. We remark that no other proof method is known in this context. In the sequel we often call the ideal-model-adversary \(\mathcal S\) a <i>simulator</i>.
								</p>
								<p class="rp_original">
									Following the above methodology, the simulator that we construct has to generate simulated messages from the uncorrupted parties to the corrupted parties. In the non-adaptive case set of corrupted parties is fixed and known to the simulator. Thus the simulator can corrupt these parties, in the ideal model, before the simulation starts. In the adaptive case the corrupted parties are chosen by the simulated adversary \(\mathcal A\) as the computation unfolds. Here the simulator corrupts a party, in the ideal model, only when the simulated adversary decides on corrupting that party. Thus the following extra problem is encountered. Consider a currently uncorrupted party \(P\). Since \(\mathcal S\) does not know the input of \(P\), it may not know which messages should be sent by \(P\) to the corrupted parties. Still, \(\mathcal S\) has to generate some <i>dummy messages</i> to be sent by the simulated \(P\) to corrupted parties. When the simulated adversary \(\mathcal A\) later corrupts \(P\) it expects to see \(P\)'s internal data. The simulator should now be able to present internal data for \(P\) that is consistent with \(P\)'s <i>newly-learned input and with the messages previously sent by \(P\)</i>, according to <i>the particular semi-honest protocol \(\pi'\)</i> run by \(P\). It turns out that this can be done for the <span class="reference" data-citation="BGW">[?]</span> protocols for computing any function in the presence of non-erasing parties. Thus, the <span class="reference" data-citation="BGW">[?]</span> protocols are adaptively secure <i>in the presence of non-erasing parties</i>. Recall, however, that not every protocol which is secure against non-adaptive adversaries is also secure against adaptive adversaries (see example in the third paragraph of <a href="#sec1">the Introduction</a>).
								</p>
							</section>
							<section id="sec2.2.3">
								<h4>2.2.3. In Face of Honest-Looking Parties</h4>
								<p class="rp_original">
									Further problems are encountered when honest-looking parties are allowed, as demonstrated by the following example. Consider a protocol \(\beta\) that instructs each party, on private input \(\sigma\), to publicise a uniformly and independently chosen value \(r\) in some domain \(D\) and terminate. \(\beta\) looks "harmless" in the sense that no information on the inputs leaks out. However, consider the following honest-looking variant of \(\beta\). Let \(f_0,f_1\) be a <span class="definable">clawfree</span> pair of permutations over \(D\). Then, on input \(\sigma\in\{0,1\}\), an honest-looking party can 'commit' to its input by publicizing \(f_\sigma(r)\) instead of publicizing \(r\). If this honest-looking variant of \(\beta\) is shown secure via an efficient black-box simulation as described above, then the constructed simulator can be used to find claws between \(f_0\) and \(f_1\). Similar honest-looking variants can be constructed for the <span class="reference" data-citation="BGW">[?]</span><span class="reference" data-citation="CCD">[?]</span> protocols.  Consequently, if clawfree pairs of permutations exist then adaptive security of the <span class="reference" data-citation="BGW">[?]</span><span class="reference" data-citation="CCD">[?]</span> protocols, <i>in the presence of honest-looking parties</i>, cannot be proven via black-box simulation. In fact, such honest-looking variants can be constructed for any "non-trivial" protocol, with similar effects.
								</p>
							</section>
						</section>
						<section id="sec2.3">
							<h3>2.3. Adaptive Security in the Computational Setting</h3>
							<p class="rp_original">
								We sketch the extra difficult encountered in constructing adaptively secure protocols <i>in the computational setting</i>, and outline our solution for non-erasing parties. Consider the following folklore methodology for constructing secure protocols in the computational setting. Start with an adaptively secure protocol \(\pi\) <i>resilient against non-erasing parties in the secure channels setting</i>, and construct a protocol \(\tilde\pi\) by encrypting each message using a standard encryption scheme. We investigate the security of \(\tilde\pi\) <i>in the computational setting</i>.
							</p>
							<section id="sec2.3.1">
								<h4>2.3.1. Proving that \(\tilde\pi\) is Non-Adaptively Secure</h4>
								<p class="rp_original">
									We first sketch how \(\tilde\pi\) can be shown <i>non-adaptively</i> secure in the computational setting, assuming that \(\pi\) is non-adaptively secure in the secure channels setting. Let \(\mathcal S\) be the ideal-model-adversary (simulator) associated with \(\pi\) in the secure channels setting. (We assume that \(\mathcal S\) operates via "black-box simulation" of the real-life adversary \(\mathcal A\) as described above.) We wish to construct, in the computational setting, a simulator \(\tilde{\mathcal S}\) for \(\tilde\pi\). The simulator \(\tilde{\mathcal S}\) operates just like \(\mathcal S\), with two exceptions. First, in the computational setting the real-life adversary expects the messages sent to corrupted parties to be encrypted. Next, the real-life adversary expects to see the ciphertexts sent between uncorrupted parties. (In the secure channels setting the adversary does not see the communication between uncorrupted parties.) \(\tilde{\mathcal S}\) will imitate this situation as follows. First each message sent to a corrupted party will be appropriately encrypted. Next, the simulated uncorrupted parties will exchange <i>dummy ciphertexts</i>. (These dummy ciphertexts can be generated as, say, encryptions of the value '0'.) The validity of simulator \(\tilde{\mathcal S}\) can be shown to follow, in a straightforward way, from the validity of \(\mathcal S\) and the security of the encryption scheme in use.
								</p>
							</section>
							<section id="sec2.3.2">
								<h4>2.3.2. Problems with Proving Adaptive Security</h4>
								<p class="rp_original">
									When adaptive adversaries are considered, the construction of a simulator \(\tilde{\mathcal S}\) in the computational setting encounters the following additional problem. Consider an uncorrupted party \(P\). Since \(\tilde{\mathcal S}\) does not know the input of \(P\), it does not know which messages should be sent by \(P\) to other <i>uncorrupted</i> parties.<sup class="footnote" id="fref2" data-footnote="2"><a href="#footnote2">2</a></sup> Still, \(\tilde{\mathcal S}\) has to generate dummy ciphertexts to be sent by the simulated \(P\) to uncorrupted parties. These dummy ciphertexts are seen by the adaptive adversary. When the adversary later corrupts the simulated \(P\), it expects to see all of \(P\)'s internal data, as specified <i>by the semi-honest protocol \(pi'\)</i>. Certainly, this data may include the cleartexts of all the ciphertexts sent and received by \(P\) in the past, including the random bits used for encryption and decryption, respectively. Thus, it may be the case that some specific dummy ciphertext \(c\) was generated as an encryption of '0', and the simulated \(P\) now needs to "convince" the adversary that \(c\) is in fact an encryption of '1' (or vice versa). This task is impossible if a standard encryption scheme (i.e. an encryption scheme where no ciphertext can be a legal encryption of both '1' and '0') is used.
								</p>
								<p class="rp_original">
									We remark that Feldman, and independently Beaver and Haber, have suggested to solve this problem as follows<sup class="reference" data-citation="F">[?]</sup><sup class="reference" data-citation="BH">[?]</sup>. Instruct each party to <i>erase</i> (say, at the end of each round) all the information involved with encrypting and decrypting of messages. If the parties indeed erase this data, then the adversary will no longer see, upon corrupting a party, how past messages were encrypted and decrypted. Thus the problem of convincing the adversary in the authenticity of past ciphertexts no longer exists. Consequently, such "erasing" protocols can be shown adaptively secure in the computational setting. However, this approach is clearly not valid in the presence of semi-honest parties. In particular, it is not known whether the <span class="reference" data-citation="F">[?]</span><span class="reference" data-citation="BH">[?]</span> protocols (or any other previous protocols) are secure in the presence of non-erasing parties.
								</p>
							</section>
							<section id="sec2.3.3">
								<h4>2.3.3. Sketch of Our Solution</h4>
								<p class="rp_original">
									We solve this problem by constructing an encryption scheme that serves as an alternative to standard encryption schemes, and enjoys an additional property roughly described as follows. One can efficiently generate dummy ciphertexts that can later be "opened" as encryptions of either '0' or '1', at wish. (Here the word 'ciphertext' is used to denote all the information seen by the adversary during the execution of the protocol.) These dummy ciphertexts are different and yet computationally indistinguishable from the valid encryptions of '0' (or '1') produced in a real communication. We call such encryption protocols <i>non-committing</i>.<sup class="footnote" id="fref3" data-footnote="3"><a href="#footnote3">3</a></sup>
								</p>
								<p class="rp_original">
									Let \(\mathcal E^{(0)}\) (resp. \(\mathcal E^{(1)}\)) denote the distribution of encryptions of the value 0 (resp. 1) in a public-key encryption scheme. For simplicity, suppose that each of these distributions is generated by applying an efficient deterministic algorithm, denoted \(A^{(0)}\) (resp. \(A^{(1)}\)), to a uniformly selected \(n\)-bit string.<sup class="footnote" id="fref4" data-footnote="4"><a href="#footnote4">4</a></sup> In a <i>traditional</i> encryption scheme (with no decryption errors) the supports of \(\mathcal E^{(0)}\) and \(\mathcal E^{(1)}\) are disjoint  (and \(\mathcal E^{(0)},\mathcal E^{(1)}\) are computationally indistinguishable). In a <i>non-committing</i> encryption scheme, the supports of \(\mathcal E^{(0)}\) and \(\mathcal E^{(1)}\) are not disjoint but the probability that an encryption (of either '0' or '1') resides in their intersection, denoted \(I\), is negligible. Thus, decryption errors occur only with negligible probability. However, the simulator can efficiently generate a distribution \(\mathcal E^{amb}\) which assumes values in \(I\) so that this distribution is computationally indistinguishable from both \(\mathcal E^{(0)}\) and \(\mathcal E^{(1)}\).<sup class="footnote" id="fref5" data-footnote="5"><a href="#footnote5">5</a></sup> Furthermore, each "ambiguous ciphertext" \(c\in I\) is generated together with two random-looking \(n\)-bit strings, denoted \(r_0\) and \(r_1\), so that \(A^{(0)}(r_0)=A^{(1)}(r_1)=c\). That is, the string \(r_0\) (resp. \(r_1\)) may serve as a witness to the claim that \(c\) is an encryption of '0' (resp. '1'). See <a href="#sec3.4">Section 3.4</a> for a definition of non-committing encryption protocols.
								</p>
								<p class="rp_original">
									Using a non-committing encryption protocol, we resolve the simulation problems which were described above. Firstly, when transforming \(\pi\) into \(\tilde\pi\), we replace every bit transmission of \(\pi\) by an invocation of the non-committing encryption protocol. This allows us to generate dummy ciphertexts for messages sent between uncorrupted parties so that at a later stage we can substantiate for each such ciphertext both the claim that it is an encryption of '0' and the claim that it is an encryption of '1'. We stress that although dummy ciphertexts appear with negligible probability in a real execution, they are computationally indistinguishable from a uniformly generated encryption of either '0' or '1'. Thus, using a non-committing encryption protocol, we construct <i>Adaptively secure protocols</i> for computing any (recursive) function <i>in the computational model in the presence of non-erasing parties.</i> Finally, we construct a non-committing encryption protocol based on a primitive called <i>common-domain trapdoor systems</i> (see <a href="#def4.3">Definition 4.3</a>). We also describe two implementations based on the RSA and Diffie-Hellman assumptions respectively. Thus, we get
								</p>
							</section>
						</section>
						<span class="rp_theorem_header" id="theorem2.1">Theorem 2.1</span>
						<p class="rp_original rp_theorem">
							If common-domain trapdoor systems exist, then there exist secure protocols for computing any (recursive) function in the computational setting, in the presence of non-erasing parties and adaptive adversaries that corrupt less than a third of the parties.
						</p>
						<p class="rp_original">
							We remark that, using standard constructions (e.g. <span class="reference" data-citation="RB">[?]</span>), our protocols can be modified to withstand adversaries that corrupt less than half of the parties.
						</p>
						<span class="rp_sub_header">Dealing with Honest-Looking Parties</span>
						<p class="rp_original rp_sub">
							In <a href="#sec5">Section 5</a>, we sketch a solution for the case of honest-looking parties, assuming, in addition to the above, also the existence of a "trusted dealer" at a pre-computation stage. We stress that this result does not hold if an initial (trusted) set-up is not allowed.
						</p>
					</section>
					<section id="sec3">
						<h2>3. Definitions</h2>
						<p class="rp_original">
							In <a href="#sec3.1">Section 3.1</a> we define semi-honest protocols (with respect to the three variants discussed in <a href="#sec2.1">Section 2.1</a>). This notion underlies all our subsequent definitions. In Sections <a href="#sec3.2">3.2</a> and <a href="#sec3.3">3.3</a> we define adaptively secure multiparty computation in the secure channels and the computational settings, respectively. Although the focus of this work is the computational setting, we state this definition also in the secure channels setting. This will enable us to discuss our results as a general transformation from adaptively secure protocols in the secure channels setting into adaptively secure protocols in the computational setting, without getting into details of specific protocols. In <a href="#sec3.4">Section 3.4</a> we define our main tool, non-committing encryption protocols. Throughout Section 3 we assume that the reader has acquired the intuition provided in <a href="#sec2">Section 2</a>.
						</p>
						<p class="rp_original">
							Let us first recall the standard definition of computational indistinguishability of distributions.
						</p>
						<span class="rp_definition_header" id="def3.1">Definition 3.1</span>
						<p class="rp_original rp_definition">
							Let \(\mathcal A=\{A_x\}_{x\in\{0,1\}^*}\) and \(\mathcal B=\{B_x\}_{x\in\{0,1\}^*}\) be two ensembles of probability distributions. We say that \(\mathcal A\) and \(\mathcal B\) are <b>computationally indistinguishable</b> if for every positive polynomial \(p\), for every probabilistic polynomial-time algorithm \(D\) and for all sufficiently long \(x\)'s, $$\left|\operatorname{Pr}\left[D(A_x)=1\right]-\operatorname{Pr}\left[D(B_x)=1\right]\right|\;\lt\;\frac{1}{p(|x|)}.$$
						</p>
						<p class="rp_original">
							We colloquially say that "\(A_x\) and \(B_x\) are computationally indistinguishable", or "\(A_x\stackrel{c}{\approx}B_x\)".
						</p>
						<section id="sec3.1">
							<h3>3.1. Semi-honest Protocols</h3>
							<p class="rp_original">
								We define semi-honest parties (or, equivalently, semi-honest protocols) for the three alternative notions of semi-honesty discussed in <a href="#sec2.1">Section 2.1</a>. First we define <b>honest-but-non-erasing</b> (or in short <b>non-erasing</b>) protocols. Informally, a protocol \(\pi'\) may omit instructions to erase data. Actually, it suffices to consider a non-erasing protocol which keeps a record of the entire history of the computation.
							</p>
							<span class="rp_definition_header" id="def3.2">Definition 3.2</span>
							<p class="rp_original rp_definition">
								Let \(\pi\) and \(\pi'\) be \(n\)-party protocols. We say that \(\pi'\) is a <b>non-erasing</b> protocol for \(\pi\) if \(\pi'\) is identical to \(\pi\) with the exception that, in addition to the instructions of \(\pi\), protocol \(\pi'\) copies the contents of each memory location accessed by \(\pi\) to a special tape (inaccessible by \(\pi\)).
							</p>
							<p class="rp_original">
								Next we define <b>honest-looking</b> protocols. Informally, a party is honest-looking if its behavior is indistinguishable from the behavior of an honest party by any external test. (Internally the party may arbitrarily deviate from the protocol.) More formally, let  \(\operatorname{COM}_\pi(\vec{x}, \vec{r})\) denote the communication among \(n\) parties running \(\pi\) on input \(\vec{x}\) and random input \(\vec{r}\) (\(x_i\) and \(r_i\) for party \(P_i\)). Let \(\operatorname{COM}_\pi(\vec{x})\) denote the random variable describing \(\operatorname{COM}_\pi(\vec{x}, \vec{r})\) when \(\vec{r}\) is uniformly chosen. For \(n\)-party protocols \(\rho\) and \(\pi\) and an index \(i\in[n]\), let \(\rho_{\operatorname{/}(i,\pi)}\) denote the protocol where party \(P_i\) executes \(pi\) and all the other parties execute \(\rho\).
							</p>
							<span class="rp_definition_header" id="def3.3">Definition 3.3</span>
							<p class="rp_original rp_definition">
								Let \(\pi\) and \(\pi'\) be \(n\)-party protocols. We say that \(\pi'\) is a <b>perfectly honest-looking</b> protocol for \(\pi\) if for any input \(\vec{x}\), for any \(n\)-party "test" protocol \(\rho\), and for any index \(i\in[n]\), we have $$\operatorname{COM}_{\rho_{\operatorname{/}(i,\pi)}}(\vec{x})\stackrel{d}{=}\operatorname{COM}_{\rho_{\operatorname{/}(i,\pi')}}(\vec{x})$$ (where \(\stackrel{d}{=}\) stands for "identically distributed"). If the test protocol \(\rho\) is restricted to probabilistic polynomial time, and \(\operatorname{COM}_{\rho_{\operatorname{/}(i,\pi)}}(\vec{x})\stackrel{c}{\approx}\operatorname{COM}_{\rho_{\operatorname{/}(i,\pi')}}(\vec{x})\), then we say that \(\pi'\) is a <b>computationally honest-looking</b> protocol for \(pi\).
							</p>
							<p class="rp_original">
								Here the "test" protocol \(\rho\) represents a collaboration of all parties in order to test whether \(P_i\) is honest.
							</p>
							<p class="rp_original">
								Next we define <b>weakly-honest</b> protocols. Here we require that Definition 3.3 is satisfied only with respect to the original protocol \(\pi\), rather than with respect to any test protocol \(\rho\).
							</p>
							<span class="rp_definition_header" id="def3.4">Definition 3.4</span>
							<p class="rp_original rp_definition">
								Let \(pi\) and \(pi'\) be \(n\)-party protocols. We say that \(\pi'\) is a <b>perfectly weakly-honest</b> protocol for \(\pi\) if for any input \(\vec{x}\) and for any index \(i\in[n]\), we have $$\operatorname{COM}_\pi(\vec{x})\stackrel{d}{=}\operatorname{COM}_{\pi/(i,\pi')}(\vec{x}).$$ If \(\pi\) is restricted to probabilistic polynomial time, and if \(\operatorname{COM}_\pi(\vec{x})\stackrel{c}{\approx}\operatorname{COM}_{\pi/(i,\pi')}(\vec{x})\), then we say that \(\pi'\) is a <b>computationally weakly-honest</b> protocol for \(\pi\).
							</p>
						</section>
						<section id="sec3.2">
							<h3>3.2. Adaptive Security in the Secure Channels Setting</h3>
							<p class="rp_original">
								We define adaptively secure multiparty computation in the secure channels setting. That is, we consider a synchronous network where every two parties are connected via a secure communication link (i.e. the adversary does not see, nor alter, messages sent between uncorrupted parties). The adversary is computationally unlimited.
							</p>
							<p class="rp_original">
								We use the standard methodology presented in <a href="#sec2.2">Section 2.2</a>. That is, the execution of a protocol for computing some function is compared to evaluating the function in an ideal model, where a trusted party is used. We substantiate the definition in three steps. First, we give an exact definition of this ideal model. Next, we formulate our (high level) motion of 'real-life' protocol execution. Finally, we describe and formalize the method of comparing computations.
							</p>
							<section id="sec3.2.1">
								<h4>3.2.1. The Computation in the Ideal Model</h4>
								<p class="rp_original">
									The computation in the ideal model, in the presence of an ideal-model-adversary \(\mathcal S\), proceeds as follows. The parties have inputs \(\vec{x}=x_1\dots x_n\in D^n\) (party \(P_i\) has input \(x_i\)) and wish to compute \(f(x_1,\dots,x_n)\), where \(f\) is a predetermined function.<sup class="footnote" id="fref6" data-footnote="6"><a href="#footnote6">6</a></sup> The adversary \(\mathcal S\) has no initial input, and is parametrized by \(t\), the maximum number of parties it may corrupt.
								</p>
								<span class="rp_sub_header">First corruption stage:</span>
								<p class="rp_original rp_sub">
									First \(\mathcal S\) proceeds in up to \(t\) iterations. In each iteration \(\mathcal S\) may decide to corrupt some party, based on \(\mathcal S\)'s random input and the information gathered so far. Once a party is corrupted its internal data (that is, its input) becomes known to \(\mathcal S\). A corrupted party remains corrupted for the rest of the computation. Let \(B\) denote the set of corrupted parties at the end of this stage.
								</p>
								<span class="rp_sub_header">Input substitution stage:</span>
								<p class="rp_original rp_sub">
									\(\mathcal S\) may alter the inputs of the corrupted parties; however, this is done without any knowledge of the inputs of the good parties. Let \(\vec{b}\) be the \(|B|\)-vector of the altered inputs of the corrupted parties, and let \(\vec{y}\) be the \(n\)-vector constructed from the input \(\vec{x}\) by substituting the entries of the corrupted parties by the corresponding entries in \(\vec{b}\).
								</p>
								<span class="rp_sub_header">Computation stage:</span>
								<p class="rp_original rp_sub">
									The parties hand \(\vec{y}\) to the trusted party (party \(P_i\) hands \(y_i\)), and receive \(f(\vec{y})\) from the trusted party.<sup class="footnote" id="fref7" data-footnote="7"><a href="#footnote7">7</a></sup>
								</p>
								<span class="rp_sub_header">Second corruption stage:</span>
								<p class="rp_original rp_sub">
									Now that the output of the computation is known, \(\mathcal S\) proceeds in another sequence of up to \(t-|B|\) iterations, where in each iteration \(\mathcal S\) may decide to corrupt some additional party, based on \(\mathcal S\)'s random input and the information gathered so far (this information new includes the value received from the trusted party by parties in \(B\)). We stress that \(\mathcal S\) may corrupt at most \(t\) parties in the entire computation.
								</p>
								<span class="rp_sub_header">Output stage:</span>
								<p class="rp_original rp_sub">
									The uncorrupted parties output \(f(\vec{y})\), and the corrupted parties output some arbitrary function, computed by the adversary, of the information gathered by the adversary (i.e. \(\vec{b}\) and \(f(\vec{y})\)). We let the \(n\)-vector \(\operatorname{IDEAL}_{f,\mathcal S}(\vec{x})= \operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_1\dots \operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_n\) denotes the outputs of the parties on input \(\vec{x}\), trusted party for computing \(f\), and adversary \(\mathcal S\) (party \(P_i\) outputs \(\operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_i\)).
								</p>
								<p class="rp_original">
									For the benefit of formalistic readers we further formalize the above discussion (in Definitions <a href="#def3.5">3.5</a> <a href="#def3.6">through</a> <a href="#def3.7">3.7</a>). Other readers are advised to skip a page up to the paragraph discussing the computation in the real-life setting.
								</p>
								<p class="rp_original">
									First, we need two technical notations.
								</p>
								<ul class="rp_original">
									<li>For a vector \(\vec{x}=x_1\dots x_n\), and a set \(B\subseteq[n]\), let \(\vec{x}_B\) denote the vector \(\vec{x}\), projected on the indices in \(B\).</li>
									<li>For an \(n\)-vector \(\vec{x}=x_1\dots x_n\), and a set \(B\subseteq[n]\), and a \(|B|\)-vector \(\vec{b}= b_1\dots b_{|B|}\), let \(\vec{x}/_{(B,\vec{b})}\) denote the vector constructed from vector \(\vec{x}\) by substituting the entries whose indices are in \(B\) by the corresponding entries from \(\vec{b}\).</li>
								</ul>
								<span class="rp_definition_header" id="def3.5">Definition 3.5</span>
								<p class="rp_original rp_definition">
									Let \(D\) be the domain of possible inputs of the parties, and let \(R\) be the domain of possible random inputs. A \(t\)-<b>limited ideal-model-adversary</b> is a quadruple \(\mathcal S=(t,b,h,O)\), where:
								</p>
								<ul class="rp_original rp_definition">
									<li>\(t\) is the maximum number of corrupted parties.</li>
									<li>\(b\,:\;[n]^*\times D^*\times R\mapsto[n]\cup\{\perp\}\) is the <b>selection</b> function for corrupting parties (the value \(\perp\) is interpreted as "no more parties to corrupt at this stage")</li>
									<li>\(h\,:\;[n]^*\times D^*\times R\mapsto D^*\) is the <b>input substitution</b> function</li>
									<li>\(O\,:\;D^*\times R\mapsto\{0,1\}^*\) is an <b>output</b> function for the bad parties.</li>
								</ul>
								<p class="rp_original">
									The set of corrupted parties is now defined as follows.
								</p>
								<span class="rp_definition_header" id="def3.6">Definition 3.6</span>
								<p class="rp_original rp_definition">
									Let \(D\) be the domain of possible inputs of the parties, and let \(\mathcal S=(t,b,h,O)\) be a \(t\)-limited ideal-model-adversary. Let \(\vec{x}\in D^n\) be an input vector, and let \(r\in R\) be a random input for \(\mathcal S\). The \(i\)<b>-th set of faulty parties</b> in the ideal model \(B^{(i)}(\vec{x},r)\), is defined as follows.
								</p>
								<ul class="rp_original rp_definition">
									<li>\(B^{(0)} (\vec{x}, r)=\phi\).</li>
									<li>Let \(b_i\stackrel{\Delta}{=}b(B^{(i)} (\vec{x},r), \vec{x}_{B^{(i)}(\vec{x},r)},r).\) For \(0\leq i\lt t\), and as long as \(b_i\neq\perp\), let $$B^{(i+1)}(\vec{x},r) \stackrel{\Delta}{=} B^{(i)} (\vec{x},r) \cup\{b_i\}$$</li>
									<li>Let \(i^*\) be the minimum between \(t\) and the first \(i\) such that \(b_i=\perp\). Let \(b_i^f\stackrel{\Delta}{=}b(B^{(i)} (\vec{x},r), \vec{x}_{B^{(i)}(\vec{x},r)},f(\vec{y}),r)\), where \(\vec{y}\) is the substituted input vector for the trusted party. That is, \(\vec{y} \stackrel{\Delta}{=} \vec{x} /_{(B^{(i^*)}(\vec{x},r), h(B^{(i^*)}(\vec{x},r), \vec{x}_{B^{(i^*)} (\vec{x},r)},r))}\). For \(i^*\leq i\lt t\), let $$B^{(i+1)}(\vec{x},r)\stackrel{\Delta}{=}B^{(i)}(\vec{x},r)\cup b_i^f.$$</li>
								</ul>
								<p class="rp_original">
									In Definition 3.7, we use \(B^{(i)}\) instead of \(B^{(i)}(\vec{x},r)\).
								</p>
								<span class="rp_definition_header" id="def3.7">Definition 3.7</span>
								<p class="rp_original rp_definition">
									Let \(f\,:\;D^n\mapsto D'\) for some sets \(D,\,D'\) be the computed function, and let \(\vec{x}\in D^n\) be an input vector. The <b>output of computing function \(f\) in the ideal model</b> with adversary \(\mathcal S=(t,b,h,O)\), on input \(\vec{x}\) and random input \(r\), is an \(n\)-vector \(\operatorname{IDEAL}_{f,\mathcal S}(\vec{x})= \operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_1\dots \operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_n\) of random variables, satisfying for every \(1\leq i\leq n\):
									
									$$ \operatorname{IDEAL}_{f,\mathcal S}(\vec{x})_i = \begin{cases}f(\vec{y} &\text{if }i\not\in B^{(t)}\\ O(\vec{x}_{B^{(t)}},f(\vec{y}),r) &\text{if }i\in B^{(t)}\end{cases} $$
									
									where \(B^{(t)}\) is the \(t\)<sup>th</sup> set of faulty parties, \(r\) is the random input of \(\mathcal S\), and \(\vec{y}= \vec{x}/_{(B^{(t)}, h(B^{(t)}, \vec{x}_{B^{(t)}}, r))}\) is the substituted input vector for the trusted party.
								</p>
							</section>
							<section id="sec3.2.2">
								<h4>3.2.2. Computation in the Real-Life Setting</h4>
								<p class="rp_original">
									Next we describe the execution of a protocol \(\pi\) in the real-life scenario. The parties engage in a synchronous computation in the secure channels setting, running a semi-honest protocol \(\pi'\) for \(\pi\) (according to any one of the notions of semi-honesty defined above). A computationally unbounded \(t\)-<b>limited real-life adversary</b> may choose to corrupt parties at any point during the computation, based on the information known to the previously corrupted parties, and as long as at most \(t\) parties are corrupted altogether. Once a party is corrupted the current contents of its memory (as determined by the semi-honest protocol \(\pi'\)) becomes available to the adversary. From this point on, the corrupted party follows the instructions of the adversary. Once the computation is completed, each uncorrupted party outputs whatever it has computed to be the function value. Without loss of generality, we use the convention by which the corrupted parties output their entire <b>view</b> on the computation. The view consists of all the information gathered by the adversary during the computation. Specifically, the view includes the inputs and random inputs of the corrupted parties and all the communication seen by the corrupted parties.
								</p>
								<p class="rp_original">
									We use the following notation. Let \(\operatorname{VIEW}_{\pi,\mathcal A}(\vec x, \vec r)\) denote the <b>view</b> of the adversary \(mathcal A\) when interacting with parties running protocol \(\pi\) on input \(\vec x\) and random input \(\vec r\) (\(x_i\) and \(r_i\) for party \(P_i\)), as described above. Let \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x,\vec r)_i\) denote the output of party \(P_i\) after running protocol \(\pi\) on input \(\vec x=x_1\dots x_n\) and random input \(\vec r=r_1\dots r_n\), and with a real life adversary \(\mathcal A\). (By the above convention, we have \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x,\vec r)_i=\operatorname{VIEW}_{\pi,\mathcal A}(\vec x,\vec r)\) for corrupted parties \(P_i\).) Let \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x)_i\) denote the random variable describing \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x,\vec r)_i\) where \(\vec r\) is uniformly chosen. Let \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x)=\operatorname{EXEC}_{\pi,\mathcal A}(\vec x)_i\dots\operatorname{EXEC}_{\pi,\mathcal A}(\vec x)_n\).
								</p>
							</section>
							<section id="sec3.2.3">
								<h4>3.2.3. Comparing Computations</h4>
								<p class="rp_original">
									Finally we require that executing a secure protocol \(\pi\) for evaluating a function \(f\) be equivalent to evaluating \(f\) in the ideal model, in the following sense.
								</p>
								<span class="rp_definition_header" id="def3.8">Definition 3.8</span>
								<p class="rp_original rp_definition">
									Let \(f\) be an \(n\)-ary function, \(\pi\) be a protocol for \(n\) parties and \(\mathcal T\) a type of semi-honest behavior (i.e. as in any of the Definitions <a href="#def3.2">3.2</a> <a href="#def3.3">through</a> <a href="#def3.4">3.4</a>). We say that \(\pi\;t\)-<b>securely computes \(f\) in the secure channels setting</b> in the presence of \(\mathcal T\)-semi-honest parties and adaptive adversaries, if for any \(\mathcal T\)-semi-honest protocol \(\pi'\) for \(\pi\) and for any \(t\)-limited real-life (adaptive) adversary \(\mathcal A\), there exists a \(t\)-limited ideal-model-adversary \(\mathcal S\), such that the complexity of \(\mathcal S\) is polynomial in the complexity of \(\mathcal A\), and for every input vector \(\vec x\) we have $$\operatorname{IDEAL}_{f,\mathcal S}(\vec x)\stackrel{d}{=}\operatorname{EXEC}_{\pi',\mathcal A}(\vec x)$$
								</p>
								<span class="rp_sub_header">Remark</span>
								<p class="rp_original rp_sub">
									Definition 3.8 is stated for a single value of \(n\). In order to discuss asymptotic complexity (in \(n\)), we assume that the function \(f\), the protocol \(\pi\), the simulator \(\mathcal S\), and the adversary \(\mathcal A\) are Turing machines that have \(n\), the number of parties, as part of their inputs.
								</p>
							</section>
							<section id="sec3.2.4">
								<h4>3.2.4. Black-box Simulation</h4>
								<p class="rp_original">
									In the sequel we use a more restricted notion of equivalence of computations, where the ideal-model adversary is limited to black-box simulation of the real-life setting. That is, for any semi-honest protocol \(\pi'\) for \(\pi\) there should exist an ideal-model adversary \(\mathcal S\) with oracle (or black-box) access to a real-life adversary. This black-box represents the input-output relations of the real-life adversary described above. For concreteness, we present the following description of the "mechanics" of this black-box, representing a real-life adversary. The black-box has a <b>random tape</b>, where the black-box expects to find its random input, and an <b>input-output tape</b>. Once a special <b>start</b> input is given on the input-output tape, the interaction on this tape proceeds in iterations, as follows. Initially, no party is corrupted. In each iteration \(l\), first the black-box expects to receive the information gathered in the \(l\)<sup>th</sup> round. (In the secure channels setting this information consists of the messages sent by the uncorrupted parties to the corrupted parties.) Next black-box outputs the messages to be sent the corrupted parties in the \(l\)<sup>th</sup> round. Next, the black-box may issue several '<b>corrupt</b> \(P_i\)' requests. Such a request should be answered by the internal data of \(P_i\), according to protocol \(\pi'\). Also, from this point on \(P_i\) is corrupted. At the end of the interaction, the <b>output of the real-life adversary</b> is defined as the contents of the random tape succeeded by the history of the contents of the input-output tape during the entire interaction. We let \(\mathcal S^{\mathcal A}\) denote the ideal-model adversary \(\mathcal S\) with black-box access to a real-life adversary \(\mathcal A\).
								</p>
								<p class="rp_original">
									The simulator is restricted to probabilistic polynomial-time (where each invocation of the black-box is counted as one operation).<sup class="footnote" id="fref8" data-footnote="8"><a href="#footnote8">8</a></sup> Furthermore, we limit the operation of the simulator as follows. We require that the <b>start</b> message is sent only once, and that no party is corrupted in the ideal model unless a request to corrupt this party is issued by the black-box.
								</p>
								<p class="rp_original">
									If <a href="#def3.8">Definition 3.8</a> is satisfied by an ideal-model adversary limited to black-box simulation as described above, then we say that \(pi\;t\)-<b>securely computes \(F\) in a simulatable way</b>. In this case we call the ideal-model adversary a <b>black-box simulator</b>, or in short a <b>simulator</b>.
								</p>
								<p class="rp_original">
									We remark that the only purpose of the technical restrictions imposed on the operation of the simulator is to facilitate proving composition theorems (such as <a href="#theorem4.2">Theorem 4.2</a>). We stress that the security of known protocols (e.g. <span class="reference" data-citation="BGW">[?]</span>) can be shown via simulators that obey these restrictions.
								</p>
							</section>
						</section>
						<section id="sec3.3">
							<h3>3.3. Adaptive Security in the Computational Setting</h3>
							<p class="rp_original">
								We now turn to define adaptively secure multiparty computation in the computational setting. Here the communication links between parties are <b>insecure</b>; that is, all messages sent on all links are seen by the adversary.<sup class="footnote" id="fref9" data-footnote="9"><a href="#footnote9">9</a></sup> All parties, as well as the adversary, are restricted to probabilistic polynomial time. Furthermore, we introduce a <b>security parameter</b>, determining 'how close' a real-life computation is to a computation in the ideal model. All parties are polynomial also in the security parameter. For simplicity of presentation, we identify the security parameter and the length of the inputs with the number of parties, denoted \(n\).
							</p>
							<p class="rp_original">
								The framework of defining adaptively secure multiparty computation in this setting is the same as in the secure channels setting (<a href="#sec3.2">Section 3.2</a>). That is, we compare the real-life computation with a computation in the same ideal model. Since the real-life adversary is restricted to probabilistic polynomial time, so is the ideal model adversary. The execution of a protocol \(\pi\) in the real-life scenario (of the computational setting), as well as the notation \(\operatorname{EXEC}_{\pi,\mathcal A}(\vec x)\), are the same as in the secure channels setting, with the exception that the real-life adversary sees all the communication between the uncorrupted parties. Needless to say that the ideal model is the same in both settings.
							</p>
							<p class="rp_original">
								We define equivalence of a real-life computation to an ideal-model computation in the same way, with the exception that here we only require that the corresponding distributions are computationally indistinguishable. Black-box simulation is defined as in the secure channels setting, with the exception that the information gathered by the adversary in each round includes the communication between all parties.
							</p>
							<span class="rp_definition_header" id="def3.9">Definition 3.9</span>
							<p class="rp_original rp_definition">
								Let \(f\) be an \(n\)-ary function, \(\pi\) be a protocol for \(n\) parties and \(\mathcal T\) a type of semi-honest behavior (i.e. as in any of the Definitions <a href="#def3.2">3.2</a> <a href="#def3.3">through</a> <a href="#def3.4">3.4</a>). We say that <b>\(\pi\;t\)-securely computes \(f\) in the computational setting</b>, in the presence of \(\mathcal T\)-semi-honest parties and adaptive adversaries, if for any \(\mathcal T\)-semi-honest protocol \(\pi'\) for \(\pi\) and for any \(t\)-limited real-life (adaptive) adversary \(\mathcal A\) there exists a \(t\)-limited ideal-model-adversary \(\mathcal S\), such that for every input vector \(\vec x\) we have $$\operatorname{IDEAL}_{f,\mathcal S}(\vec x)\stackrel{c}{\approx}\operatorname{EXEC}_{\pi',\mathcal A}(\vec x).$$ If \(\mathcal S\) is restricted to black-box simulation of real-life adversaries, as described above, then we say that <b>\(pi\;t\)-securely computes \(f\) in a simulatable way</b> in the computational scenario.
							</p>
						</section>
						<section id="sec3.4">
							<h3>3.4. Non-committing Encryption</h3>
							<p class="rp_original">
								We present a concise definition of a non-committing encryption protocol in our multiparty scenario. First define the <b>bit transmission</b> function \(\operatorname{BTR}\;:\;\{0,1,\perp\}^n\,\mapsto\,\{0,1,\perp\}^n\). This function is parameterized by two identities of parties (i.e. indices \(s,r\in[n]\)), with the following interpretation. \(\operatorname{BTR}_{s,r}\) describes the secure transmission of a bit from party \(P_s\) (the sender) to party \(P_r\) (the receiver). That is, for \(\vec x=x_1,\dots,x_n\in\{0,1,\perp\}^n\) let $$\operatorname{BTR}_{s,r}(\vec x)_i=\begin{cases} x_s &\text{if }i=r\\\perp &\text{otherwise}\end{cases}$$ where \(\operatorname{BTR}_{s,r}(\vec x)_i\) is the \(i\)<sup>th</sup> component of the vector \(\operatorname{BTR}_{s,r}(\vec x)\). We are interested in input vectors \(\vec x\) where \(x_s\) (i.e. the senders input is in \(\{0,1\}\). All other inputs are assumed to be \(\perp\).
							</p>
							<span class="rp_definition_header" id="def3.10">Definition 3.10</span>
							<p class="rp_original rp_definition">
								Let \(s,r\in[n]\) and \(s\not=r\). A protocol \(\epsilon\) is a \(t\)-resilient (in the presence of \(\mathcal T\)-semi-honest parties and adaptive adversaries), <b>non-committing encryption</b> protocol (from \(P_s\) to \(P_r\)) if \(\epsilon\;t\)-securely computes \(\operatorname{BTR}_{s,r}\), in a simulatable way, in the computational model, in the presence of \(\mathcal T\)-semi-honest parties and an adaptive adversary.
							</p>
							<p class="rp_original">
								It may not be immediately evident how Definition 3.10 corresponds to the informal description of non-committing encryptions, presented in <a href="#sec2.3">Section 2.3</a>. A closer look, however, will show that the requirements from the simulator associated with a non-committing encryption protocol (according to Definition 3.10) imply these informal descriptions. In particular, in the case where the simulated adversary corrupts the sender and receiver only after the last communication round, the simulator has to first generate some simulated communication between the parties, without knowing the transmitted bit. (This communication serves as the "dummy ciphertext".) When the sender and/or the receiver are later corrupted, the simulator has to generate internal data that correspond to any required value of the transmitted bit.
							</p>
						</section>
					</section>
					<section id="sec4">
						<h2>4. Non-erasing Parties</h2>
						<p class="rp_original">
							We show that any recursive function can be securely computed in the computational setting, in the presence of adaptive adversaries and non-erasing parties. In <a href="#sec4.1">Subsection 4.1</a> we show how, using a non-committing encryption protocol, a simulatable protocol for computing some function \(f\) in the computational setting can be constructed from any simulatable protocol for computing \(f\) in the secure channels setting. In <a href="#sec4.2">Subsection 4.2</a> we present our construction of non-committing encryption. We use the following result as our starting point:
						</p>
						<span class="rp_theorem_header" id="theorem4.1">Theorem 4.1</span>
						<p class="rp_original rp_theorem">
							The <span class="reference" data-citation="BGW">[?]</span><span class="reference" data-citation="CCD">[?]</span> protocols for computing any function of \(n\) function of \(n\) inputs are \((\lceil\frac n 3\rceil-1)\)-securely computable in a simulatable way, <b>in the secure channels setting</b>, in the presence of non-erasing parties and adaptive adversaries.<sup class="footnote" id="fref10" data-footnote="10"><a href="#footnote10">10</a></sup>
						</p>
						<section id="sec4.1">
							<h3>4.1. Adaptive Security Given Non-Committing Encryption</h3>
							<p class="rp_original">
								The following theorem formalizes the discussion in <a href="#sec2.3">Section 2.3</a>.
							</p>
							<span class="rp_theorem_header" id="theorem4.2">Theorem 4.2</span>
							<p class="rp_original rp_theorem">
								Let \(f\) be an \(n\)-ary function, \(t\lt n\) and \(\pi\) be a protocol that \(t\)-securely computes \(f\) in a simulatable way <b>in the secure channels setting</b>, in the presence of non-erasing parties and adaptive adversaries. Suppose that \(\epsilon_{s,r}\) is a \(t\)-resilient non-committing encryption protocol, resilient to non-erasing parties and adaptive adversaries, for transmission from \(P_s\) to \(P_r\). Let \(\tilde\pi\) be the protocol constructed from \(\pi\) as follows. For each bit \(\sigma\) transmitted by \(\pi\) from party \(P_s\) to party \(P_r\), protocol \(\tilde\pi\) invokes a copy of an \(\epsilon_{s,r}\) for transmitting \(\sigma\). Then \(\tilde\pi\;t\)-securely computes \(f\), in a simulatable way <b>in the computational setting</b>, in the presence of non-erasing parties and adaptive adversaries.
							</p>
							<span class="rp_sub_header">Proof (sketch)</span>
							<p class="rp_original rp_sub">
								Let \(\pi'\) be a non-erasing protocol for \(\pi\) and let \(\mathcal S\) be a simulator for \(\pi'\) <i>in the secure channels setting</i>. For simplicity we assume that in protocol \(\pi\), as well as in the interaction generated by \(\mathcal S\), each party sends one bit to each other party in each round. Let \(\delta\) be the (computational model) simulator that corresponds to the non-erasing protocol \(\epsilon'\) for the non-committing encryption protocol \(\epsilon\). Given these two different simulators, we construct a simulator \(\tilde{\mathcal S}\) for protocol \(\tilde\pi\) in the computational setting. The simulator \(\tilde{\mathcal S}\) will be a modification of \(\mathcal S\) and will use several copies of \(\delta\) as subroutines.
							</p>
							<p class="rp_original rp_sub">
								Recall that \(\mathcal S\) is supposed to interact with a black box representing a real-life adversary <i>in the secure channels setting</i>. That is, at each round \(\mathcal S\) generates all the messages sent from uncorrupted parties to corrupted parties. Furthermore, whenever the black box decides to corrupt some party \(P\), machine \(\mathcal S\) generates internal data for \(P\) which is consistent with \(P\)'s input and with the messages previously sent by \(P\) to corrupted parties.
							</p>
							<p class="rp_original rp_sub">
								The simulator \(\tilde{\mathcal S}\), interacts with a black box representing an arbitrary real-life adversary <i>in the computational setting</i>, denoted by \(\tilde{\mathcal A}\). The simulator \(\tilde{\mathcal S}\) is identical to \(\mathcal S\) with the exception that for each bit sent in the interaction simulated by \(\mathcal S\), the simulator \(\tilde{\mathcal S}\) invokes a copy of \(\delta\) and \(\tilde{\mathcal S}\) incorporates the outputs of the various copies of \(\delta\) in its (i.e. \(\tilde{\mathcal S}\)'s) communication with \(\tilde{\mathcal A}\). Likewise, \(\tilde{\mathcal S}\) extracts the transmitted bits from the invocations of \(\delta\) corresponding to message transmissions from corrupted parties to uncorrupted ones. (The way \(\tilde{\mathcal S}\) handles these invocation will be discussed below.) At this point we stress that \(\tilde{\mathcal A}\) is the only adversary that \(\tilde{\mathcal S}\) needs to simulate and to this end it "emulates" real life adversaries of its choice for the copies of \(\delta\). In particular, when \(\mathcal S\) asks to corrupt some party \(P\), the simulator \(\tilde{\mathcal S}\) corrupts the same party \(P\). When \(\mathcal S\) generates \(P\)'s view <i>in the secure channel setting</i>, \(\tilde{\mathcal S}\) will complete this view into \(P\)'s view <i>in the computational setting</i> by using the various copies of \(\delta\).
							</p>
							<p class="rp_original rp_sub">
								We describe how \(\tilde{\mathcal S}\) handles the various copies of \(\delta\). As stated above, \(\tilde{\mathcal S}\) emulates a real-life adversary for each copy of \(\delta\) using the communication tapes by which this copy is supposed to interact with its black-box/adversary. The information that \(\delta\) expects to receive from its black box is extracted, in the obvious manner, from the information that \(\tilde{\mathcal S}\) receives from \(\tilde{\mathcal A}\). That is, \(\tilde{\mathcal S}\) hands \(\delta\) the messages, sent by the corrupted parties, that are relevant to the corresponding invocation of \(\epsilon'\). Furthermore, all the past and current requests for corrupting parties (issued by \(\tilde{\mathcal A}\)) are handed over to \(\delta\). The partial view received from each copy of \(\delta\) is used in the emulation of the corresponding black box (of this \(\delta\) copy) as well as incorporated in the information handed by \(\tilde{\mathcal S}\) to \(\tilde{\mathcal A}\). When \(\tilde{\mathcal A}\) asks to corrupt some party \(P\), the simulator \(\tilde{\mathcal S}\) emulates a 'corrupt \(P\)' request to each copy of \(\delta\) and obtains the internal data of \(P\) in the correspoding sub-protocol \(\epsilon\) which it (i.e. \(\tilde{\mathcal S}\)) hands to \(\tilde{\mathcal A}\) (along with the information obtained by \(\mathcal S\)&mdash;the secure channel simulator). Finally, observe that \(\delta=\delta_{s,r}\) (where \(P_s\) and \(P_r\) are the designated sender and receiver) also expects to interact with parties in the ideal model. This interaction consists of issuing 'corrupt' requests and obtaining the internal data (of the ideal model). This interaction is (also) emulated by \(\tilde{\mathcal S}\) as follows. Whenever \(\delta\) wishes to corrupt a party \(P\) which is either \(P_s\) or \(P_r\), the simulator \(\tilde{\mathcal S}\) finds out which bit, \(\sigma\) was supposed to be sent in this invocation of \(\epsilon'_{r,s}\) and passes \(\sigma\) to \(\delta_{r,s}\). We stress that \(\sigma\) is available to \(\tilde{\mathcal S}\) since at this point in time \(P\) has already been corrupted and furthermore \(\tilde{\mathcal S}\) (which mimics \(\mathcal S\)) has already obtained \(P\)'s view in the secure channel setting. (Here we use Definitions <a href="#def3.9">3.9</a> and <a href="#def3.10">3.10</a> which guarantee that \(\delta\) corrupts a party only if this party is already corrupted by \(\delta\)'s black box. We also use the fact that \(\tilde{\mathcal S}\) is playing \(\delta\)'s black box and is issuing a 'corrupt \(P\)' request only after receiving such a request from \(\tilde{\mathcal A}\) and having simulated this corruption as \(\mathcal S\).). In case \(P\) is neither \(P_s\) not \(P_r\) the simulator \(\tilde{\mathcal S}\) passes \(\perp\) (as \(P\)'s input) to \(\delta\).
							</p>
							<p class="rp_original rp_sub">
								Let \(\tilde\pi'\) be a non-erasing protocol for \(\tilde\pi\) and \(\tilde{\mathcal A}\) be as above (i.e. an arbitrary real-life adversary in the computational setting). We claim that \(\tilde{\mathcal S}^\tilde{\mathcal A}\) (i.e. the ideal-model adversary \(\tilde{\mathcal S}\) with black box access to \(\tilde{\mathcal A}\)) properly simulates the execution of \(\tilde\pi'\). We need to show that for any adversary \(\tilde{\mathcal A}\) and for any input \(\vec x\) we have $$\operatorname{IDEAL}_{f,\tilde{\mathcal S}^\tilde{\mathcal A}}(\vec x)\stackrel{c}{\approx}\operatorname{EXEC}_{\tilde\pi',\tilde{\mathcal A}}(\vec x).$$ Here we present only a rought sketch of the proof of this claim. The plan is to construct a real-life adversary \(\mathcal A\) in the secure channels setting, and prove the following sequence of equalities by which the above claim follows: $$\operatorname{IDEAL}_{f,\tilde{\mathcal S}^\tilde{\mathcal A}}(\vec x)\stackrel{d}{=}\operatorname{IDEAL}_{f,\mathcal{S}^\mathcal{A}}(\vec x)\stackrel{d}{=}\operatorname{EXEC}_{\pi',\mathcal A}(\vec x)\stackrel{c}{\approx}\operatorname{EXEC}_{\pi',\tilde{\mathcal A}}(\vec x).$$ Regardless of what \(\mathcal A\) is, the second equality follows immediately from the hypothesis that \(\mathcal S\) is a simulator for \(\pi'\) (the non-erasing protocol for \(\pi\)) <i>in the secure channels setting</i>. It remains to construct \(\mathcal A\) so that the other two equalities hold.
							</p>
							<p class="rp_original rp_sub">
								The real-life adversary \(\mathcal A\) of the secure channel setting will operate via a simulation of \(\tilde{\mathcal A}\) (the real-life adversary of the computational setting), imitating the simulation carried out by \(\tilde{\mathcal S}\). That is, for each bit communicated by \(\pi\), machines \(\mathcal A\) will invoke a copy of \(\delta\) while emulating an adversary in accordance with \(\tilde{\mathcal A}\). In particular, \(\tilde{\mathcal A}\) will be given all ciphertexts sent in the open as well as al internal data of corrupted parties (Regardless if these parties were corrupted before, during, or after the 'real' transmission). Furthermore, when \(\tilde{\mathcal A}\) corrupts a party \(P\), machine \(\mathcal A\) corrupts \(P\) and hands \(\tilde{\mathcal A}\) the internal data of \(P\), along with the outputs of the relevant copies \(\delta\), just as \(\tilde{\mathcal S}\) does. At the end of the computation \(\mathcal A\) outputs whatever \(\tilde{\mathcal A}\) outputs (that is, \(\mathcal A\) outputs \(\tilde{\mathcal A}\)'s view of the computation). It folows from the definition of \(\mathcal A\) that the execution of \(\mathcal S\), with black-box access to \(\mathcal A\), is in fact identical to the execution of \(\tilde{\mathcal S}\) with black-box access to \(\tilde{\mathcal A}\). Thus, \(\operatorname{IDEAL}_{f,\tilde{\mathcal S}^\mathcal{A}}(\vec x)\stackrel{d}{=}\operatorname{IDEAL}_{f,\mathcal{S}^\mathcal{A}}(\vec x)\) which establishes the first equality in the previous equation.
							</p>
							<p class="rp_original rp_sub">
								It remains to show that \(\operatorname{EXEC}_{\pi',\mathcal A}(\vec x)\stackrel{c}{\approx}\operatorname{EXEC}_{\tilde\pi',\tilde{\mathcal A}}(\vec x)\). Essentially the difference between these two executions is that \(\operatorname{EXEC}_{\pi',\mathcal A}(\vec x)\) is a real-life execution in the secure channel setting which is augmented by invocation of \(\delta\) (performed by \(\mathcal A\)), whereas \(\operatorname{EXEC}_{\tilde\pi',\tilde{\mathcal A}}(\vec x)\) is a real-life execution in the computational-setting in which honest parties use the encryption protocol \(\epsilon'\). However, the security of \(\epsilon\) means that invocations of \(\delta\) are indistinguishable from executions by \(\epsilon'\) (both in presence of adaptive adversaries). Using induction on the number of rounds, one thus establishes the last equality of the above equation.
							</p>
						</section>
						<section id="sec4.2">
							<h2>4.2. Constructing Non-Committing Encryption</h2>
							<p class="rp_original">
								Before describing our non-committing encryption protocol, let us note that one-time pad is a valid non-committing encryption protocol.<sup class="footnote" id="fref11" data-footnote="11"><a href="#footnote11">11</a></sup> The drawback of this trivial solution is that it requires an initial setup in which each pair of parties share a random string of length at least the number of bits they need to exchange. Such an initial setup is no desirable in practice and does not resolve the theoretically important problem of dealing with a setting in which <i>no secret information is shared a-priori</i>.
							</p>
							<p class="rp_original">
								Our scheme uses a collection of <span class="definable" data-define="trapdoor permutation">trapdoor permutations</span> together with a corresponding hardcore predicate<sup class="reference" data-citation="BM">[?]</sup><sup class="reference" data-citation="Y">[?]</sup><sup class="reference" data-citation="GrL">[?]</sup>. Actually, we need a collection of trapdoor permutations with the additional property that they are many permutations over the same domain. Furthermore, we assume that given a permutation \(f\) over a domain \(D\) (but not \(f\)'s trapdoor), one can efficiently generate at random another permutation \(f'\) over \(D\) together with the trapdoor of \(f'\). Such a collection is called a <b>common-domain trapdoor system.</b>
							</p>
							<span class="rp_definition_header" id="def4.3">Definition 4.3</span>
							<p class="rp_original rp_definition">
								A <b>common-domain trapdoor system</b> is an infinite set of ifnite permutations \(\{f_{\alpha,\beta}\,:\,D_\alpha\stackrel{1-1}{\mapsto}D_\alpha\}_{(\alpha,\beta)\in P}\), where \(P\subseteq\{0,1\}^*\times\{0,1\}^*\), so that:
							</p>
							<ul class="rp_original rp_definition">
								<li><b>domain selection:</b> There exists a probabilistic polynomial-time algorithm \(G_1\) so that on input \(1^n\) algorithm \(G_1\) outputs a description \(\alpha\in\{0,1\}^n\) of domain \(D_\alpha\).</li>
								<li><b>function selection:</b> There exists a probabilistic polynomial-time algorithm \(G_2\) so that on input \(\alpha\), algorithm \(G_2\) outputs a pair \((\beta,t(\beta))\) so that \((\alpha,\beta)\in P\). (\(\beta\) is a description of a permutation over \(D_\alpha\) and \(t(\beta)\) is the corresponding trapdoor.)</li>
								<li><b>domain sampling:</b> There exists a probabilistic polynomial-time algorithm \(S\) that on input \(\alpha\), uniformly selects an element of \(D_\alpha\).</li>
								<li><b>function evaluation:</b> There exists a probabilistic polynomial-time algorithm \(F\) that on inputs \((\alpha,\beta)\in P\) and \(x\in D_\alpha\) returns \(f_{\alpha,\beta}(x)\).</li>
								<li><b>function inversion:</b> There exists a polynomial-time algorithm \(I\) that on inputs \((\alpha,t(\beta))\) and \(y\in D_\alpha\), where \((\alpha,\beta)\in P\), returns \(f^{-1}_{\alpha,\beta}(y)\).</li>
								<li><b>one-wayness:</b> For any probabilistic polynomial-time algorithm \(A\), the probability that on input \((\alpha,\beta)\in P\) and \(y=f_{\alpha,\beta}(x)\), algorithm \(A\) outputs \(x\) is negligible (in \(n\)), where the probability distribution is over the random choices of \(\alpha=G_1(1^n),\,\beta=G_2(\alpha),\,x=S(\alpha)\) and the coin tosses of algorithm \(A\).</li>
							</ul>
							<span class="rp_sub_header">Remarks</span>
							<ul class="rp_original rp_sub">
								<li>The standard definitions of trapdoor permutations can be derived from the above by replacing the two selection algorithms, \(G_1\) and \(G_2\), by a single algorithm \(G\) that on input \(1^n\) generates a pair \((\beta,t(\beta))\) so that \(\beta\) specifies a domain \(D_\beta\) as well as a permutation \(f_\beta\) over this domain (and \(t(\beta)\) is \(f_\beta\)'s trapdoor). Thus, the standard definition does not guarantee any structural resemblance among domains of different permutations. Furthermore, it does not allow to generate a new permutation with corresponding trapdoor for a given domain (or given permutation). Nevertheless some popular trapdoor permutations can be formulated in a way which essentially meets the requirements of a common-domain trapdoor system.</li>
								<li>Common-domain trapdoor systems can be constructed based on an arbitrary family of trapdoor permutations, \(\{f_\beta\,:\;D_\beta\stackrel{1-1}{\mapsto}D_\beta\}\), with the extra property that the domain of any permutation, generated on input \(1^n\), has non-negligible inside \(\{0,1\}^n\) (i.e. \(|D_\beta|\geq\frac{1}{\operatorname{poly}(|\beta|)}\cdot2^{|\beta|}\)). We construct a common-domain family where the domain is \(\{0,1\}^n\) and the permutations are natural extensions of the given permutations. That is, we let \(G_1(1^n)=1^n,\,G_2(1^n)=G(1^n)\) and extends \(f_\beta\) into \(g_\beta\) so that \(g_\beta(x)=f_\beta(x)\) if \(x\in D_\beta\) and \(g_\beta(x)=x\) otherwise. This yields a collection of "common-domain" permutations, \(\{g_\beta\,:\;\{0,1\}^{|\beta|}\stackrel{1-1}{\mapsto}\{0,1\}^{|\beta|}\}\), which are weakly one-way. Employing amplification techniques (e.g. <span class="reference" data-citation="Y">[?]</span><span class="reference" data-citation="GILVZ">[?]</span>) we obtain a proper common-domain system.</li>
							</ul>
							<p class="rp_original">
								In the sequel we refer to common-domain trapdoor systems in a less formal way. We say that two one-way permutations, \(f_a\) and \(f_b\), are a <b>pair</b> if they are both permutations over the same domain (i.e. \(a=(\alpha,\beta_1)\) and \(b=(\alpha,\beta_2)\), where the domain is \(D_\alpha\)). We associate the permutations with their descriptions (and the corresponding inverse permutations with their trapdoors). Finally, as stated above, we augment any common-domain trapdoor system with a hardcore predicate, denoted \(B\). (That is, \(B\) is a polynomial-time computable, but give (\(f_a\) and) \(f_a(x)\) it is infeasible to predict \(B(x)\) with non-negligible advantage over &frac12;.)
							</p>
							<span class="rp_sub_header">Outline of Our Scheme</span>
							<p class="rp_original rp_sub">
								The scheme consists of two stages. In this first stage, called the <b>key generation</b> stage, the parties arrive at a situation where the sender has two trapdoor permutations \(f_a,\,f_b\) of a common-domain system, the trapdoor of only <i>one</i> of which is known to the receiver. Furthermore, the simulator will be able to generate, in a simulated execution of the protocol, two trapdoor permutations with the same distribution as in a real execution and such that the trapdoors of both permutations are known. (The simulator will later open dummy ciphertexts as either '0' or '1' by claiming the decryption key held by the receiver is either \(f_a^{-1}\) or \(f_b^{-1}\). The correspondence between \(\{0,1\}\) and \(\{a,b\}\) will be chosen at random by the simulator and never revealed). The key generation stage is independent of the bit to be transmitted (and can be performed before this bit is even determined).
							</p>
							<p class="rp_original rp_sub">
								Our most general implementation of this stage, based on any common-domain system, requires participation of all parties. It is described in <a href="#sec4.2.2">Section 4.2.2</a>. In the implementations based on the RSA and DH assumptions (see <a href="#sec4.3">Section 4.3</a>) the key-generation stage of only one message sent from the receiver to the sender.
							</p>
							<p class="rp_original rp_sub">
								The second stage, in which the actual transmission takes place, consists of only one message sent from the sender to the receiver. This stage consists of <b>encryption</b> and <b>decryption</b> algorithms, invoked by the sender and the receiver respectively.
							</p>
							<p class="rp_original rp_sub">
								We first present, in <a href="#sec4.2.1">Section 4.2.1</a>, the encryption and decryption algorithms as well as observations that will be instrumental for the simulation. In <a href="#sec4.2.2">Section 4.2.2</a> we present the key generation protocol. (A reader that is satisfied with a construction based on specific number theoretic assumptions may, for simplicity, skip Section 4.2.2 and read <a href="#sec4.3">Section 4.3</a> instead.) Finally we show that these together constitute the desired non-committing encryption protocol.
							</p>
							<section id="sec4.2.1">
								<h4>4.2.1. Encryption and Decryption</h4>
								<p class="rp_original">
									Let \(f_a\) and \(f_b\) be two randomly selected permutations over the domain \(D\), and let \(B\) be a hardcore predicate associated with them. The scheme uses a security parameter, \(k\), which can be thought to equal \(log_2|D|\).
								</p>
								<span class="rp_sub_header">Encryption</span>
								<p class="rp_original rp_sub">
									To encrypt a bit \(\sigma\in\{0,1\}\) with encryption key \((f_a,f_b)\), the sender proceeds as follows. First it chooses \(x_1,\dots,x_{8k}\) at random from \(D\), so that \(B(x_i)=\sigma\) for \(i=1,\dots,5k\) and \(B(x_i)=1-\sigma\) otherwise (i.e. for \(i=5k+1,\dots,8k\)). For each \(x_i\) it computes \(y_i=f_a(x_i)\). These \(x_i\)'s (and \(y_i\)'s) are <b>associated with \(f_a\)</b> (or with \(a\)). Next it repeats the process with respect to \(f_b\). That is, \(x_{8k+1},\dots,x_{16k}\) are chosen at random from \(D\), so that \(B(x_i)=\sigma\) for \(i=8k+1,\dots,13k\) and \(B(x_i)=1-\sigma\) otherwise, and \(y_i=f_b(x_i)\) for \(i=8k+1,\dots,16k\). The latter \(x_i\)'s (and \(y_i\)'s) are <b>associated with \(f_b\)</b> (or with \(b\)). Finally, the sender applies a random re-ordering (i.e. permutation) \(\phi\,:\;[16k]\rightarrow[16k]\) to \(y_1,\dots,y_{16k}\) and send the resulting vector, \(y_{\phi(1)},\dots,y_{\phi(16k)}\), to the receiver.
								</p>
								<span class="rp_sub_header">Decryption</span>
								<p class="rp_original rp_sub">
									Upon receiving the ciphertext \(y_1,\dots,y_{16k}\), when having private key \(f^{-1}_r\) (where \(r\in\{a,b\}\)), the receiver computes \(B(f^{-1}_r(y_1)),\dots,B(f_r^{-1}(y_{16k}))\), and outputs the majority value among these bits.
								</p>
								<section id="sec4.2.1.1">
									<h5>Correctness of decryption</h5>
									<p class="rp_original">
										Let us first state a simple technical claim.
									</p>
									<span class="rp_theorem_header" id="claim4.4">Claim 4.4</span>
									<p class="rp_original rp_theorem">
										For all but a negligible fraction of the \(\alpha\)'s and all but a negligible fraction of permutation pairs \(f_a\) and \(f_b\) over \(D_\alpha\), $$\left|\operatorname{Pr}(B(f_b^-1(f_a(x)))=B(x))-\frac 1 2\right|\text{ is negligible}\quad\quad(2)$$ where the probability is taken uniformly over the choices of \(x\in D_\alpha\).
									</p>
									<span class="rp_sub_header">Proof</span>
									<p class="rp_original rp_sub">
										Assume for the sake of contradiction that the claim does not hold. Then, without loss of generality, there exists a positive polynomial \(p\) so that for infinitely many \(n\)'s, we have $$\operatorname{Pr}\left(\left|\left\{y\in D_\alpha\,:\,B\left(f_b^{-1}(y)\right)\,=\,B\left(f_a^{-1}(y)\right)\right\}\right|\;\gt\;\left(\frac 1 2+\frac{1}{p(n)}\right)\cdot |D_\alpha|\right)\;\gt\;\frac{1}{p(n)}$$ when \(f_a\) and \(f_b\) are independently generated from \(\alpha=G_1(1^n)\). This means that for these \((\alpha,a,b)\)'s \(B(f_a^{-1}(y))\) gives a non-trivial prediction for \(B(f_b^{-1}(y))\). Intuitively this cannot be the case and indeed this leads to a contradiction as follows.
									</p>
									<p class="rp_original rp_sub">
										Given \(a=(\alpha,\beta)\in P\) and \(y\in D_\alpha\) we may predict \(B(f_a^{-1}(y))\) as follows. First we randomly generate a new permutation. \(f_b\), over \(D_\alpha\), together with its trapdoor. Next we test to see if indeed \(B(f_a^{-1}(z))\) is correlated with \(B(f_b^{-1}(z))\). (The testing is done by uniformly selecting polynomially many \(x_i\)'s in \(D_\alpha\), computing \(z_i=f_a(x_i)\), and comparing \(B(f_a^{-1}(z_i))=B(x_i)\) with \(B(f_b^{-1}(z_i))\).) If a non-negligible correlation is detectes then we output \(B(f_b^{-1}(y))\) (as our prediction for \(B(f_a^{-1}(y))\)). Otherwise we output a uniformly selected bit. (Note that \(\left|\operatorname{Pr}\left(B(x)=1\right)-\frac 1 2\right|\) must be negligible otherwise a constant function contradicts the hardcore hypothesis.)
									</p>
									<p class="rp_original">
										From this point on, we assume that the pair \((f_a,f_b)\) satisfies Equation 2.
									</p>
									<span class="rp_theorem_header" id="lemma4.5">Lemma 4.5</span>
									<p class="rp_original rp_theorem">
										Let \(\vec y=y_1,\dots,y_{16k}\) be a random encryption of a bit \(\sigma\). Then with probability \(1-2^{-\operatorname\Omega(k)}\) the bit decrypted from \(\vec y\) is \(\sigma\).
									</p>
									<span class="rp_sub_header">Proof</span>
									<p class="rp_original rp_sub">
										Assume without loss of generality that the private key is \(f_a^{-1}\). Then the receiver outputs the majority value of the bits \(B(f_a^{-1}(y_1)),\dots,B(f_a^{-1}(y_{16k}))\). Recall that \(8k\) of the \(y_i\)'s are associated with \(f_a\). Out of them, \(5k\) (of the \(y_i\)'s) satisfy \(B(f_a^{-1}(y_1))=B(x_i)=\sigma\), and \(3k\) satisfy \(B(f_a^{-1}(y_i))=B(x_i)=1-\sigma\). Thus the receiver outputs \(1-\sigma\) only if at least \(5k\) out of the rest of the \(y_i\)'s (that is, the \(y_i\)'s associated with \(f_b\)) satisfy \(B(f_a^{-1}(y_i))=1-\sigma\). However, Eq2 implies that \(|\operatorname(Pr)(B(f_a^{-1}(y_i))=\sigma)-\frac 1 2|\) is negligible for each \(y_i\) associated with \(f_b\). Thus only an expected \(4k\) of the \(y_i\)'s associated with \(f_b\) satisfy \(B(f_a^{-1}(y_i))=1-\sigma\). Using a large deviation bound, it follows that decryption errors occur with probability \(2^{-\operatorname\Omega(k)}\).
									</p>
								</section>
								<section id="sec4.2.1.2">
									<h5>Simulation Assuming Knowledge of Both Trapdoors</h5>
									<p class="rp_original">
										In <a href="#lemma4.7">Lemma 4.7</a> (below) we show how the simulator, knowing the trapdoors of both \(f_a\) and \(f_b\), can generate "dummy ciphertexts" \(\vec z=z_1,\dots,z_{16k}\) that can be later "opened" as encryptions of both 0 and 1. Essentially, the values \(B(f_a^{-1}(z_i))\) and \(B(f_b^{-1}(z_i))\) for each \(z_i\) are carefully chosen so that this "cheating" is possible. We use the following notations. Fix an encryption key (\(f_a,f_b\)). Let the random variable \(\lambda_\sigma=(\sigma;\vec x,\phi;\vec y;r,f_r^-1)\) describe a <i>legal encryption and decryption process</i> of the bit \(\sigma\). That is: 
									</p>
									<ul class="rp_original">
										<li>\(\vec x=x_1,\dots,x_{16k}\) is a vector of domain elements chosen at random as specified in the encryption algorithm.</li>
										<li>\(\phi\) is a random permutation on \([16k]\).</li>
										<li>\(\vec y=y_1,\dots,y_{16k}\) is generated from \(\vec x\) and \(\phi\) as specified in the encryption algorithm.</li>
										<li>\(r\) is uniformly chosen in \(\{a,b\}\) and \(f_r^{-1}\) is the inverse of \(f_r\). (Note that the decrypted bit is defined by the majority of the bits \(B(f_r^{-1}(y_i)\).)</li>
									</ul>
									<p class="rp_original">
										We remark that the information seen by the adversary, after the sender and receiver are corrupted, includes either \(\lambda_0\) or \(\lambda_1\) (but not both).
									</p>
									<p class="rp_original">
										Let us first prove a simple technical claim, that will help us in proving <a href="#lemma4.7">Lemma 4.7</a>. Let \(\operatorname{BIN}_m\) denote the binomial distribution over \([m]\).
									</p>
									<span class="rp_sub_header">Claim 4.6</span>
									<p class="rp_original rp_sub">
										There exists an efficiently samplable distribution \(\mu\) over \(\{0,1,\dots,4k\}\) so that the distribution \(\widetilde\mu\) construted by sampling an integer from \(\mu\) and adding \(2k\) is statistically close to \(\operatorname{BIN}_{8k}\). That is, the statistical distance between \(\tilde\mu\) and \(\operatorname{BIN}_{8k}\) is \(2^{-\Omega(k)}\).
									</p>
									<span class="rp_sub_header">Proof</span>
									<p class="rp_original rp_sub">
										Let \(\operatorname{BIN}_{8k}(i)\) denote the probability of \(i\) under \(\operatorname{BIN}_{8k}\) (i.e. \(\operatorname{BIN}_{8k}(i)=\binom{8k}{i}\cdot 2^{-8k}\)). We construct the distribution  \(\mu\) (over \(\{0,1,\dots,4k\}\) so that \(\operatorname{Pr}[\mu=i]=\operatorname{BIN}_{8k}(i+2k)\) for \(i=1,\dots,4n\) and \(\operatorname{Pr}(\mu=0)\) equals the remaining mass of \(\operatorname{BIN}_{8k}\) (i.e. equals \(\sum\nolimits_{i=0}^{2k}\operatorname{BIN}_{8k}(i)+\sum\nolimits_{i=6k+1}^{8k}\operatorname{BIN}_{8k}(i)\)).
									</p>
									<p class="rp_original rp_sub">
										It can be easily seen that each \(i\in\{2k+1,\dots,6k\}\) occurs under \(\tilde\mu\) with exactly the same probability as under \(\operatorname{BIN}_{8k}\). Integers \(i\) such that \(i\lt2k\) or \(i\gt6k\) have probability 0 under \(\tilde\mu\) (whereas \(2k\) is more likely to occur under \(\mu\) than under \(\operatorname{BIN}_{8k}\)). Thus, the statistical distance between \(\tilde\mu\) and \(\operatorname{BIN}_{8k}\) equals the probability, under \(\operatorname{BIN}_{8k}\), that \(i\) is smaller than \(2k\) or larger than \(6k\). This probability is bounded by \(2^{-\Omega(k)}\).
									</p>
									<span class="rp_theorem_header" id="lemma4.7">Lemma 4.7</span>
									<p class="rp_original rp_theorem">
										Let (\(f_a,f_b\)) be the public key, and assume that both \(f_a^{-1}\) and \(f_b^{-1}\) are known. Then it is possible to efficiently generate \(\vec z,\vec x^{(0)},\vec x^{(1)},\phi^{(0)},\phi^{(1)},r^{(0)},r^{(1)}\), such that:
									</p>
									<ul class="rp_original rp_theorem">
										<li>\((0;\vec x^{(0)},\phi^{(0)};\vec z;r^{(0)},f_{r^{(0)}}^{-1})\stackrel{c}{\approx}\lambda_0.\)</li>
										<li>\((1;\vec x^{(1)},\phi^{(1)};\vec z;r^{(1)},f_{r^{(1)}}^{-1})\stackrel{c}{\approx}\lambda_1.\)</li>
									</ul>
									<p class="rp_original rp_theorem">
										Here \(\stackrel{c}{\approx}\) stands for 'computationally indistinguishable'. We stress that the <i>same</i> <b>dummy ciphertext</b>, \(\vec z\), appears in both (1) and (2).
									</p>
									<span class="rp_sub_header">Proof</span>
									<p class="rp_original rp_sub">
										Before describing how the dummy ciphertext \(\vec z\) and the rest of the data are constructed, we summarize in Figure 1, the distribution of the <span class="definable" data-define="hardcore bit">hardcore bits</span>, \(B(f_a^{-1}(Y_1)),\dots,b(F_a^{-1}(y_{16k}))\) and \(B(f_b^{-1}(y_1)),\dots,B(f_b^{-1}(y_{16k}))\), with respect to a real encryption \(y_{\phi(1)},\dots,y_{\phi(16k)}\) of the bit \(\sigma=0\). Here \(\widetilde{BIN}_{8k}\) denotes the distribution of the number of 1s in \(B(f_b^{-1}(y_i))\) for \(i=1,\dots,8k\). Eq. (2) implies that the statistical difference between \(\operatorname{BIN}_{8k}\) and \(\widetilde{BIN}_{8k}\) is negligible. The distribution of \(B(f_a^{-1}(y_i))\) for \(i=8k+1,\dots,16k\) is similar. Given only \(\lambda_0\) (or only \(\lambda_1\)), only three-quarters of the \(B(f^{-1}_s(y_i))\)'s, \(i\in[16k]\) and \(s\in\{a,b\}\), are known. Specifically, consider \(\lambda_\sigma=(\sigma;\vec x,\phi;\vec y;r,f_r^{-1})\), and suppose that \(r=a\). Then all the \(B(f_a^{-1}(y_i))\)'s can be computed using \(f_a^{-1}\). In addition, for \(i=8k+1,\dots,16k,\,B(f_b^{-1}(y_i))=B(x_i)\) is known too. However, for \(i\in[8k],\,B(f_b^{-1}(y_i))=B(f_b^{-1}f_a(x_i))\) is not known and in fact it is (computationally) unpredictable (from \(\lambda_\sigma\)). A similar analysis holds for \(r=b\); in this case the unpredictable bits are \(B(f_a^{-1}(y_i))=B(f_a^{-1}f_b(x_i))\) for \(i=8k+1,\dots,16k\).
									</p>
									<table style="width:50%;margin:5px auto; border-collapse:collapse; border:1px solid">
										<tr>
											<th></th>
											<th>\(I=\{1,\dots,8k\}\)</th>
											<th>\(I=\{8k+1,\dots,16k\}\)</th>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\forall\,i\in I\)</td>
											<td style="border-right:1px solid">\(y_i=f_a(x_i)\)</td>
											<td>\(y_i=f_b(x_i)\)</td>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\nolimits_{i\in I}B(f_a^{-1}(y_i))=\)</td>
											<td style="border-right:1px solid">\(3k\)</td>
											<td>\(\widetilde{BIN}_{8k}\)</td>
										</tr>
										<tr>
											<td style="border-right:1px solid">\(\sum\nolimits_{i\in I}B(f_b^{-1}(y_i))=\)</td>
											<td style="border-right:1px solid">\(\widetilde{BIN}_{8k}\)</td>
											<td>\(3k\)</td>
										</tr>
									</table>
									<p class="rp_original">
										Figure 1: The distribution of the \(B(f_s^{-1}(y_i))\)'s with respect to \(\lambda_0\), where \(s\in\{a,b\}\). (The case of \(\lambda_1\) is similar, with the exception that \(5k\) replaces \(3k\).)
									</p>
									<span class="rp_sub_header">Initial Construction and Conditions</span>
									<p class="rp_original rp_sub">
										Keeping the structure of \(\lambda_\sigma\) in mind, we construct \(\vec z\), along with \(\vec x^{(0)},\vec x^{(1)},\phi^{(0)},\phi^{(1)},r^{(0)} and r^{(1)}\) as follows. First, we select uniformly a bijection, \(\rho\), of \(\{0,1\}\) to \(\{a,b\}\) (i.e. either \(\rho(0)=a\) and \(\rho(1)=b\) or the other way around) and set \(r^{(0)}=\rho(0)\) and \(r^{(1)}=\rho(1)\). Next, we choose, in the way described below, two binary vectors \(\vec\gamma^{(0)}=\gamma_1^{(0)},\dots,\gamma_{16k}^{(0)}\) and \(\vec\gamma^{(1)}=\gamma_1^{(1)},\dots,\gamma_{16k}^{(1)}\). We choose random values \(v_1,\dots,v_{16k}\) such that \(\gamma_i^{(0)}=B(f^{-1}_{\rho(0)}(v_i))\) and \(\gamma_i^{(1)}=B(f^{-1}_{\rho(1)}(v_i))\), for each \(i\in[16k]\). We uniformly select a permutation \(\psi\) over \([16k]\) and let the permuted vector \(v_{\psi(1)},\dots,v_{\psi(16k)}\) be the dummy ciphertext \(\vec z=(z_1,\dots,z_{16k})\). It remains to determine \(\phi^{(0)}\) and \(\phi^{(1)}\), which in turn determine \(\vec x^{(0)}\) and \(\vec x^{(1)}\) so that \(x_i^{(\sigma)}=f_a^{-1}(z_{(\phi^{(\sigma)})^{-1}(i)})\) for \(i\in[8k]\) and \(x_i^{(\sigma)}=f_b^{-1}(z_{\phi^{(\sigma)}(i)})\) otherwise. This should be done so that both permutations \(\phi^{(0)}\) and \(\phi^{(1)}\) are uniformly but not necessarily independently) distributed and so that the known \(B(f_s^{-1}(y_i^{(\sigma)}))\)'s match the distribution seen in a legitimate encryption of \(\sigma\). We stress that \((\sigma;\vec x^{(\sigma)},\phi^{(\sigma)};\vec z;r^{(\sigma)},f_{r^{(\sigma)}}^{-1})\) should appear as a valid encryption of \(\sigma\). In particular, for each \(\sigma\in\{0,1\}\) there should exist a permutation \(\psi^{(\sigma)}\,(=(\phi^{(\sigma)})^{-1}\circ\phi)\) over \([16k]\) so that<sup class="footnote" id="fref12" data-footnote="12"><a href="#footnote12">12</a></sup>
									</p>
									<ol class="rp_original">
										<li>\(\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(i)}=B(f_a^{-1}(v_{\psi^{(\sigma)}(i)}))=B(f_a^{-1}(z_{\psi^{(\sigma)}(i)}))=B(x_i^{(\sigma)}=\sigma,\text{ for }i=1,\dots,5k.\) (E.g. if \(\rho(0)=a\) this means \(\gamma^{(0)}_{\psi^{(\sigma)}(i)}=\sigma.\))</li>
										<li>\(\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(i)}=B(f_a^{-1}(v_{\psi^{(\sigma)}(i)}))=B(f_a^{-1}(z_{\psi^{(\sigma)}(i)}))=B(x_i^{(\sigma)}=1-\sigma,\text{ for }i=5k+1,\dots,8k.\) (E.g. if \(\rho(0)=a\) this means \(\gamma^{(0)}_{\psi^{(\sigma)}(i)}=1-\sigma.\))</li>
										<li>\(\gamma^{(\rho^{-1}(b))}_{\psi^{(\sigma)}(i)}=B(f_b^{-1}(v_{\psi^{(\sigma)}(i)}))=B(f_b^{-1}(z_{\psi^{(\sigma)}(i)}))=B(x_i^{(\sigma)}=\sigma,\text{ for }i=8k+1,\dots,13k.\) (E.g. if \(\rho(0)=a\) this means \(\gamma^{(1)}_{\psi^{(\sigma)}(i)}=\sigma.\))</li>
										<li>\(\gamma^{(\rho^{-1}(b))}_{\psi^{(\sigma)}(i)}=B(f_b^{-1}(v_{\psi^{(\sigma)}(i)}))=B(f_b^{-1}(z_{\psi^{(\sigma)}(i)}))=B(x_i^{(\sigma)}=1-\sigma,\text{ for }i=13k+1,\dots,16k.\) (E.g. if \(\rho(0)=a\) this means \(\gamma^{(1)}_{\psi^{(\sigma)}(i)}=1-\sigma.\))</li>
										<li>Let \(I=[8k]\) if \(\rho(\sigma)=b\) and \(I=\{8k+1,\dots,16k\}\) otherwise. Then, \(\gamma^{(\sigma)}_{\psi^{(\sigma)}(i)}=B(f^{-1}_{\rho(\sigma)}(v_{\psi^{(\sigma)}(i)}))=B(f^{-1}_{\rho(\sigma)}(z_{\phi^{(\sigma)}(i)}))=B(f^{-1}_{\rho(\sigma)}(f_{\rho(1-\sigma)}(x_i^{(\sigma)})))\) equals \(\sigma\) with probability negligibly close to &frac12;, for \(i\in I\). (E.g. for \(\rho(0)=a\) and \(\sigma=0\) we have \(\operatorname{Pr}\left[\gamma^{(0)}_{\psi^{(\sigma)}(i)}\,=\,1\right]\approx\frac 1 2\) for \(i=8k+1,\dots,16k\), whereas for \(\rho(0)=a\) and \(\sigma=1\) we have \(\operatorname{Pr}\left[\gamma^{(1)}_{\psi^{(\sigma)}(i)}\,=\,1\right]\approx\frac 1 2\) for \(i=1,\dots,8k\).)</li>
									</ol>
									<p class="rp_original rp_sub">
										This allows setting \(\phi^{(\sigma)}=\psi\circ(\psi^{(\sigma)})^{-1}\) so that \(x^{(\sigma)}_{\phi^{(\sigma)}(i)}\) is "mapped" to \(z_i\) while \(\phi^{(\sigma)}\) is uniformly distributed (i.e. \(x_i^{(\sigma)}=f_a^{-1}(v_{\psi^{(\sigma)}(i)})=f_a^{-1}(z_{\psi^{-1}(\phi^{(\sigma)}(i))})=f_a^{-1}(z_{(\phi^{(\sigma)})^{-1}(i)})\text{ for }i\in[8k]\text{ and }x_i^{(\sigma)}=f_b^{-1}(z_{\phi^{(\sigma)}(i)})\) otherwise).
									</p>
									<span class="rp_sub_header">Initial Setting of \(\vec\gamma^{(0)},\vec\gamma^{(1)},\psi^{(0)}\text{ and }\psi^{(1)}\)</span>
									<p class="rp_original rp_sub">
										The key issue is how to select \(\vec\gamma^{(0)}\) and \(\vec\gamma^{(1)}\) so that the five condition stated above hold (for both \(\sigma=0\) and \(\sigma=1\)). As a first step towards this goal we consider the four sums $$S_1^\sigma\stackrel{def}{=}\sum\limits_{i=1}^{8k}\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(i)}\,,\quad S_2^\sigma\stackrel{def}{=}\sum\limits_{i=8k+1}^{16k}\gamma^{(\rho^{-1}(b))}_{\psi^{(\sigma)}(i)}$$$$S_3^\sigma\stackrel{def}{=}\sum\limits_{i=1}^{8k}\gamma^{(\rho^{-1}(b))}_{\psi^{(\sigma)}(i)}\,,\quad S_4^\sigma\stackrel{def}{=}\sum\limits_{i=8k+1}^{16k}\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(i)}$$ The above conditions imply \(S_1^\sigma=S_2^\sigma=5k\cdot\sigma+2k\cdot(1-\sigma)=3k+2k\sigma\) as well as \(S_3^\sigma\stackrel{d}{=}\widetilde{BIN}_{8k}\) if \(\rho(\sigma)=b\) and \(S_4^\sigma\stackrel{d}{=}\widetilde{BIN}_{8k}\). (Note that \(S_3^\sigma,S_4^\sigma\text{ and }\widetilde{BIN}^{8k}\) are random variables.)
									</p>
									<p class="rp_original rp_sub">
										To satisfy the above summation conditions we partition \([16k]\) into 4 equal sized subsets denoted \(I_1,I_2,I_3,I_4\) (e.g. \(I_1=[4k],I_2=\{4k+1,\dots,8k\},I_3=\{8k+1,\dots,12k\},I_4=\{12k+1,\dots,16k\}\)). This partition induces a similar partition on the \(\gamma_i^{(0)}\)'s and the \(\gamma_i^{(1)}\)'s. The \(\gamma_i^{(0)}\)'s and the \(\gamma_i^{(1)}\)'s in each set are chosen using four different distributions which satisfy the conditions summarized in Figure 2.
									</p>
									<p class="rp_annotation">
										Paragraph break not originally present.
									</p>
									<table style="width:50%;margin:5px auto; border-collapse:collapse; border:1px solid">
										<tr>
											<th></th>
											<th>\(I=I_1\)</th>
											<th>\(I=I_2\)</th>
											<th>\(I=I_3\)</th>
											<th>\(I=I_4\)</th>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(0)}\stackrel{d}{=}\)</td>
											<td style="border-right:1px solid">\(3k\)</td>
											<td style="border-right:1px solid">0</td>
											<td style="border-right:1px solid">\(2k\)</td>
											<td>\(\mu\)</td>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(1)}\stackrel{d}{=}\)</td>
											<td style="border-right:1px solid">\(\mu\)</td>
											<td style="border-right:1px solid">\(4k\)</td>
											<td style="border-right:1px solid">\(2k\)</td>
											<td>\(k\)</td>
										</tr>
									</table>
									<p class="rp_original">
										Figure 2: The distribution of the \(\gamma^{(0)}\)'s and \(\gamma^{(1)}\)'s. (\(\mu\) is as in Claim 4.6.)
									</p>
									<p class="rp_original rp_sub">
										Suppose \(\rho(0)=a\). Then we may set \(\psi^{(0)}([8k])=I_1\cup I_2\) and \(\psi^{(0)}(\{8k+1,\dots,16k\})=I_3\cup I_4\), and  \(\psi^{(1)}([8k])=I_1\cup I_3\) and \(\psi^{(1)}(\{8k+1,\dots,16k\})=I_2\cup I_4\), where \(\pi(I)=J\) means that the permutation \(\pi\) maps the elements of the set \(I\) onto the set \(J\). (It would have been more natural but less convenient to write \((\psi^{(I)})^{-1}(I_1\cup I_3)=[8k]\text{ and }(\psi^{(1)})^{-1}(I_2\cup I_4)=\{8k+1,16k\}\).) We claim that, for each \(\sigma\in\{0,1\}\), the above setting satisfies the three relevant summation conditions. Consider, for example, the case \(\sigma=0\) (depicted in Figure 3).
									</p>
									<p class="rp_annotation">
										Paragraph break not originally present.
									</p>
									<table style="width:50%;margin:5px auto; border-collapse:collapse; border:1px solid">
										<tr>
											<th></th>
											<th>\(I=\{1,\dots,8k\}=(\psi^{(0)})^{-1}(I_1\cup I_2)\)</th>
											<th>\(I=\{8k+1,\dots,16k\}=(\psi^{(0)})^{-1}(I_3\cup I_4)\)</th>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(0)}=\)</td>
											<td style="border-right:1px solid">\(S_1^0=3k+0=3k\)</td>
											<td>\(\S_4^0=2k+\mu\stackrel{d}{=}\widetilde{BIN}_{8k}\)</td>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(1)}=\)</td>
											<td style="border-right:1px solid">no condition</td>
											<td>\(S_2^0=2k+k=3k\)</td>
										</tr>
									</table>
									<p class="rp_original">
										Figure 3: Using \(\psi^{(0)}\) the \(\gamma_i^{(0)}\)'s and \(\gamma_i^{(1)}\)'s satisfy the summation conditions \(S_1^0,S_2^0\) and \(S_4^0\).
									</p>
									<p class="rp_original rp_sub">
										Then \(S_1^0=\sum\nolimits_{i=1}^{8k}\gamma_i^{(0)}=3k\) and \(S_2^0=\sum\nolimits_{i=8k+1}^{16k}\gamma_i^{(1)}=3k\) as required. Considering \(S_4^0=\sum\nolimits_{i=8k+1}^{16k}\gamma_i^{(0)}\) we observe that it is distributed as \(2k+\mu=\tilde\mu\) (of Claim 4.6) which in turn is statistically close to \(\widetilde{BIN}_{8k}\). We stress that the above argument holds for any way of setting the \(\psi^{(\cdot)}\)'s as long as they obey the equalities specified (e.g. for any bijection \(\pi\,:\;I_1\cup I_2\stackrel{1-1}{\mapsto}I_1\cup I_3\), we are allowed to set \(\psi^{(1)}=\pi(i)\) for all \(i\in I_1\cup I_2\)). The case \(\sigma=1\) follows similarly; here \(S_1^1=\sum\nolimits_{i\in I_1\cup I_3}\gamma_i^{(0)}=5k,\,S_2^1=\sum\nolimits_{i\in I_2\cup I_4}\gamma_i^{(1)}=5k\text{ and }S_3^1=\sum\nolimits_{i\in I_1\cup I_3}\gamma_i^{(1)}=\mu=2k\) (see Figure 4).
									</p>
									<p class="rp_annotation">
										Paragraph break not originally present.
									</p>
									<table style="width:50%;margin:5px auto; border-collapse:collapse; border:1px solid">
										<tr>
											<th></th>
											<th>\(I=\{1,\dots,8k\}=(\psi^{(0)})^{-1}(I_1\cup I_3)\)</th>
											<th>\(I=\{8k+1,\dots,16k\}=(\psi^{(0)})^{-1}(I_2\cup I_4)\)</th>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(0)}=\)</td>
											<td style="border-right:1px solid">\(S_1^1=3k+2k=5k\)</td>
											<td>no condition</td>
										</tr>
										<tr style="border-bottom:1px solid">
											<td style="border-right:1px solid">\(\sum\limits_{i\in I}\gamma_i^{(1)}=\)</td>
											<td style="border-right:1px solid">\(S_3^1=\mu=2k\stackrel{d}{=}\widetilde{BIN}_{8k}\)</td>
											<td>\(S_4^1=4k+k=5k\)</td>
										</tr>
									</table>
									<p class="rp_original">
										Figure 4: Using \(\psi^{(1)}\) the \(\gamma_i^{(0)}\)'s and \(\gamma_i^{(1)}\)'s satisfy the summation conditions \(S_1^1,S_2^1\) and \(S_3^1\).
									</p>
									<p class="rp_original rp_sub">
										In case \(\rho(0)=b\) we set \(\psi^{(0)}([8k])=I_3\cup I_4,\,\psi^{(0)}(\{8k+1,\dots,16k\})=I_1\cup I_2,\,\psi^{(1)}([8k])=I_2\cup I_4,\text{ and }\psi^{(1)}(\{8k+1,\dots,16k\})=I_1\cup I_3\). The claim that, for each \(\sigma\in\{0,1\}\), the above setting satisfies the three relevant summation conditions, is shown analogously.
									</p>
									<span class="rp_sub_header">Refinement of \(\vec\gamma^{(0)},\,\vec\gamma^{(1)},\,\psi^{(0)},\text{ and }\psi^{(1)}\)</span>
									<p class="rp_original rp_sub">
										However, the above summation conditions do not guarantee satisfaction of all the five conditions. In particular, we must use permutations \(\psi^{(\cdot)}\) which guarantee the correct positioning visible bits within the \(8k\)-bit long block. That is, we must have $$\begin{align}\left(\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(1)},\dots,\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(8k)}\right)&=\left(\sigma^{5k},(1-\sigma)^{3k}\right)\\\left(\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(8k+1)},\dots,\gamma^{(\rho^{-1}(a))}_{\psi^{(\sigma)}(16k)}\right)&=\left(\sigma^{5k},(1-\sigma)^{3k}\right)\end{align}$$ that is, equality between the sequences and not merely equality in the number of 1's. Clearly there is no problem to set the \(\psi^{(\cdot)}\)'s so that these equalities hold and thus Conditions (1) through (4) are satisfied. It is left to satisfy  Condition (5).
									</p>
									<p class="rp_original rp_sub">
										Suppose that \(\rho(\sigma)=a\). In this case, the third summation requirement guarantees \(\sum\nolimits_{i=8k+1}^{16k}\gamma^{(\sigma)}_{\psi^{(\sigma)}(i)}\stackrel{d}{=}\widetilde{BIN}_{8k}\). This is indeed consistent with the requirement that these \(\gamma^{(\sigma)}_{\psi^{(\sigma)}(i)}\)'s are almost uniformly and independently distributed. But this is not sufficient. In particular, we also need \(\sum_{i\in J}\gamma^{(\sigma)}_{\psi^{(\sigma)}(i)}\stackrel{d}{=}\widetilde{BIN}_{3k},\text{ where }J=\{8k\lt i\leq 16k\,:\;\gamma^{(1-\sigma)}_{\psi^{(\sigma)}(i)}=1-\sigma\}\) and furthermore the above sum needs to be independent of \(\sum\nolimits_{i\in\{8k+1,\dots,16k\}-J}\gamma_{\psi^{(\sigma)}(i)}^{(\sigma)}\) (which in turn should be statistically close to \(BIN_{5k}\)). Let us start with the case \(\sigma = 0\). In this case we need $$\sum\limits_{i\in J}\gamma_i^{(0)}\stackrel{d}=\widetilde{BIN}_{3k},\quad\quad(3)$$ where \(J\in\{i\in I_3\cup I_4\,:\,\gamma_i^{(1)}=1\}\), and this sum needs to be independent of \(\sum\nolimits_{i\in I_3\cup I_4-J}\gamma_i^{(0)}\). By Figure 2, we have \(|J\cap I_3|=2k\). We further restrict the distributions \(\gamma_i^{(0)}\)'s and \(\gamma_i^{(1)}\)'s so that in part \(I_3\) the four possible outcomes of the pairs \((\gamma_i^{(0)},\gamma_i^{(1)})\) are equally likely (e.g. for exactly \(k\) integers \(i\in I_3\) we have \((\gamma_i^{(0)},\gamma_i^{(1)})=(0,0)\). Consider \(J'=J\cap I_4\) (note \(|J'|=k\)). To satisfy Eq. (3) we construct a random variable \(\mu'\in\{0,1,\dots,k\}\) (analogously to Claim 4.6) so that \(p_j\stackrel{def}{=}\operatorname{Pr}[\mu'=j]=BIN_{3k}(k+j)\) for \(j\in[k]\) (with the rest of the mass on \(\mu'=0\)) and constrain the \(\gamma_i^{(0)}\)'s to satisfy \(\operatorname{Pr}\left[\sum\nolimits_{i\in J'}\gamma_i^{(0)}=j\right]=p_j\). We get \(\sum_{i\in J}\gamma_i^{(0)}=k+\mu'\stackrel{d}=\widetilde{BIN}_{3k}\) (analogously to claim 4.6). A minor problem occurs: the new restriction on the \(\gamma_i^{(0)}\)'s conditions \(\sum_{i\in I_4-J'}\gamma_i^{(0)}\) which we want to be distributed as some \(\mu''\stackrel{d}=BIN_{5k}-2k\) and independently of \(\mu'\) (the reason being that \(\mu'+\mu''\) should be distributed equally to \(\mu\)). However this condition has a negligible effect since we can sample \(\mu'\) and \(\mu\) and set the \(\gamma_i^{(0)}\)'s accordingly, getting into trouble only in case \(\mu\lt\mu'\) which happens with negligible probability (since \(\operatorname{Pr}[\mu\lt\mu']\lt\operatorname{Pr}[\mu\lt k]=2^{-\operatorname\Omega(k)}\).
									</p>
									<p class="rp_original rp_sub">
										The case \(\sigma=1\) gives rise to the requirement $$\sum\limits_{i\in J}\gamma_i^{(1)}\stackrel{d}=\widetilde{BIN}_{3k},\quad\quad(4)$$ where \(J=\{i\in I_1\cup I_3\,:\,\gamma_i^{(0)}=0\}\), and this sum needs to be independent of \(\sum\nolimits_{i\in I_1\cup I_3-J}\gamma_i^{(1)}\). To satisfy Eq. (4) we restrict the \(\gamma_i^{(1)}\)'s in \(J'\stackrel{def}=J\cap I_1\) analogously to satisfy \(\sum\nolimits_{i\in J'}\gamma_i^{(1)}=\mu'\). Finally we observe that generating the \(\gamma_i^{(0)}\)'s and \(\gamma_i^{(1)}\)'s at random so that they satisfy the above requirements makes them satisfy Condition (5).
									</p>
									<span class="rp_sub_header">Beyond the Five Conditions</span>
									<p class="rp_original rp_sub">
										In the above construction we have explicitly dealt with conditions which obviously have to hold for the construction to be valid. We now show that indeed this suffices. Namely, we claim that $$(\sigma;\vec x^{(\sigma)},\phi^{(\sigma)};\vec z;r^{(\sigma)},f^{-1}_{r^{(\sigma)}})\stackrel{c}{\approx}\lambda_\sigma=(\sigma;\vec x,\phi;\vec y;r,f_r^{-1}).\quad\quad(5)$$ Consider the case \(\sigma=0\). Both \(r^{(0)}\) and \(r\) are uniformly chosen in \(\{a,b\}\) and so we consider, <span class="definable">WLOG</span>, \(r=r^{(0)}=a\). Furthermore, \(\phi^{(0)}\) is a random permutation and \(f_a(x^{(0)}_1)=z_{\phi^{(0)}}\text{ for }i=1,\dots,8k,\text{ and }f_b(x^{(0)}_1)=z_{\phi^{(0)}}\text{ for }i=8k+1,\dots,16k\), which matches the situation w.r.t \(\phi,\vec x,\text{ and }\vec y\). It remains to compare the distributions of \(B(f_s^{-1}(\cdot))\)'s, \(s\in\{a,b\}\), with respect to \(\vec x^{(0)}\) and with respect to \(\vec x\). By the above analysis we know that the entries corresponding to \(s=a\) and to \((s=b)\wedge(i\leq8k)\) are distributed similarly in the two cases. Thus, we need to compare \(B(f_b^{-1}(f_a(x_1^{(0)}))),\dots,B(f_b^{-1}(f_a(x^{(0)}_{8k})))\) and \(B(f_b^{-1}(f_a(x_1))),\dots,B(f_b^{-1}(f_a(x_{8k})))\). Recall that the \(x_i\)'s are selected at random subject to \(B(x_i)=0\) for \(i=1,\dots,5k\text{ and }B(x_i)=1\text{ for }i=5k+1,\dots,8k\). An analogous condition is imposed on the \(x_i^{(0)}\)'s but in addition we also have \(B(f_b^{-1}(f_a(x_1^{(0)})))=1\) for \(i=1,\dots,4k\), and some complicated conditions on \(B(f_b^{-1}(f_a(x_1^{(0)})))=1\), for \(i=4k+1,\dots,8k\) (i.e. the distributions of 1's here is governed by \(\mu\) and furthermore in the first \(k\) elements the number of 1's is distributed identically to \(\mu'\). Thus distinguishing \(\vec x\) from \(\vec x^{(0)}\) amounts to distinguishing, given \(f_a,f_b\,:\;D\mapsto D\) and the trapdoor for \(f_a\) (but not for \(f_b\)), between the two distributions 
									</p>
									<ol class="rp_original rp_sub">
										<li>\((u_1,\dots,u_{8k})\), where the \(u_i\)'s are independently selected so that \(B(u_i)=0\) if \(i\in[5k]\) and \(B(u_i)=1)\) otherwise; and</li>
										<li>\((w_1,\dots,w_{8k})\), where the \(w_i\)'s are uniformly selected under the conditions
											<ul>
												<li>\(B(w_i)=0\text{ if }i=[5k]\text{ and }B(w_i)=1\) otherwise,</li>
												<li>\(B(f_b^{-1}(f_a(w_i)))=1\text{ for }i\in[4k]\)</li>
												<li>\(\sum\nolimits_{i=4k+1}^{5k}B(f_b^{-1}(f_a(w_i)))=\mu'\), and</li>
												<li>\(\sum\nolimits_{i=5k+1}^{8k}B(f_b^{-1}(f_a(w_i)))=\mu''\), for some \(\mu''\stackrel{d}=\mu-\mu'\).</li>
											</ul>
										</li>
									</ol>
									<p class="rp_original rp_sub">
										We claim that distinguishing these two distributions yields a contradiction to the security of the hardcore predicate \(B\). Suppose, on the contrary, that an efficient algorithm \(A\) can distinguish these two distributions. Using a hybrid argument we construct an argument \(A'\) which distinguishes the uniform distribution over \(D'\stackrel{def}=\{x\in D\,:\,B(x)=\tau\}\) and a distribution over \(D'\) that is uniform over both \(D'_0\stackrel{def}=\{x\in D'\,:\,B(f_b^{-1}(f_a(x)))=0\}\) and \(D'_1\stackrel{def}=\{x\in D'\,:\,B(f_b^{-1}(f_a(x)))=1\}\), where \(\tau\) is a bit which can be efficiently determined. (We stress that the latter distribution is not uniform on \(D'\) but rather uniform on each of its two parts.) Without loss of generality, we assume \(\tau=0\). It follows that \(A'\) must distinguish inputs uniformly distributed in \(D_0'\) from inputs uniformly distributed in \(D_1'\). We not transform \(A;\) into an algorithm, \(A''\), that distinguishes a uniform distribution over \(\{y\in D\,:\,B(f_b^{-1}(y))=0\}\) from a uniform distribution over \(\{y\in D\,:\,B(f_b^{-1}(y))=1\}\). On input \(y\in D_a\) and \(f_b\,:\;D\mapsto D\), algorithm \(A''\) first generates another permutation \(f_a\), over \(D\), together with the trapdoor for \(f_a\). Next, it computes \(x=f_a^{-1}(y)\) and stop (outputting 0) if \(B(x)=1\) (i.e. \(x\not\in D'\)). Otherwise, \(A''\), invokes \(A'\) on \(x\) and outputs \(A'(x)\). In this case \(B(f_b^{-1}(f_a(x)))=B(f_b^{-1}(y))\) (and \(B(x)=0\)) so the output will be significantly different in case \(B(f_b^{-1}(y))=0\) and in case \(B(f_b^{-1}(y))=1\). We observe that \(\operatorname{Pr}[B(x)=0]\approx\frac 12\) (otherwise a constant function violates the security of \(B\)), and conclude that one can distinguish a random \(y\) with \(B(f_b^{-1}(y))=0\) from a random \(y\) with \(B(f_b^{-1}(y))=1\) (which contradicts the security of \(B.\quad\blacksquare\).
									</p>
								</section>
							</section>
							<section id="sec4.2.2">
								<h4>4.2.2. Key Generation</h4>
								<p class="rp_original">
									We describe how the keys are generated, based on any common-domain trapdoor system. We use Oblivious Transfer<sup class="reference" data-citation="R">[?]</sup><sup class="reference" data-citation="EGL">[?]</sup> in our constructions. Oblivious Transfer (OT) is a protocol executed by a sender \(S\) with inputs \(s_1\) and \(s_2\), and by a receiver \(R\) with input \(\tau\in\{1,2\}\). After executing an OT protocol, the receiver should know \(s_\tau\), and learn nothing else. The sender \(S\) should learn nothing from participating in the protocol. In particular \(S\) should not know whether \(R\) learns \(s_1\) or \(s_2\). We are only concerned with the case where \(R\) is uncorrupted and non-erasing.
								</p>
								<p class="rp_original">
									We use the implementation of OT described in <span class="reference" data-citation="GMW">[?]</span> (which in turn originates in <span class="reference" data-citation="EGL">[?]</span>). This implementation has an additional property, discussed below, that is useful in our construction. For self containment we sketch, in Figure 5, the <span class="reference" data-citation="GMW">[?]</span> protocol for OT of one bit.
								</p>
								<p class="rp_original">
									It can be easily verified that the receiver outputs the correct value of \(\sigma_\tau\) in Step 4. Alsi, if the receiver is semi-honest in the non-erasing sense, then it cannot predict \(\sigma_{3-\tau}\) with more than negligible advantage over &frac12;.<sup class="footnote" id="fref13" data-footnote="13"><a href="#footnote13">13</a></sup> The sender view of the interaction is uncorrelated with the value of \(\tau\in\{1,2\}\). Thus it learns nothing from participating in the protocol.
								</p>
								<p class="rp_original">
									The important additional property of this protocol is that, in a simulated execution of the protocol, the simulator can learn both \(\sigma_1\) and \(\sigma_2\) by uniformly selecting \(z_1,z_2\in D\) and having the receiver \(R\) send \(f(z_1),f(z_2)\) (in Step 2). Furthermore, if \(R\) is later corrupted, then the simulator can "convince" the adversary that \(R\) received either \(\sigma_1\) or \(\sigma_2\), at wish, by claiming that in Step 2 party \(R\) chose either \((x_1,x_2)=(z_1,f(z_2))\text{ or }(x_1,x_2)=(f(z_1),z_2)\), respectively.
								</p>
								<p class="rp_original">
									In Figure 6 we describe our key generation protocol. This protocol is valid as long as at least one party remains uncorrupted.
								</p>
								<div style="border:1px solid black;width:85%;padding:2.5%;margin:10px auto;">
									<h3>Oblivious Transfer (OT)</h3>
									<p class="rp_original">
										The parties proceed as follows, using a trapdoor-permutations generator and the associated hardcore predicate \(B\).
									</p>
									<ol class="rp_original">
										<li>On input \(\sigma_1,\sigma_1\in\{0,1\}\), the sender generates a one-way trapdoor permutation \(f\,:\;D\mapsto D\) with its trapdoor \(f^{-1}\), and sends \(f\) to the receiver.</li>
										<li>On input \(\tau\in\{1,2\}\), the receiver selects \(x_1,x_2\in D\), computes \(y_\tau=f(x_\tau),\text{ sets }y_{3-\tau}=x_{3-\tau}\), and sends \((y_1,y_2)\) to the sender.</li>
										<li>Upon receiving \((y_1,y_2)\), the sender sends the pair \((\sigma_1\oplus B(f^{-1}(y_1)),\sigma_2\oplus B(f^{-1}(y_2)))\) to the receiver.</li>
										<li>Having received \((b_1,b_2)\), the receiver outputs \(s_\tau=b_\tau\oplus B(x_\tau)\) (as the message received).</li>
									</ol>
									<p class="rp_original">
										Figure 5: The <span class="reference" data-citation="GMW">[?]</span> Oblivious Transfer Protocol
									</p>
								</div>
								<div style="border:1px solid black;width:85%;padding:2.5%;margin:10px auto;">
									<h3>key-generation (\(\epsilon_G\))</h3>
									<p class="rp_original">
										For generating an encryption key \((f_a,f_b)\) known to the sender, and a decryption key \(f_r^{-1}\) known only to the receiver \(R\), where \(r\) is uniformly distributed in \(\{a,b\}\).
									</p>
									<ol class="rp_original">
										<li>The receiver generates a common domain \(D_\alpha\) and sends \(\alpha\) to all parties.</li>
										<li>Each party \(P_i\) generates two trapdoor permutations over \(D_\alpha\), denoted \(f_{a_i}\) and \(f_{b_i}\), and sends \((f_{a_i},f_{b_i})\) to \(R\). The trapdoors of \(f_{a_i}\) and \(f_{b_i}\) are kept secret by \(P_i\).</li>
										<li>The receiver \(R\) chooses uniformly \(\tau\in\{1,2\}\) and invokes the OT protocol with each party \(P_i\) for a number of times equal to the length of the description of the trapdoor of a permutation over \(\alpha\). In all invocations the receiver uses input \(\tau\). In the \(j\)<sup>th</sup> invocation of OT, party \(P_i\) acting as sender uses input \((\sigma_1,\sigma_2)\), where \(\sigma_1\) (resp. \(\sigma_2\)) is the \(j\)<sup>th</sup> bit of the trapdoor of \(f_{a_i}\) (resp. \(f_{b_i}\)). (Here we use the convention by which, without loss of generality, the trapdoor may contain all random choices made by \(G_2\) when generating the permutation. This allows \(R\) to verify the validity of the data received from \(P_i\).)</li>
										<li>Let \(H\) be the set of parties with which all the OTs were completed successfully. Let \(f_a\) be the composition of the permutations \(f_{a_i}\)'s for \(P_i\in H\), in some canonical order, and let \(f_b\) be defined analogously (e.g. \(a\) is the concatenation of the \(a_i\) with \(i\in H\)). Let \(r=a\) if \(\tau=1\) and \(r=b\) otherwise. The trapdoor of \(f_r\) is known only to \(R\) (it is the concatenation of the trapdoors obtained in Step 3).</li>
										<li>\(R\) now sends the public key \((f_a,f_b)\) to the sender.</li>
									</ol>
									<p class="rp_original">
										Figure 6: The key generation protocol
									</p>
								</div>
							</section>
							<section id="sec4.2.3">
								<h4>4.2.3. Simulation (Adaptive Security of the Encryption Protocol)</h4>
								<p class="rp_original">
									Let \(\epsilon\) denote the combined encryption and decryption protocols, preceded by the key generation protocol.
								</p>
								<span class="rp_theorem_header" id="theorem4.8">Theorem 4.8</span>
								<p class="rp_original rp_theorem">
									Protocol \(\epsilon\) is an \((n-1)\)-resilient non-committing encryption protocol for \(n\) parties, in the presence of non-erasing parties.
								</p>
								<span class="rp_sub_header">Proof (sketch)</span>
								<p class="rp_original rp_sub">
									Let \(P_r\) be the sender and \(P_s\) be the receiver. Recall that a non-committing encryption protocol is a protocol that securely computes the bit transmission function, \(BTR_{s,r}\) in a simulatable way. Let \(\epsilon'\) be a non-erasing protocol for \(\epsilon\). We construct a simulator \(\mathcal S\) such that \(IDEAL_{BTR_{s,r},\mathcal S^{\mathcal A}}(\sigma)\stackrel{d}=EXEC_{\epsilon',\mathcal A}(\sigma)\), for any \((n-1)\)-limited adversary \(\mathcal A\) and for any input \(\sigma\in\{0,1\}\) of \(P_{\mathcal S}\).
								</p>
								<p class="rp_original rp_sub">
									The simulator \(\mathcal S\) proceeds as follows. First an invocation of the key generation protocol \(\epsilon_G\) is simulated, in such a way that \(\mathcal S\) knows both trapdoors \(f_a^{-1}\) and \(f_b^{-1}\). (This can be done using the additional property of the <span class="reference" data-citation="GMW">[?]</span> Oblivious Transfer protocol, as described above.) For each party \(P\) that \(\mathcal A\) corrupts during this stage, \(\mathcal S\) hands \(\mathcal A\) the internal data held by \(P\) in the simulated interaction. We stress that as long as at least one party remains uncorrupted, the adversary knows at most <i>one</i> of \(f_a^{-1},f_b^{-1}\). Furthermore, as long as \(P_\tau\) remains uncorrupted, the adversary view of the computation is independent of whether \(P_\tau\) has \(f_a^{-1}\) or \(f_b^{-1}\).
								</p>
								<p class="rp_original rp_sub">
									Once the simulation of the key generation protocol is completed, \(\mathcal S\) instructs the trusted party in the ideal model to notify \(P_\tau\) of the function value. (This value of \(P_s\)'s input, \(\sigma\).) If at this point either \(P_s\) or \(P_\tau\) is corrupted, then \(\mathcal S\) gets to know the encrypted bit. In this case \(\mathcal S\) generates a true encryption of the bit \(\sigma\), according to the protocol. If neither \(P_s\) nor \(P_\tau\) are corrupted, then \(\mathcal S\) generates the values \(\vec z,\vec x^{(0)},\vec x^{(1)},\phi^{(0)},\phi^{(1)},r^{(0)},r^{(1)}\) as in <a href="#lemma4.7">Lemma 4.7</a>, and lets \(\vec z\) be the ciphertext that \(P_s\) sends to \(P_\tau\) in the simulated interaction.
								</p>
								<p class="rp_original rp_sub">
									If at this stage \(\mathcal A\) corrupts some party \(P\) which is not the sender or the receiver, then \(\mathcal S\) hands \(\mathcal A\) the internal data held by \(P\) in the simulated interaction. If \(\mathcal A\) corrupts \(P_s\), then \(\mathcal S\) corrupts \(P_s\) in the ideal model and learns \(\sigma\). Next \(\mathcal S\) hands \(\mathcal A\) the values \(\vec x^{(\sigma)},\phi^{(\sigma)}\) for \(P_s\)'s internal data. If \(\mathcal A\) corrupts \(P_\tau\), then \(\mathcal S\) corrupts \(P_\tau\) in the ideal model, learn \(\sigma\), and hands \(\mathcal A\) the value \(f^{-1}_{\tau^{(\sigma)}}\) for \(P_s\)
									's internal data.
								</p>
								<p class="rp_original rp_sub">
									The validity of the simulation follows theorem <a href="#lemma4.7">Lemma 4.7</a> and from the properties of the <span class="reference" data-citation="GMW">[?]</span> Oblivious Transfer protocol. \(\quad\quad\blacksquare\).
								</p>
							</section>
						</section>
						<section id="sec4.3">
							<h3>4.3. Alternative Implementations of Non-Committing Encryption</h3>
							<p class="rp_original">
								We describe two alternative implementations of our non-committing encryption scheme, based on the RSA and DH assumptions, respectively. These implementations have the advantage that the key generation stage can be simplified to consist of a single message sent from the receiver to the sender.
							</p>
							<section id="sec4.3.1">
								<h4>4.3.1. An Implementation Based on RSA</h4>
								<p class="rp_original">
									We first construct the following common-domain trapdoor system. The common domain, given security parameter \(n\), is \(\{0,1\}^n\). A permutation over \(\{0,1\}^n\) is chosen as follows. First, choose a number \(N\) uniformly from \([2^{n-1},\dots,2^n]\), <i>together with its factorization</i> (via Bach's algorithm<sup class="reference" data-citation="B">[?]</sup>). Next choose a prime \(2^n\lt e\lt2^{n+1}\). (This way, we are assured that \(\operatorname{gcd}(e,\operatorname\phi(N))=1\), where \(\operatorname\phi(\cdot)\) is Euler's totient function, even if the factorization of \(N\) is not known.) Let \(f_N(x)=x^e\pmod N\) if \(x\lt N\) and \(f_N(x)=x\) otherwise. With non-negligible probability \(N\) is a product of two large primes. Thus, thus construction yields a collection of common domain permutations which are weakly one-way. Employing an amplification procedure (e.g. <span class="reference" data-citation="Y">[?]</span><span class="reference" data-citation="GILVZ">[?]</span>) we obtain a proper common-domain system.
								</p>
								<p class="rp_original">
									This common-domain trapdoor system can be used as described in <a href="#sec4.2">Section 4.2</a>. However, here the key generation stage can be simplified considerably. Observe that it is possible to choose a permutation from the above distribution <i>without knowing its trapdoor</i>. Specifically, this is done by choosing the numbers \(N\) of the different instances of \(f_N\) in the direct way, without knowing their factorization. Thus, the receiver will choose two trapdoor permutations \(f_a,f_b\), where only the trapdoor to \(f_\tau\) (i.e. \(f_\tau^{-1}\)) is known, \(r\in_R\{a,b\}\). Both \(f_a,f_b\) are now sent to the sender, who proceeds as in <a href="#sec4.2.1">Section 4.2.1</a>. In a simulated execution the simulator will choose both \(f_a\) and \(f_b\) together with their trapdoors.<sup class="footnote" id="fref14" data-footnote="14"><a href="#footnote14">14</a></sup>
								</p>
							</section>
							<section id="sec4.3.2">
								<h4>4.3.2. An Implementation Based on DH</h4>
								<p class="rp_original">
									Consider the following construction. Although it fails to satisfy <a href="#def4.3">Definition 4.3</a>, it will be 'just as good' for our needs. The common domain, given security parameter \(n\), is a prime \(2^{n-1}\lt p\lt2^n\) where the factorization of \(p-1\) is known. Also a generator \(f\) of \(Z_p^*\) is fixed. \(p\) and \(g\) are publicly known. All computations are done modulo \(p\). To choose a permutation over \(Z_p^*\), choose an element \(v\in_R\mathcal Z^*_{p-1}\) and let \(f_v(x)=x^v\). The public description of \(f_v\) is \(y\stackrel{\Delta}=g^v\). The 'trapdoor' is \(v\stackrel\Delta=v^{-1}\pmod{p-1}\).
								</p>
								<p class="rp_original">
									This construction has the following properties:
								</p>
								<ul class="rp_original">
									<li>Although it is hard to compute \(f_v\) if only \(p,g,y\) are known, it is easy to generate random elements \(x\in_R Z^*_p\) together with \(f_v(x)\): choose \(z\in_R\mathcal Z^*_p\), and set \(x=g^z\) and \(f_v(x)=y_z\). (This holds since \(f_v(x)=x^v=g^zv=y^z\).)</li>
									<li>If \(u\) is known then it is easy to compute \(f_v^{-1}(x)=x^u\).</li>
									<li>An algorithm \(A\) that inverts \(f_v\) given only \(p,g,y\) can be easily transformed into an algorithm \(A'\) that given \(p,g,g^\alpha,g^\beta\) outputs \(g^{\alpha\beta}\) (that is, into an algorithm that contradicts the Diffe-Hellman (DH) assumption). Specifically, assume that \(A(p,g,g^v,x^v)=x\). Then on input \(p,g,g^\alpha,g^\beta\), algorithm \(A'\) will run \(A(p,g^\alpha,g,g^\beta)\) to obtain\(g^{\alpha\beta}\).</li>
									<li>It is possible to choose a permutation from the above distribution <i>without knowing its trapdoor</i>. Specifically, this is done by uniformly choosing numbers \(y\in_R Z^*_p\) until a generator is found. (It is easy to decide whether a given \(y\) is a generator of \(Z^*_p\) when the factorization of \(p-1\) is known.)</li>
								</ul>
								<p class="rp_original">
									Note that both in the encryption process and in the simulation it is not necessary to compute the permutations \(f\) on arbitrary inputs. It suffices to be able to generate random elements \(x\) in the domain together with their function value \(f(x)\). Thus, this construction is used in a similar way to the previous one.
								</p>
							</section>
						</section>
						<section id="sec4.4">
							<h3>4.4. A Concluding Remark to Section 4</h3>
							<p class="rp_original">
								Our solutions for non-erasing parties may appear somewhat unsatisfactory since they are based on 'trusting' the receiver to choose trapdoor permutations without knowing the trapdoor, whereas the permutation can be chosen together with its trapdoor by simple 'honest-looking' behavior. Recall, however, that if honest looking parties are allowed then no (non-trivial) protocol can be proven adaptively secure (via black-box simulation if clawfree pairs exist). We do not see a meaningful way to distinguish between the 'honest-looking behavior' that foils the security of our constructions and the 'honest-looking behavior', described in <a href="#sec2.2">Section 2.2</a>, that foils provability of the adaptive security of any protocol.
							</p>
						</section>
					</section>
					<section id="sec5">
						<h2>5. Honest-looking Parties</h2>
						<p class="rp_original">
							Our construction for honest-looking parties assumes the existence of a "trusted dealer" at a pre-computation stage. The dealer chooses, for each party \(P), a truly random string \(r_P\) and hands \(r_P\) to \(P\), to be used as random input. (We call \(r_P\) a <b>certified random input</b> for \(P\).) Next, the dealer generates \(n-1\) shares of \(r_P\), so that \(r_P\) can be reconstructed from all \(n-1\) shares, but any subset of \(n-2\) shares are independent of \(r_P\). Finally the dealer hands one share to each party <i>other than \(P\)</i>.
						</p>
						<p class="rp_original">
							Now all parties are able to jointly reconstruct \(r_P\), and thus verify whether \(P\) follows its protocol. Consequently, if party \(P\) is honest-looking (i.e. \(P\) does not take any chance of being caught cheating), then it is forced to use \(r_P\) exactly as instructed in the protocol. Party \(P\) is now limited to non-erasing behavior, and the construction of <a href="#sec4">Section 4</a> applies. (We note that the use of certified random inputs does not limit the simulator. That is, upon corruption of party \(P\), the simulator can still compute some convenient value (r_P'\) to be used as \(P\)'s random input, and then "convince" the adversary that the certified random input of \(P\) was \(r'_P\). The adversary will not notice anything wrong since it will never have all the shares of the certified random input.)
						</p>
					</section>
				</div>
            </div>
			<div class="main_toplevel main_section main_color7" id="footnotes">
				<div class="main_fullwidth">
					<h1>Footnotes</h1>
					
					<ol id="footnotelist">
						<li id="footnote1">
							We illustrate this distinction via the following example. Let \(f(x,y)=g(x\oplus y)\) where \(g\) is a one-way permutation and \(\oplus\) denotes bitwise exclusive or. Assume that parties \(A\) and \(B\) have inputs \(x\) and \(y\) respectively, and consider the following protocol for computing \(f\): Party \(A\) announces \(x\), party \(B\) announces \(y\), and both parties compute \(f(x,y)\). Our intuition is that this protocol is insecure against adversaries that may corrupt one party (say \(B\)): it "gives away for free" both \(x\) and \(y\), whereas computing \(x\) given \(y\) and \(f(x,y)\), may take the adversary a large amount of time. Indeed, if the ideal-model adversary \(\mathcal S\) limited to probabilistic polynomial time (and one-way permutations exist), then this protocol is insecure against adversaries that corrupt one party. However, under the model allowing \(\mathcal S\) unlimited computational power regardless of \(\mathcal A\)'s complexity, this protocol is considered secure since \(\mathcal S\) can invert \(g\).
						</li>
						<li id="footnote2">
							There is also the easier problem of generating the messages sent by \(P\) to corrupted parties. This was the problem discussed in the <a href="#sec2.3.2">previous subsection</a>. However, our hypothesis that \(\mathcal S\) is a simulator for the secure channel model means that \(\mathcal S\) is able to generate these cleartext messages. Thus, all that \(\tilde{\mathcal S}\) needs to do is encrypt the messages that it has obtained from \(\mathcal S\).
						</li>
						<li id="footnote3">
							This "non-committing property" is reminiscent of the "Chameleon blobs" of <span class="reference" data-citation="BCC">[?]</span>. The latter are <i>commitment</i> schemes where the recipient of a commitment \(c\) can generate by himself the de-commitments of \(c\) to both 0 and 1, whereas the sender is "effectively committed" to a specific bit value.
						</li>
						<li id="footnote4">
							Each of these algorithms is also given an \(n\)-bit encryption key.
						</li>
						<li id="footnote5">
							Consequently, it must be that \(\mathcal E^{(0)}\) and \(\mathcal E^{(1)}\) are computationally indistinguishable. Thus, a non-committing encryption scheme is also a secure encryption scheme in the traditional sense.
						</li>
						<li id="footnote6">
							A more general formulation allows different parties to compute a different functions of the input. Specifically, in this case the range of \(f\) is an \(n\)-fold Cartesian product and the interpretation is that the \(i\)<sup>th</sup> party should get the \(i\)<sup>th</sup> component of \(f(\vec{x})\).
						</li>
						<li id="footnote7">
							In the case where each party computes a different function of the inputs, as discussed in the previous footnote, the trusted party will hand each party its specified output.
						</li>
						<li id="footnote8">
							For simplicity, we assume that the computed function is polynomially computable. Alternatively, the simulator is polynomial in the complexity of the function.
						</li>
						<li id="footnote9">
							For simplicity we assume that the links are authenticated, namely the adversary cannot <i>alter</i> the communication. Authenticity can be achieved via standard primitives.
						</li>
						<li id="footnote10">
							A security proof of the <span class="reference" data-citation="BGW">[?]</span> construction can be extracted from <span class="reference" data-citation="C">[?]</span> (chapter 3), which deals with the more involved asynchronous model.
						</li>
						<li id="footnote11">
							Assume that each pair of parties share a sufficiently long secret random string, and each message is encrypted by bitwise XORing it with a new segment of the shared random string. Then <a href="#def3.10">Definition 3.10</a> is satisfied in a straightforward way. Specifically, the simulated message from the sender to the receiver (i.e. the dummy ciphertext), denoted \(c\), can be uniformly chosen in \(\{0,1\}\). When either the sender or the receiver are corrupted, and the simulator has to demonstrate that \(c\) is an encryption of a bit \(\sigma\), the simulator claims that the corresponding shared random bit was \(\tau=c\oplus\sigma\). Clearly \(\tau\) is uniformly distributed, regardless of the value of \(\sigma\).
						</li>
						<li id="footnote12">
							In each of the following five conditions, the first equality is by the construction of the \(v_i\)'s, the second equality is by the definition of the \(z_i\)'s, and the third equality represents the relation between \(\vec x^{(\sigma)},\,\vec z,\text{ and }\phi{(\sigma)}\) that holds in a valid encryption (of \(\sigma\)). In conditions (1) through (4), the last equality represents the relation between \(\vec x ^{(\sigma)}\)\text{ and }\sigma\) that holds in a valid encryption of \(\sigma\). In condition (5), the last equality represents the information computable from \(\vec z\) using (the trapdoor) \(f^{-1}_{r^{(\sigma)}}\). Here we refer to the inverses of the \(z_i\)'s which are not \(x_i^{(\sigma)}\)'s. The hardcore values of these inverses should be uniformly distributed.
						</li>
						<li id="footnote13">
							This statement does not hold if \(R\) is semi-honest only in the honest-looking sense. Ironically, this 'flaw' is related to the useful (non-committing) feature discussed below.
						</li>
						<li id="footnote14">
							A similar idea was used in <span class="reference" data-citation="DP">[?]</span>.
						</li>
					</ol>
				</div>
            </div>
            <div class="main_toplevel main_section main_color8" id="references">
				<div class="main_fullwidth">
					<h1>References</h1>
					<ol id="referencelist">
						<li id="B">E. Bach, "How to generate factored random numbers", <i>SIAM J. on Comput.</i>, Vol. 17, No. 2, 1988, pp.179-193.</li>
						<li id="Be1">D. Beaver, "Foundations of Secure Interactive Computing", <i>CRYPTO</i>, 1991.</li>
						<li id="Be2">D. Beaver, "Adaptive Zero-Knowledge and Computational Equivocation", <i>28th STOC</i>, 1996.</li>
						<li id="BH">D. Beaver and S. Haber, "Cryptographic Protocols Provably secure Against Dynamic Adversaries", <i>Eurocrypt</i>, 1992.</li>
						<li id="BGW">M. Ben-Or, S. Goldwasser, and A. Wigderson. <a href="17.html">Completeness Theorems for Non-Cryptographic Fault-Tolerant Distributed Computation</a>. <i>Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing,</i> pages 1-10. ACM. 1988. </li>
						<li id="BM">M. Blum, and S. Micali, "How to generate Cryptographically strong sequences of pseudo-random bits", <i>SIAM J. on Computing</i>, Vol. 13, 1984, pp. 850-864.</li>
						<li id="BCC">G. Brassard, D. Chaum and C. Crepeau, "Minimum Disclosure Proofs of Knowledge", <i>JCSS</i>, Vol. 37, No. 2, 1988, pp. 156-189.</li>
						<li id="C">R. Canetti, "Studies in Secure Multi-Party Computation and Applications", Ph.D. Thesis, Department of Computer Science and Applied Math, Weizmann Institute of Science, Rehovot, Israel, June 1995.</li>
						<li id="CDNO">R. Canetti, C. Dwork, M. Naor, and R. Ostrovsky, "Deniable Encryptions", manuscript.</li>
						<li id="CCD">D. Chaum, C. Cr&eacute;peau, I. Damg&aring;rd. <a href="34.html">Multiparty Unconditionally Secure Protocols (extended abstract)</a>. <i>Proceedings of the Twentieth Annual ACM Symposium on the Theory of Computing,</i> pp. 11-19. ACM. 1988.</li>
						<li id="DP">A. De-Santis and G. Persiano, "Zero-Knowledge proofs of knowledge without interaction", <i>33rd FOCS</i>, pp. 427-436, 1992.</li>
						<li id="EGL">S. Even, O. Goldreich and A. Lempel, "A randomized protocol for signing contracts", <i>CACM</i>, vol. 28, No. 6, 1985, pp. 637-647.</li>
						<li id="F">P. Feldman, personal communication via Cynthia Dwork, 1988.</li>
						<li id="GILVZ">O. Goldreich, R. Impagliazzo, L. Levin, R. Venkatesan and D. Zuckerman, "Security Preserving Amplification of Hardness", <i>31st FOCS</i>, 1990, pp. 318-326.</li>
						<li id="GrL">O. Goldreich and L. Levin, "A Hard-Core Predicate to any One-Way Function", <i>21st STOC</i>, 1989, pp. 25-32.</li>
						<li id="GMW">O. Goldreich, S. Micali, and A. Wigderson. <a href="81.html">How to Play Any Mental Game or a Completeness Theorem for Protocols with Honest Majority</a>. <i>Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing</i>, pages 218–229. ACM. 1987.</li>
						<li id="GwL">S. Goldwasser and L. Levin, "Fair Computation of General Functions in Presence of Immoral Majority", <i>CRYPTO</i>, 1990.</li>
						<li id="MR">S. Micali and P. Rogaway, "Secure Computation", <i>CRYPTO</i>, 1991.</li>
						<li id="R">M. Rabin, "How to exchange secrets by oblivious transfer", Tech. Memo TR-81, Aiken Computation Laboratory, Harvard U., 1981.</li>
						<li id="RB">T. Rabin and M. Ben-Or. <a href="76.html">Verifiable Secret Sharing and Multiparty Protocols with Honest Majority</a>. <i>Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing</i>, pages 73-85. ACM. 1989.</li>
						<li id="RSA">R. Rivest, A. Shamir, and L. Adleman, "A Method for Obtaining Digital Signatures and Public Key Cryptosystems", <i>CACM</i>, Vol. 21, Feb. 1978, pp. 120-126.</li>
						<li id="Y">A. Yao, "Theory and applications of trapdoor functions", <i>23rd FOCS</i>, 1982, pp. 80-91.</li>
					</ol>
				</div>
            </div>
            <div class="main_toplevel main_section main_color9">
                <div class="rp_problems">
                    <p><a href="mailto:multipartycomputationorg+5@gmail.com">Problem with this page?</a></p>
                </div>
                <p>Created by Nicolas Schank 2014, Brown University</p>
				<p>All original work is free for any use by anyone whatsoever.</p>
				<p>For more information about liability and licensing of the original paper, see <a href="../liability.html">Liability</a>.</p>
            </div>
        </div>
    </body>
</html>
<!--
6. Annotate paper
	7.0. proofread
	7.1. copy assumptions
	7.2. copy theorems
	7.3. copy definitions
	7.4. mark definitions
	7.5. mark equations
	7.6. link to previous research
7. Write protocol descriptions 
8. Check previous research for places to link
9. Write intro, goals, results
10. Tags
11. Consider implementations
12. Find reference
-->