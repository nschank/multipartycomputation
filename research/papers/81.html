<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>How to Play Any Mental Game - Goldreich Micali Wigderson, 1987</title>
        <link rel="stylesheet" type="text/css" href="../../style/main.css">
        <link rel="stylesheet" type="text/css" href="../../style/equation.css">
        <link rel="stylesheet" type="text/css" href="../../style/ref.css">
        <link rel="stylesheet" type="text/css" href="../../style/glossary.css">
        <link rel="stylesheet" type="text/css" href="../../style/researchPaper.css">
        <link rel="icon" href="../../img/favicon.ico" type="image/x-icon">
		<script type="text/javascript" src="../../script/blockShare.js"></script>
        <script type="text/javascript" src="../../script/equation.js"></script>
        <script type="text/javascript" src="../../script/ref.js"></script>
        <script type="text/javascript" src="../../script/glossary.js"></script>
        <script type="text/javascript" src="../../script/def.js"></script>
        <script type="text/javascript"
                src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        <script type="text/javascript">
			<!--
            function fill(box)
			{
				switch (box)
				{
					case "{0,1}*":
						return "<p>The set of all 0/1 strings of any length, including the empty string.</p>";
					case "HS":
						return "<p>The function \\(HS\\) returns the RV consisting of the public history that is the sequence of all messages sent in an execution of \\(S\\).</p>";
					case "HSi":
						return "<p>The function \\(HS_i\\) returns the RV consisting of the sequence of the internal configurations of machine \\(i\\) in an execution of \\(S\\).</p>";
					case "HST":
						return "<p>For \\(T\\subset \\{1,…,n\\}\\), the function \\(HS_T(x_1,\\dots,x_n)\\) denotes the vector of the private histories of the members of \\(T\\) in an execution of \\(S\\).</p>";
					case "OSi":
						return "<p>The function \\(OS_i(x_1,\\dots,x_n,CI)\\) denotes the RV consisting of the private output of machine \\(i\\) in an execution of \\(S\\).</p>";
					case "PUCk":
						return "<p>The probability function \\(P(U,C,k)\\) calculates the probability that \\(C_k\\) outputs 1 on input a random string from \\(U_k\\), where \\(U\\) is a <span class=\"definable\">poly-bounded family of random variables</span> and \\(C\\) is a <span class=\"definable\">poly-size family of circuits</span>. We assume that the length of the strings that are assigned positive probability by \\(U_k\\) equals the number of Boolean inputs of \\(C_k\\).</p>";
					default:
						return "No info on this equation yet.";
				}
			}

			function authorLink(ref)
			{
				switch (ref)
				{
					default:
						return "#";
				}
			}

			self_def["computational indistinguishability"] = "<a href=\"#def1\">Definition 1</a></p><p class=\"gl_definition\">Informally, groups of random variables are indistinguishable from each other when large groups of them can't be differentiated consistently.";
			self_def["dynamic adversary"] = "A good machine can choose, during the run of the protocol, to become an adversary.";
			self_def["engagement protocol"] = "A protocol to perform before playing a Tm-game that will 'lock in' players and force them to play the game. If they stop playing, the other players in the game can team up in order to perfectly simulate that player's interactions, assuming a majority of the original players is still playing.";
			self_def["game network"] = "A network of \\(n\\) interacting polynomial-time Turing machines";
			self_def["knowledge function"] = "A function \\(K_i\\) that takes a state in a game \\(\\sigma\\) and returns all information that player \\(i\\) knows during that state.";
			self_def["one-out-of-two oblivious transfer"] = "Alice has two messages, \\(m_0\\) and \\(m_1\\). By using a cryptosystem \\(E\\) she computes \\(\\sigma_0=E(m_0)\\) and \\(\\sigma_1=E(m_1)\\) and sends \\(\\sigma_0, \\sigma_1\\) to Bob. Bob chooses one of these encryptions, \\(\\sigma_i\\). A one-out-of-two OT allows Bob to read the corresponding message \\(m_i\\), while Alice will not know which message Bob has read (whenever \\(m_0\\neq m_1\\)).";
			self_def["pay-off function"] = "A function \\(p\\) that takes in the final state of a game and returns the outcome of the game based on that state. As an example, the pay-off function for poker takes in the state of everyone's hands after the last turn is played, and would return information like who won, how much they won, and how much everyone else lost.";
			self_def["poly-bounded family of random variables"] = "A family \\(U=\\{U_k\\}\\) such that, for some constant \\(e&gt;0\\), all RVs \\(U_k\\in U\\) assign positive probability only to strings whose length is exactly \\(k^e\\).";
			self_def["poly-size family of circuits"] = "A family \\(C=\\{C_k\\}\\) of Boolean circuits \\(C_k\\) with one Boolean output such that, for some constants \\(e,d&gt;0\\), all \\(C_k\\in C\\) have at most \\(k^e\\) gates and \\(k^d\\) Boolean inputs.";
			self_def["purely playable game"] = "An \\(n\\)-person game that can be implemented by the \\(n\\) players without invoking any <span class=\"definable\" data-define=\"trusted party\">trusted parties</span>.";
			self_def["royal flush"] = "For those of you who don't play poker, a royal flush is the highest hand in poker (with no jokers) and consists of an Ace, King, Queen, Jack, and 10, all of the same suit.";
			self_def["Turing machine game"] = "<a href=\"#def2\">Definition 2</a></p><p class=\"gl_definition\">A Tm-game problem consists of a pair \\((\\bar{M},1^k)\\), that is, the description of a <span class=\"definable\">Turing machine</span> \\(M\\) and an integer \\(k\\), the security parameter, presented in unary. \\(n\\) parties, respectfully and individually owning secret inputs \\(x_1,\\dots,x_n\\) would like to correctly run \\(M(x_1,\\dots,x_n)\\) without revealing (to the other players) more about the \\(x_i\\)'s than is already contained in the output of the Turing machine itself.";
			//-->
        </script>
    </head>
    <body>
        <div class="main_foreground">
            <div class="main_toplevel main_header">
                <h1>Multiparty Computation</h1>
            </div>
            <div class="main_toplevel main_navigation">
                <a href="../../index.html"><div class="main_navbox"><h2>home</h2></div></a>
                <a href="../../learn.html"><div class="main_navbox"><h2>learn</h2></div></a>
                <a href="../../research.html"><div class="main_navbox"><h2>research</h2></div></a>
                <a href="../../nextsteps.html"><div class="main_navbox"><h2>build</h2></div></a>
                <a href="../../resources.html"><div class="main_navbox"><h2>resources</h2></div></a>
                <a href="../../aboutus.html"><div class="main_navbox"><h2>about us</h2></div></a>
            </div>
            <div class="main_toplevel main_section main_color1">
                <div class="main_section_nav_container">
                    <div class="main_section_nav_box"><a href="../timeline.html">Timeline</a></div>
                    <div class="main_section_nav_box"><a href="../title.html">By Title</a></div>
                    <div class="main_section_nav_box"><a href="../authors.html">By Author</a></div>
                    <div class="main_section_nav_box"><a href="../tag.html">By Category</a></div>
                </div>
                <div class="main_window main_fullwidth"><a id="_A"></a>
                    <div class="rp_linkbox"><a href="pdf/81.pdf"><img src="../../img/PDF.png" class="rp_link" alt="view pdf" /></a></div>
					
                    <span class="rp_title">How to Play Any Mental Game or A Completeness Theorem for Protocols with Honest Majority</span>
                    <span class="rp_info">1987
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <a class="rp_author" href="../authors/goldreich.html">Oded Goldreich</a> <a class="rp_author" href="../authors/micali.html">Silvio Micali</a> <a class="rp_author" href="../authors/wigderson.html">Avi Wigderson</a></span>
						
                    <div class="main_toc">
                        <h4 style="color:#333333">Table of Contents</h4>
                        <ol type="A">
                            <li><a href="#_A">Overview</a>
								<ol>
									<li><a href="#a_intro">Introduction</a></li>
									<li><a href="#a_goals">Goals and Results</a></li>
									<li><a href="#a_defs">Definitions</a></li>
									<li><a href="#a_theorems">Theorems</a></li>
									<li><a href="#a_protocols">Protocols</a></li>
									<li><a href="#a_ref">Referencing This Paper</a></li>
								</ol>
							</li>
                            <li><a href="#_B">Annotated Extended Abstract</a></li>
                        </ol>
                    </div>
					
                    <div class="rp_snippet">
						Our algorithm automatically solves all the multi-party protocol problems addressed in complexity-based cryptography during the last 10 years.
					</div>
						
                    <section id="a_intro">
						<h2>Introduction</h2>
						<p class="rp_analysis">
							<b>Paper Title</b> is...
						</p>
					</section>
					<section id="a_goals">
						<h2>Goals and Results</h2>
						<p class="rp_analysis">
							Some goals that they had.
						</p>
						<p class="rp_analysis">
							Don't forget some results, too!
						</p>
					</section>
					<section id="a_defs">
						<h2>Definitions</h2>
						<a href="#def1"><span class="rp_definition_header">Definition 1 - Computational Indistinguishability</span></a>
						<p class="rp_definition">
							Two <span class="definable" data-define="poly-bounded family of random variables">poly-bounded families of RV's</span> are <span class="definable">computationally indistinguishable</span> if, for all <span class="definable" data-define="poly-size family of circuits">poly-size families of circuits</span> \(C\), for all constants \(f&gt;0\) and all sufficiently large \(k\in N\),
						</p>
						<p class="rp_definition">
							$$|P(U,c,k)-P(V,C,k)|&lt;k^{-f}$$
						</p>
						<a href="#def2"><span class="rp_definition_header">Definition 2 - Turing machine game</span></a>
						<p class="rp_original rp_definition">
							A <span class="definable" data-define="Turing machine game">Tm-game</span> problem consists of a pair \((\bar{M},1^k)\), that is, the description of a <span class="definable">Turing machine</span> \(M\) and an integer \(k\), the security parameter, presented in unary. \(n\) parties, respectfully and individually owning secret inputs \(x_1,\dots,x_n\) would like to correctly run \(M(x_1,\dots,x_n)\) without revealing (to the other players) more about the \(x_i\)'s than is already contained in the output of the Turing machine itself.
						</p>
					</section>
					<section id="a_theorems">
						<h2>Theorems</h2>
						<a href="#theorem1"><span class="rp_theorem_header">Theorem 1</span></a>
						<p class="rp_original rp_theorem">
							If <span class="definable" data-define="trapdoor function">trapdoor functions</span> exist, there exists a Tm-game solver for <span class="definable" data-define="passive adversary">passive adversaries</span>.
						</p>
						<a href="#theorem2"><span class="rp_theorem_header">Theorem 2</span></a>
						<p class="rp_original rp_theorem">
							Given \(n\) players willing to play, less than half of which are <span class="definable" data-define="malicious adversary">malicious</span>, all <span class="definable" data-define="Turing machine game">Tm-games</span> are playable.
						</p>
						<a href="#theorem3"><span class="rp_theorem_header">Theorem 3</span></a>
						<p class="rp_original rp_theorem">
								If any <span class="definable">trapdoor function</span> exists, any game is <span class="definable" data-define="purely playable game">playable</span> if more than half of the players are honest.
							</p>
					</section>
					<section id="a_protocol">
						<h2>Protocols</h2>
						<h3><a href="#secxx">Some Protocol Defined</a></h3>
						<ul class="rp_analysis">
							<li><b>Number of parties: </b></li>
							<li><b>Function(s): </b></li>
							<li><b>Privacy constraints: </b></li>
							<li><b>Security constraints: </b></li>
							<li><b>Cheating: </b></li>
							<li><b>Bits exchanged: </b></li>
							<li><b>Runtime:</b></li>
						</ul>
					</section>
					<section id="a_ref">
						<h2>Referencing This Paper</h2>
						<p class="rp_analysis">To cite this paper, simply copy and paste the below into your citation:</p>
						<p class="rp_self_reference">
							A reference for this paper
						</p>
					</section>
                </div>
            </div>
            <div class="main_toplevel main_section main_color5"><a id="_B"></a>
                <div class="main_window main_fullwidth">
                    <div class="main_toc">
                        <h4 style="color:#333333">Table of Contents</h4>
                        <ol type="A">
                            <li><a href="#_A">Overview</a></li>
                            <li>
                                <a href="#_B">Annotated Paper</a>
                                <ol>
									<li value="0"><a href="#abstract">Abstract</a></li>
                                    <li><a href="#sec1">Introduction</a></li>
									<li>
										<a href="#sec2">Preliminary Definitions</a>
										<ol>
											<li><a href="#sec2.1">Notation and Conventions for Probabilistic Algorithms</a></li>
											<li><a href="#sec2.2">Game Networks and Distributed Algorithms</a></li>
											<li><a href="#sec2.3">Adversaries</a></li>
											<li><a href="#sec2.4">Indistinguishability of Random Variables</a></li>
										</ol>
									</li>
									<li><a href="#sec3">Tm-games with Passive ADversaries</a></li>
									<li>
										<a href="#sec4">Hints on How to Play Tm-games with Passive Adversaries</a>
										<ol>
											<li><a href="#sec4.1">A New and General Oblivious Transfer Protocol</a></li>
											<li><a href="#sec4.2">Strengthening Yao's Combined Oblivious Transfer</a></li>
											<li><a href="#sec4.3">The TM-game Solver for passive adversaries</a></li>
										</ol>
									</li>
									<li><a href="#sec5">Malicious Adversaries</a></li>
									<li>
										<a href="#sec6">General Games</a>
										<ol>
											<li><a href="#sec6.1">Games</a></li>
											<li><a href="#sec6.2">Playable Games</a></li>
											<li><a href="#sec6.3">A General Result</a></li>
											<li><a href="#sec6.4">A Completeness Theorem for Fault-Tolerant Computation</a></li>
										</ol>
									</li>
									<li><a href="#sec7">Recent Developments</a></li>
									<li><a href="#sec8">Acknowledgements</a></li>
                                </ol>
                            </li>
                        </ol>
                    </div>
					<section id="abstract">
						<h2>Abstract</h2>
						<p class="rp_original">
							We present a polynomial-time algorithm that, given as input the description of a game with incomplete information and any number of players, produces a protocol for playing the game that leaks no partial information, provided the majority of the players is honest.
						</p>
						<p class="rp_original">
							Our algorithm automatically solves all the multi-party protocol problems addressed in complexity-based cryptography during the last 10 years. It actually is a completeness theorem for the class of distributed protocols with honest majority. Such completeness theorem is optimal in the sense that, if the majority of the players is not honest, some protocol problems have no efficient solution.
						</p>
					</section>
					
					<section id="sec1">
						<h2>Introduction</h2>
						<p class="rp_original">
							Before discussing how to “make playable" a general game with incomplete information (which we do in section 6) let us address the problem of making playable a special class of games, the <span class="definable">Turing machine</span> games (<i>Tm-games</i> for short).
						</p>
						<p class="rp_original">
							Informally, \(n\) parties, respectively and individually owning secret inputs \(x_1,\dots,x_n\) would like to <i>correctly</i> run a given Turing machine \(M\) on these \(x_i\)‘s while keeping the maximum possible privacy about them. That is, they want to compute \(y=M(x_1,\dots,x_n)\) without revealing more about the \(x_i\)’s than it is already contained in the value \(y\) itself. For instance, if \(M\) computes the sum of the \(x_i\)’s, every single player should not be able to learn more than the sum of the inputs of the other parties. Here \(M\) may very well be a probabilistic Turing machine. In this case, all player want to agree on a single string \(y\), selected with the right probability distribution, as \(M\)’s output.
						</p>
						<p class="rp_original">
							The correctness and privacy constraint of a Tm-game can be easily met with the help of an extra, <span class="definable">trusted party</span> \(P\). Each player \(i\) simply gives his secret input \(x_i\) to \(P\). \(P\) will privately run the prescribed Turing machine, \(M\), on these inputs and publicly announce \(M\)’s output. Making a Tm-game playable essentially means that the correctness and privacy constraints can be satisfied by the \(n\) players themselves, without invoking any extra party. Proving that Tm-games are playable retains most of the flavor and difficulties of our general theorem.
						</p>
					</section>
					<section id="sec2">
						<h2>Preliminary Definitions</h2>
						<section id="sec2.1">
							<h3>Notation and Conventions for Probabilistic Algorithms</h3>
							<p class="rp_original">
								We emphasize the number of inputs received by an <span class="definable">algorithm</span> as follows. If algorithm \(A\) receives only one input we write “\(A(\cdot)\)", if it receives two inputs we write \(A(\cdot,\cdot)\) and so on.
							</p>
							<p class="rp_original">
								RV will stand for "random variable"; in this paper we only consider RVs that assume values in <span class="equation" data-equation="{0,1}*">\(\{0,1\}^*\)</span>. In fact, we deal almost exclusively with random variables arising from probabilistic algorithms. (We make the natural assumption that all parties may make use of probabilistic methods.)
							</p>
							<p class="rp_original">
								If \(A(\cdot)\) is a probabilistic algorithm, then for any input \(x\) the notation \(A(x)\) refers to the RV which assigns to the string \(\sigma\) the probability that \(A\), on input \(x\) outputs \(\sigma\). If \(S\) is a RV that assigns positive probability only to a single element \(e\), we denote the value \(e\) by \(S\). (For instance, if \(A(\cdot)\) is an algorithm that, on input \(x\) outputs \(x^3\), then we may write \(A(2)=8\). This is in agreement with traditional notation.
							</p>
							<p class="rp_original">
								If \(f(\cdot)\) and \(g(\cdot,\cdots)\) are probabilistic algorithms then \(f(g(\cdot,\cdots))\) is the probabilistic algorithm obtained by composing \(f\) and \(g\) (i.e. running \(f\) on \(g\)'s output). For any inputs \(x,y,\cdots\) the associated RV is denoted \(f(g(x,y,\cdots))\).
							</p>
							<p class="rp_original">
								Let PA denote the set of probabilistic polynomial-time algorithms. We assume that a natural representation of these algorithms as binary strings is used.
							</p>
							<p class="rp_original">
								By \(1^k\) we denote the unary representation of integer \(k\).
							</p>
						</section>
						<section id="sec2.2">
							<h3>Game Networks and Distributed Algorithms</h3>
							<p class="rp_original">
								Let us start by briefly describing the communication networks in which games will be played. This is the standard network supporting the execution of multi-party protocols.
							</p>
							<p class="rp_original">
								Informally, a <span class="definable">game network</span> of size \(n\) is a collection of (interacting) probabilistic polynomial-time Turing machines. Each machine has a private read-only input tape, a private write-only output tape and a private read-write work tape. All machines share a common read-only input tape and a common write-only output tape. The \(n\) machines communicate by means of \(n\cdot(n-1)\) special tapes. Machine \(i\) publically sends messages (strings) to machine \(j\) by means of a special tape \(i \rightarrow j\) on which only \(i\) can write and that all other machines can read. There is a common clock whose pulses define time intervals \(1,2,\dots\). Messages are sent at the beginning of a time interval and are received within the same time interval. We stress, though, that our result is largely independent from the specific communication mechanism, and also holds for "less equipped" communication networks.<sup id="fref1"><a href="#footnote1">(1)</a></sup>
							</p>
							<p class="rp_original">
								A probabilistic distributed algorithm \(S\) running in a game network of size \(n\) is a sequence of programs \(S=(S_1,\dots,S_n)\), where \(S_i\) is the program of the \(i\)-th Turing machine in the network. We denote by PDA the class of all probabilistic polynomial-time distributed algorithms.
							</p>
							<p class="rp_original">
								Let \(S\in PDA\) run in a game network of size \(n\) with common input \(CI\) and (respective) private inputs \(x_1,\dots,x_n\). Then <span class="equation" data-equation="HS">\(HS(x_1,\dots,x_n,CI)\)</span> denotes the RV consisting of the public history that is the sequence of all messages sent in an execution of \(S\); <span class="equation" data-equation="HSi">\(HS_i(x_1,\dots,x_n,CI)\)</span> denotes the RV consisting of the internal configurations of machine \(i\), that is the sequence of the internal configurations of machine \(i\) in an execution of \(S\); for \(T\subset \{1,\dots,n\}\), <span class="equation" data-equation="HST">\(HS_T(x_1,\dots,x_n)\)</span> denotes the vector of the private histories of the members of \(T\) in an execution of \(S\); and <span class="equation" data-equation="OSi">\(OS_i(x_1,\dots,x_n,CI)\)</span> denotes the RV consisting of the private output of machine \(i\) in an execution of \(S\).
							</p>
						</section>
						<section id="sec2.3">
							<h3>Adversaries</h3>
							<p class="rp_original">
								We consider two interesting types of adversaries (faulty machines) in a game network: passive ones (a new notion) and malicious ones (a more standard notion).
							</p>
							<p class="rp_original">
								A <span class="definable">passive adversary</span> is a machine that may compute more than required by its prescribed program, but the messages it sends and what it outputs are in accordance to its original program. (Passive adversaries may be thought of as machines who only try to violate the privacy constraint. They keep on running their prescribed programs correctly, but also run, “on the side”, their favorite polynomial time program to try to compute more than their due share of knowledge. In an election protocol, a passive adversary may be someone who respects the majority’s opinion&mdash;and thus does not want to corrupt the tally&mdash;and yet wants to discover who voted for whom.)
							</p>
							<p class="rp_original">
								A <span class="definable">malicious adversary</span> is, instead, a machine that deviates from its prescribed program in any possible action. That is, we allow the program of such a machine to be replaced by any fixed probabilistic polynomial-time program. (Malicious adversaries not only have a better change of disrupting the privacy constraint, but could also make the outcome of a Tm-game vastly different than in an ideal run with a <span class="definable">trusted party</span>.)
							</p>
							<p class="rp_original">
								We allow machines in a game network to become adversarial in a <span class="definable" data-define="dynamic adversary">dynamic</span> fashion, during the execution of a protocol. We also allow adversarial machines (of either type) to undetectedly cooperate. Adversarial machines are not allowed, however, to monitor the private tapes or the internal state of good machines.
							</p>
							<p class="rp_original">
								We believe the malicious-adversary scenario to be the most adversarial among all the natural scenarios in which cryptography may help.
							</p>
							<p class="rp_original">
								Jumping ahead, we will show that all Tm-games are playable with any number of passive adversaries or with \(&lt;\frac{n}{2}\) malicious adversaries. 
							</p>
						</section>
						<section id="sec2.4">
							<h3>Indistinguishability of Random Variables</h3>
							<p class="rp_original">
								Throughout this paper, we will only consider families of RVs \(U=\{U_k\}\) where the parameter \(k\) ranges in the natural numbers. Let \(U=\{U_k\}\) and \(V=\{V_k\}\) be two families of RVs. The following notion of computational indistinguishability expresses the fact that, when the length of \(k\) increases, \(U_k\) becomes “replaceable” by \(V_k\), in the following sense. A random sample is selected either from \(U\), or from \(V\), and it is handed to a “judge”. After studying the sample, the judge will proclaim his verdict: 0 or 1. (We may interpret 0 as the judge’s decision that that the sample came from \(U_k\); 1 as the decision that the sample came from \(V_k\).) It is then natural to say that \(U_k\) becomes “replaceable” by \(V_k\) for \(k\) large enough if, when \(k\) increases, the verdict of any computationally bounded judge becomes “meaningless”, that is essentially uncorrelated to which of the two distributions the sample came from.
							</p>
							<p class="rp_original">
								To formalize the notion of computational indistinguishability we make use of nonuniformity. Thus, our "judge", rather than polynomial time Turing machine, will be a <span class="definable">poly-size family of circuits</span>. That is a family \(C=\{C_k\}\) of Boolean circuits \(C_k\) with one Boolean output such that, for some constants \(e,d&gt;0\), all \(C_k\in C\) have at most \(k^e\) gates and \(k^d\) Boolean inputs. In order to feed samples from our probability distributions to such circuits, we will consider only <span class="definable" data-define="poly-bounded family of random variables">poly-bounded families of RVs</span>. That is families \(U=\{U_k\}\) such that, for some constant \(e&gt;0\), all RV \(U_k\in U\) assigns positive probability only to strings whose length is exactly \(k^e\). If \(U=\{U_k\}\) is a poly-bounded family of RVs and \(C=\{C_k\}\) a poly-bounded sequence of circuits, we denote by <span class="equation" data-equation="PUCk">\(P(U,C,k)\)</span> the probability that \(C_k\) outputs 1 on input a random string from \(U_k\). (Here we assume that the length of the strings that are assigned positive probability by \(U_k\) equals the number of Boolean inputs of \(C_k\).)
							</p>
							<span class="rp_definition_header" id="def1">Definition 1 - Computational Indistinguishability</span>
							<p class="rp_original rp_definition">
								Two <span class="definable" data-define="poly-bounded family of random variables">poly-bounded families of RV's</span> are <span class="definable">computationally indistinguishable</span> if, for all <span class="definable" data-define="poly-size family of circuits">poly-size families of circuits</span> \(C\), for all constants \(f&gt;0\) and all sufficiently large \(k\in N\),
							</p>
							<p class="rp_definition">
								$$|P(U,c,k)-P(V,C,k)|&lt;k^{-f}$$
							</p>
							<p class="rp_original">
								This notion was already used by Goldwasser and Micali<a href="#citation1"><sup class="reference" data-citation="1">[1]</sup></a> in the context of encryption, and by Yao<a href="#citation2"><sup class="reference" data-citation="2">[2]</sup></a> in the context of pseudo-random generation. For other notions of indistinguishability and further discussion see <a href="#citation3"><span class="reference" data-citation="3">[3]</span></a>.
							</p>
							<p class="rp_original">
								<b>Remark 1 - </b>
								Let us point out the robustness of the above definition. In this definition, we are handing out computationally bounded "judge" only samples of size 1. This, however, is not restrictive. It should be noticed that the two families of RVs \(\{U_k\}\) and \(\{V_k\}\) are computationally indistinguishable with respect to samples of size 1 if and only if they are computationally indistinguishable with respect to samples whose size is bounded by a fixed polynomial in \(k\).
							</p>
						</section>
					</section>
					<section id="sec3">
						<h2>Tm-games with Passive Adversaries</h2>
						<span class="rp_definition_header" id="def2">Definition 2 - Turing machine game</span>
						<p class="rp_original rp_definition">
							A <span class="definable" data-define="Turing machine game">Tm-game</span> problem consists of a pair \((\bar{M},1^k)\), that is, the description of a Turing machine \(M\) and an integer \(k\), the security parameter, presented in unary.
						</p>
						<p class="rp_original">
							Let us now make some simplifications that will expedite our exposition. Without loss of generality in our scenario, we assume that, when \((\bar{M},1^k)\) is the common input in a game network, all private inputs have the same length \(l\) and that \(T(l)\), the running time of \(M\) on inputs of size \(l\), is less than \(k\).
						</p>
						<p class="rp_original">
							Let \(S\in PDA\). We say that \(S\) is a <i>Tm-game solver for passive adversaries</i> if, for all Tm-game problems \((\bar{M},1^k)\) given as common input and for all (respective) private inputs \(x_1,\dots,x_n\),
						</p>
						<ol class="rp_original">
							<li value="1">
								<i>(Agreement constraint)</i>
								<p>
									At the end of each execution of \(S\), for all machines \(i\) and \(j\), \(i\)'s private output equals \(j\)'s private output.
								</p>
							</li>
							<li value="2">
								<i>(Correctness constraint)</i>
								<p>
									<span class="equation" data-equation="OSi">\(OS_1(x_1,\dots,x_n,(\bar{M},1^k))\)</span>\(=M(x_1,\dots,x_n)\), and
								</p>
							</li>
							<li value="3">
								<i>(Privacy constraint)</i>
								<p>
									\(\forall T\subset\{1,\dots,n\}\) and \(\forall A\in PPT, \exists B\in PPT\) such that \(\{A_k\}\) and \(\{B_k\}\) are computationally undistinguishable RVs. Here</p>
								<p>
									\(A_k=A((\bar{M},1^k),\)<span class="equation" data-equation="HS">\(HS((\bar{M},1^k))\)</span>,<span class="equation" data-equation="HST">\(HS_T((\bar{M},1^k))\)</span>\()\)
								</p>
								<p>and</p>
								<p>
								\(B_k=B((\bar{M},1^k),M(x_1,\dots,x_n),\{(i,x_i):i\in T\})\)
								</p>
							</li>
						</ol>
						<p class="rp_original">Let us now interpret the above definition</p>
						<h5>The agreement constraint</h5>
						<p class="rp_original">
							This constraint essentially says that all machines agree on a single, common string as the output of \(S\).
						</p>
						<h5>The correctness constraint</h5>
						<p class="rp_original">
							This constraint ensures that the output of a game solver \(S\) coincides with the one of \(M\). As \(M\) may be probabilistic, the equality of the correctness constraint must be interpreted as equality between RVs.
						</p>
						<h5>The privacy constraint</h5>
						<p class="rp_original">
							Notice that passive adversaries appear in the above definition in an implicit way. Algorithm \(A\) can be thought as all the members of \(T\) being passive adversaries computing after an execution of \(S\). In fact, passive adversaries are obliged to send messages according to \(S\) and their private history, in an execution of \(S\), as an explicit input to \(A\). Let us stress that the private history of a machine \(i\) contains the name \(i\), the private input \(x_i\), and \(M\)'s output as well. Thus the privacy constraint essentially says that whatever the passive adversaries may compute after computing \(S\), they could also easily deduce from the desired \(M\)'s output \(y\) and their own private inputs (which they are entitled to have!). In fact, if they are given \(y\) by running \(S\), the passive adversaries will see, in addition to \(y\), only the public history and their own private history. However, whatever they could efficiently compute with this additional input, they could also have computer without it. In other words, \(S\) keeps whatever privacy of the inputs of the good parties is not "betrayed" by the value of \(y\) itself. For instance, if \(M\) computes the sum of the \(x_i\)'s, then the privacy constraint will allow the adversarial players to compute (at the end of \(S\)) essentially only the sum of the inputs of the good parties. As for another example, if \(M\) is the identity function, then the privacy constraint holds vacuously. Same if the set \(T\) is the set of all players.
						</p>
					</section>
					<section id="sec4">
						<section>
							<p class="rp_original">
								At a first glance enforcing both correctness and privacy constraints of a <span class="definable">Tm-game</span> appears easy only for special cases of \(M\), say the ones computing a constant function. Nonetheless,
							</p>
							<span class="rp_theorem_header" id="theorem1">Theorem 1</span>
							<p class="rp_original rp_theorem">
								If <span class="definable" data-define="trapdoor function">trapdoor functions</span> exist, there exists a Tm-game solver for <span class="definable" data-define="passive adversary">passive adversaries</span>.
							</p>
							<p class="rp_original">
								In this extended abstract we limit ourselves to give a few indications, in an informal manner, about the proof of the above theorem. 
							</p>
						</section>
						<section id="sec4.1">
							<h3>A New and General Oblivious Transfer Protocol</h3>
							<p class="rp_original">
								In <a href="#citation4"><span class="reference" data-citation="4">[4]</span></a>, Rabin proposes the beautiful notion of an <span class="definable">oblivious transfer</span>. This is a probabilistic polynomial-time algorithm that allows A(lice), who knows the prime factorization of an integer \(n\), to send it to B(ob), who knows just \(n\), so that B will receive \(n\)'s factorization with probability &frac12; and A does not know whether or not B received it. Clearly, Rabin's notion of an OT supposes that factoring is computationally hard. Under this assumption, he proposed a protocol that, if A and B are allowed to be at most <span class="definable" data-define="passive adversary">passive adversaries</span>, correctly implements an OT. This protocol, however, may not work (i.e. no longer possesses a proof of correctness) if A and B are allowed to be malicious. Using the interactive proof-systems of <a href="#citation1"><span class="reference" data-citation="1">[1]</span></a>, Fischer, Micali, Rackoff, and Wittenberg found<a href="#citation5"><sup class="reference" data-citation="5">[5]</sup></a> a protocol that correctly implements OT under the simple (and in this context minimal) assumption that factoring is hard. Rabin's OT has proved to be a very fruitful notion, as exemplified by various applications proposed by Blum.<a href="#citation6"><sup class="reference" data-citation="6">[6]</sup></a>
							</p>
							<p class="rp_original">
								A more general and useful notion of OT has been proposed by Even, Goldreich, and Lempel<a href="#citation7"><sup class="reference" data-citation="7">[7]</sup></a>, the <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span>. In their framework, A has two messages, \(m_0\) and \(m_1\). By using a cryptosystem \(E\) she computes \(\sigma_0=E(m_0)\) and \(\sigma_1=E(m_1)\) and sends \(\sigma_0, \sigma_1\) to B. B chooses one of these encryptions, \(\sigma_i\). A one-out-of-two OT allows B to read the corresponding message \(m_i\), while A will not know which message B has read (whenever \(m_0\neq m_1\)). This notion achieves the right level of generality and is crucial to what follows. Even, Goldreich, and Lempel also proposed the first implementation of a one-out-of-two OT using public-key cryptosystems. Their protocol has the merit of having freed the implementation of an oblivious transfer from the algebraic setting to which it appeared to be confined. Their protocol, though, requires a quite strong set of assumptions even when the adversaries are only passive.
							</p>
							<p class="rp_original">
								Below, we contribute a new protocol that correctly implements a <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span> in presence of <span class="definable" data-define="passive adversary">passive adversaries</span>. The existence of <span class="definable" data-define="trapdoor permutation">trapdoor permutations</span> suffices to prove the correctness of our protocol.
							</p>
							<section>
								<h4>Trapdoor and One-Way Functions</h4>
								<p class="rp_original">
									A satisfactory definition of a <span class="definable" data-define="trapdoor permutation">trapdoor permutation</span> is given in <a href="#citation8"><span class="reference" data-citation="8">[8]</span></a>. Here, let us informally say that a family of trapdoor permutations \(f\) possesses the following properties:
								</p>
								<ul class="rp_original">
									<li>It is easy, given an integer \(k\), to randomly select permutations \(f\) in the family which have \(k\) as their security parameter, together with some extra "trapdoor" information allowing easy inversion of the permutations chosen.</li>
									<li>It is easy to randomly select a point in \(f\)'s domain.</li>
									<li>It is hard to invert \(f\) without knowing \(f\)'s trapdoor on a random element in \(f\)'s domain.</li>
								</ul>
								<p class="rp_original">
									We can interpret the above by saying that a party A can randomly select a pair of permutations, \((f,f^{-1})\), inverses of each other. This will enable A to easily evaluate and invert \(f\); if not A publicizes \(f\) and keeps secret \(f^{-1}\), then inverting \(f\) will be hard for any other party. We may write \(f_k\) to emphasize that \(k\) is the security parameter of our permutation.
								</p>
								<p class="rp_original">
									Trapdoor permutations are a special case of <span class="definable" data-define="one-way permutation">one-way permutations</span>. These are permutations enjoying the three properties above, except that we do not insist that the trapdoor information exists.
								</p>
							</section>
							<section>
								<h4>Random Bits in One-Way Permutations</h4>
								<p class="rp_original">
									Our <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span> protocol makes use of the trapdoor functions \(f\) hiding a random bit \(B_f\). Here \(B_f\) is a polynomial-time computable Boolean function; the word "bit" is appropriate as \(B_f\) evaluates to 1 for half of the values in \(f\)'s domain.
								</p>
								<p class="rp_original">
									We say that \(\{B_f\}\) is a <span class="definable" data-define="hardcore bit">random bit</span> in a family \(\{f\}\) of trapdoor permutations if \(\forall\) predicting algorithm \(Alg\) that, on inputs \(f=f_k\) and \(f(x)\), outputs, in \(T(k)\) steps, a guess for \(B_f((x))\) that is correct with probability \(\epsilon, \exists Alg'\) that, on inputs \(f\) and \(f(x)\), outputs \(x\) in <span class="equation" data-equation="polytk">\(poly(T(k),\epsilon^{-1})\)</span> expected time.
								</p>
								<p class="rp_original">
									Thus, \(f\) being trapdoor, no probabilistic, polynomial-time algorithm given \(f_k(x)\), can correctly predict \(B_{f_k}(x)\) with probability \(&gt;\frac{1}{2}+\frac{1}{poly(k)}\). We might as well flip a coin. Thus, for a one-way permutation \(f\), given \(f(x)\) the value of \(B_f(x)\) cannot be guessed in polynomial time essentially better than at random.
								</p>
								<p class="rp_original">
									Then notion of a random bit in a one-way permutation was introduced by Blum and Micali<a href="#citation9"><sup class="reference" data-citation="9">[9]</sup></a> who showed a random bit in the <span class="definable" data-define="discrete logarithm problem">Discrete Logarithm Problem</span>, a well known candidate for one-way permutation. Chor and Goldreich show random bits in the RSA function. Do all one-way functions have a random bit? We do not know the answer to this question, but Yao<a href="#citation2"><sup class="reference" data-citation="2">[2]</sup></a> has shown the next best thing. Namely, that given a one-way (trapdoor) permutation \(f\), one can construct a one-way (trapdoor) permutation \(F\) with random bit \(B_F\) (For a detailed proof of this theorem see <a href="#citation10"><span class="reference" data-citation="10">[10]</span></a>). Levin<a href="#citation11"><sup class="reference" data-citation="11">[11]</sup></a> has actually proved a more general version of this theorem.
								</p>
							</section>
							<section id="protocol1">
								<h4>Our Protocol</h4>
								<p class="rp_original">
									Without loss of generality, we assume that the two messages in the <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span> both consist of a single bit.
								</p>
								<p class="rp_original">
									In our protocol, both \(A,B\in PA\). \(A\)'s inputs are a pair of bits \((b_0,b_1)\) and their corresponding pair of encryptions \((E(b_0),E(b_1))\) where \(E\) is a probabilistic encryption algorithm<a href="#citation1"><sup class="reference" data-citation="1">[1]</sup></a>. The pair \((E(b_0),E(b_1))\) is also an input to \(B\) who has an additional private input bit \(\alpha\). It is desired that, even if some party is a passive adversary, the following two properties hold:
								</p>
								<ol type="i" class="rp_original">
									<li>\(B\) will read the bit \(b_\alpha\), but will not be able to predict the other bit \(b_\bar{\alpha}\) essentially better than at random.</li>
									<li>\(A\) cannot predict \(\alpha\) better than at random.</li>
								</ol>
								<p class="rp_original">
									We achieve this by means of the following protocol:
								</p>
								<ol class="rp_original">
									<li value="1">
										\(A\) randomly selects \((f,f^{-1})\), a trapdoor function of size \(k\) (having a random bit \(B_f\)) together with its inverse. She keeps \(f^{-1}\) secret and sends \(f\) to \(B\).
									</li>
									<li value="2">
										\(B\) randomly selects \(x_0\) and \(x_1\) in \(f\)'s domain and computes \(z=f(x_0)\) and sends \(A\) the pair
										$$(u,v)=\begin{cases}(f(x_0),x_1)&\mbox{if } \alpha =0\\(x_0,f(x_1))&\mbox{if }\alpha =1\end{cases}$$
									</li>
									<li value="3">
										\(A\) computes \((c_0,c_1)=(B_f(f^{-1}(u)),B_f(f^{-1}(v)))\). She sets \(d_0=b_0\bigoplus c_0\) and \(d_1=b_1\bigoplus c_1\) and sends \((d_0,d_1)\) to \(B\).
									</li>
									<li value="4">
										\(B\) computes \(b_\alpha=d_\alpha\bigoplus B_f(x_\alpha)\)
									</li>
								</ol>
								
								<p class="rp_original">
									First notice that \(A,B\in PA\) and that \(B\) correctly reads \(b_\alpha\). Property i) is satisfied as \(B\) only sees \(b_\bar{\alpha}\) exclusive-ored with a bit essentially 50-50 unpredictable to him. Thus he cannot correctly guess \(b_\bar{\alpha}\) essentially better than at random. Let us now show that ii) holds. As \(f\) is a permutation, randomly selecting \(x\) in \(f\)'s domain and computing \(f(x)\) yields a randomly selected element in \(f\)'s domain. Thus \((u,v)\) is a pair of randomly selected elements in \(f\)'s domain both if \(\alpha=0\) or \(\alpha=1\). As \((u,v)\) is the only message \(B\) sends to \(A\), not even with infinite computing power will \(A\) find out whether \(B\) has read \(b_0\) or \(b_1\).
								</p>
								<p class="rp_original">
									Notice that the protocol makes use of the fact that the adversaries are at most <span class="definable" data-define="passive adversary">passive</span> in a crucial way. Should, in fact, \(B\) send \((u,v)=(f(x_0),f(x_1))\) in step 2, he will easily read both bits. Thus, we will make use of additional ideas to handle <span class="definable" data-define="malicious adversary">malicious adversaries.</span>
								</p>
								<p class="rp_original">
									Notice also that we never made use of the encryptions \(E(b_0)\) and \(E(b_1)\). \(b_0\) and \(b_1\) could have been bits in "\(A\)'s mind." We have added these encryptions for uniformity with the next protocol in which the two messages must appear encrypted. Another reason is that, when we will handle <span class="definable" data-define="malicious adversary">malicious adversaries</span>, we will need these encryptions to define the problem.
								</p>
								<p class="rp_original">
									It is easy to see that, having solved the single-bit messages case, we have also solved the case of arbitrary messages \(m_0\) and \(m_1\) of equal, known length \(l\). In fact, we can repeat the above protocol \(l\) times so that, if \(\alpha\) is 0 (1), \(B\) is required at the \(i\)-th time to learn the \(i\)th bit of \(m_0 (m_1)\).
								</p>
							</section>
							
						</section>
						<section id="sec4.2">
							<h3>Strengthening Yao's Combined Oblivious Transfer</h3>
							<section>
								<p class="rp_original">
									In <a href="#citation12"><span class="reference" data-citation="12">[12]</span></a>, Yao presented a protocol that we call <span class="definable">combined oblivious transfer</span> (COT). The protocol involves two parties A and B, respectively owning private inputs \(a\) and \(b\) and any chosen function \(g\). It possesses the following property: upon termination, A computes \(g(a,b)\), while B has no idea of what A has computed. If we think of \(a\) and \(b\) as secrets, B appeats to be obliviously transferring a prescribed combination of his and A's secret to A. Yao implemented COT based on the assumption that factoring is hard, (which yields, as shown by <a href="#citation6"><span class="reference" data-citation="6">[6]</span></a>) a particular trapdoor permutation. We strengthen his result by showing that COT can be correctly implemented based on any trapdoor permutation. We do this by using the <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span> of section 4.1 in Yao's scheme. Let us consider first the case where \(a\) and \(b\) are bits and \(g\) is the Boolean \(AND\). Consider figure 1.
								</p>
								<img src="../../img/81.fig1.jpg" alt="A COT AND-gate" class="rp_original" />
								<p class="rp_original">
									Here \(E_1,\dots,E_6\) are independently selected encryption algorithms, respectively having decryption keys \(D_1,\dots,D_6\). \(E_1\) and \(E_2\) label the first input-wire, \(E_3\) and \(E_4\) the second input-wire, and \(E_5\) and \(E_6\) the output-wire. Each row in the gate is formed by the encryption of two strings. \(m\) and \(n\) are two randomly selected strings whose bit-by-bit exclusive-or equals \(D_5\). \(p\) and \(q\) are two randomly selected strings whose xor equals \(D_6\); so are \(s\) and \(t\); so are \(u\) and \(v\). The 4 rows have been put in the gate in random order. \(E_1E_2\) and \(E_5E_6\) are publicly labelled by complementary bits. \(E_3\) and \(E_4\) are each secretly labelled by a bit; more precisely, \(E_3\) is SECRETLY labelled 0 with probability &frac12; and \(E_4\) is labelled with the complement of \(E_3\)'s bit. (This secrecy is pictorially indicated by drawing \(E_3\) and \(E_4\)'s bits by a dotted line.) Define the value of a wire to be 0 (1) if one ONLY possesses the decoding algorithm of encryption algorithm labelled 0 (1). Then figure 1 is an or-gate. For instance, assume that both the input-wires have value 0. That is, one possesses only \(D_1\) and \(D_4\). Then one is able to decrypt both entries only in the third row. By taking the xor of \(u\) and \(v\), one easily obtains \(D_6\), but has no idea what \(D_5\) may be. Thus the output-wire has value \(0=AND(0,0)\).
								</p>
								<p class="rp_original">
									To COTransfer \(AND(a,b)\), B generates a <span class="definable" data-define="combined oblivious transfer">COT</span> AND-gate like in figure 1, keeping for himself all decoding algorithms and all the strings in the rows. Then he gives A the decoding algorithm of the second input-wire that corresponds to the value of \(b\), his own input. Notice that as the association between \(E_3,E_4\) and 0,1 is scret (and \(E_1,E_2,E_3,E_4\) enter symmetrically in the gate rows), this will not betray \(b\) at all. Now A will get either \(D_1\) or \(D_2\) according to the value of \(a\) by means of our <span class="definable" data-define="one-out-of-two oblivious transfer">one-out-of-two OT</span>. Thus, B will not know which algorithm she got. At this point A can easily compute the value of the output wire. Thus, she will be the only one to know \(AND(a,b)\).
								</p>
								<p class="rp_original">
									It is trivial to build a <span class="definable" data-define="combined oblivious transfer">COT</span> NOT gate. Notice that B may also keep secret the corresponding between 0,1 and \(E_5,E_6\). 
								</p>
								<img src="../../img/81.fig2.jpg" alt="A COT NOT-gate" class="rp_original" />
								<p class="rp_original">
									This allows the output wire to become an input wire of another gate. If the encryption algorithms of this second gate are publically labelled 0/1 (see fig. 2), we see that A may evaluate any 2-gates function on her and B's inputs, without knowing intermediate results. Better said, B can "COTransfer" the value of any 2-gates function. By cascading this way COT AND-gates and COT NOT-gates (which are trivial to design), we can see that B can COTransfer the value of any function, provided that there is an upper bound to the length of A's and B's inputs, (else, the length of the inputs will be betrayed).
								</p>
							</section>
						</section>
						<section id="sec4.3">
							<h3>The Tm-game Solver for passive adversaries</h3>
							<p class="rp_original">
								Recall that a <span class="definable" data-define="Turing machine game">Tm-game</span> solver wants to compute \(M(x_1,\dots,x_n)\) while respecting the privacy constraint. We want to use <span class="definable" data-define="combined oblivious transfer">COT</span> as a subroutine to construct a Tm-solver. This does not appear to be straightforward. For instance, if two parties \(i\) and \(j\) use COT so that \(i\) will compute \(g(x_i,x_j)\) for some function \(g\), this would already be a violation of the privacy constraint. Recall also that the Tm-game solver has to be polynomial not only in \(M\)'s running time, but also in \(n\), the number of players.
							</p>
							<p class="rp_original">
								We find a way out by making special use of a lemma of Barrington's<a href="#citation13"><sup class="reference" data-citation="13">[13]</sup></a> that simulates computation by composing permutations in \(S_5\), the symmetric group on 5 elements. The general picture is the following. First, transform the <span class="definable">Turing machine</span> \(M\) of a <span class="definable" data-define="Turing machine game">Tm-game</span> to an equivalent circuit \(C\) in a standard way. The Boolean inputs of \(C\) will be \(b_1^1,\dots,b_l^1,\dots,b_1^n,\dots,b_l^n\), the bits of the \(n\), \(l\)-bit long inputs of our parties. This circuit \(C\) is then transformed to <span class="definable">straight-line program</span> as in <a href="#citation13"><span class="reference" data-citation="13">[13]</span></a>. This straight-line program is essentially as long as \(C\) is big. In it,
							</p>
							<ul class="rp_original">
								<li>0,1 are encoded by two (specially selected) 5-permutations</li>
								<li>the variables range in \(S_5\) and</li>
								<li>each instruction consists of multiplying (composing) two 5-permutations \(\sigma\) and \(\tau\), where \(\sigma\) (\(\tau\)) is either a constant, a variable, or the inverse (in \(S_5\)) of a variable.</li>
							</ul>
							<p class="rp_original">
								At the start, each party takes each of his private bits and encodes it by a 5-permutation \(\sigma\) as in <a href="#citation13"><span class="reference" data-citation="13">[13]</span></a>. Then he divides \(\sigma\). That is, he selects at random \(n-1\) 5-permutations \(\sigma_1,\dots,\sigma_{n-1}\) and gives the pair \((i,\sigma_i)\) to party \(i\) (possibly himself). He then sets \(\sigma_n=(\sigma_1\cdots\sigma_{n-1})^{-1}\cdot\sigma\) and gives \((n,\sigma_n)\) to party \(n\). Now, inductively, assume that each variable is divided among the parties. That is, for each variable \(\sigma\), each player \(i\) possesses an index permutation pair \((x,\sigma_x)\) so that \(\prod\limits_{x=1}^n\sigma_x=\sigma\) and, given only \(n-1\) pieces, \(\sigma\) cannot be guessed better than at random. We now want to show that each instruction can be performed (i.e. that each party can compute his individual piece of the result) respecting the privacy constraint. There are essentially 3 cases.
							</p>
							<p class="rp_original">
								<i>Case 1</i>: The instruction is of the form \(\sigma\cdot c\), where \(\sigma\) is a variable and \(c\) a constant. By induction, each party has a piece of the form \((x,\sigma_x)\). Then the party owning the piece \((n,\sigma_n)\) sets his new piece to be \((n,\sigma_n\cdot c)\) and all other parties leave their pieces untouched. It is immediately check that the ordered product of the new pieces is \(\sigma\cdot c\) and that privacy has been preserved against \(n-1\) <span class="definable" data-define="passive adversary">passive adversaries</span>.
							</p>
							<p class="rp_original">
								<i>Case 2</i>: The instruction is of the form \(\sigma^{-1}\cdot c\) where, again, \(\sigma\) is a variable and \(c\) a constant. It will be enough to show how to compute pieces for \(\sigma^{-1}\) respecting the privacy constraint. To do this, if a party has a piece \((x,\sigma_x)\), he sets his new piece to be \((n-x+1, \sigma_x^{-1})\).
							</p>
							<p class="rp_original">
								<i>Case 3</i>: the instruction is of the form \(\sigma\cdot\tau\), where both \(\sigma\) and \(\tau\) are variables. Then \(\sigma\cdot\tau=\sigma_1\cdots\sigma_n\cdot\tau_1\cdots\tau_n\), and assume for simplicity that party \(i\) possesses piece \(\sigma_i\) and \(\tau_i\). Unfortunately, party 1 cannot compute his piece of \(\sigma\cdot\tau\) by multiplying his own two pieces. In fact, they are \(n\) positions apart in the product and \(S_5\) is not commutative (a fact crucial in Barrington's argument). The idea will then consist of making "partial progress". That is, moving party 1's pieces closer together by "swapping" \(\sigma_n\) and \(\tau_1\). This can be correctly accomplised by giving party 1 a piece \(\tau_1'\) and party \(n\) a piece \(\sigma_n'\) so that \(\tau_1'\cdot\sigma_n'=\sigma_n\cdot\tau_1\). This way the product of the new (and newly ordered pieces) would remain \(\sigma\cdot\tau\). One way of doing this would be of having party 1 and party \(n\) tell each other \(\sigma_n\) and \(\tau_1\). However, this would violate the privacy constraint with respect to a set of \(n-1\) passive adversaries. Instead, we use <span class="definable" data-define="combined oblivious transfer">COT</span> in the following way. Party \(n\) randomly selects a 5-permutation \(\rho\). Consider now the function \(g\) such that, for 5-permutations \(x\),\(y\), and \(z\), \(g(x,(y,z))=w\) where \(w\cdot z=y\cdot x\). Let now party 1 (with the role of A and input \(a=\tau_1\)) and party \(n\) (with the role of B and input \(b=(\sigma_n,\rho)\)) play COT with the function \(g\). Set \(\tau_1'=g(a,b)\) and \(\sigma_n=\rho\). Then we have made the desired partial progress. In fact, not only the product of the new pieces is unaltered, but we have also respected the privacy constraint. Informally, party \(n\)'s new piece is a random 5-permutation selected by party \(n\) himself and thus cannot give him and information neither about party 1's old piece nor the new one; moreover the transference of \(g(a,b)\) is oblivious and thus cannot give party \(n\) any knowledge either. On the other side, party 1 is dealt a new piece \(g(\tau_1,(\sigma_n,\rho))\) and he knows \(\tau_1\). However, as for all \(x\) and \(y\), \(g(x,(y,\cdot))\) is injective on \(S_5\), and \(\rho\) has been randomly and secretly selected by party \(n\), als o party 1 does not get any knowledge that he did not possess before! Notice also that during this "swap" we did not create any other pieces. Thus, after \(n\) "swaps" the only two pieces of party 1 will be in the first two positions in the product and he can thus multiply them together. This product will be party 1's piece for the variable \(\sigma\cdot\tau\). It should be verified that the entire walk of party 1 \(\tau\)-piece towards the left preserves correctness and does not violate the privacy constraint. Essentially because a new, random piece is created at each step. This way, after \(O(n^2)\) "swaps", and in polynomial time, all parties receive their piece of \(\sigma\cdot\tau\).
							</p>
							<p class="rp_original">
								At the end of the <span class="definable">straight-line program</span>, for each output variable \(\gamma\), each party publicizes his own piece \((x,\gamma_x)\), the ordered product of these pieces is computed, and the output bit recovered so as to satisfy both the correctness and the privacy constraint. (A more formal argument will be given in the final paper.)
							</p>
						</section>
					</section>
					<section id="sec5">
						<h2>Malicious Adversaries</h2>
						<p class="rp_original">
							The complexity of our <span class="definable" data-define="Turing machine game">Tm-game</span> solver greatly increases when up to half of the players is allowed to be malicious and can more powerfully collaborate to try to disrupt the correctness and the privacy constraints. We used essentially all the cryptographic tools developed in the last ten years in the (correct) hope that they would make possible protocol design. Also, the proof of its correctness is rather delicate and unsuitable for an abstract. We will give it in the final paper. Here we only indicate what making playable at Tm-game with <span class="definable">malicious adversaries</span> may <i>mean</i> and which general ideas are involved in our solution.
						</p>
						<p class="rp_original">
							As in this case some of the parties may not follow their prescribed programs at all, it is necessary to clarify what a private input is. After all, what stops someone from pretending that his private input is different from what it actually is? To avoid this, we assume that the parties have established their private inputs by <span class="definable" data-define="commitment">announcing correct encodings</span> of them. Their inputs are <i>by definition</i> the unique decryption of their respective encodings. Moreover, it should be clear that seeking a solution to a <span class="definable" data-define="Turing machine game">Tm-game</span> problem makes sense only if the parties are "willing to play". If, say, one of them "commits suicide", carrying with himself what his private input was, there is very little one can do besides investing exponential time and break his encryption. However we can, loosely speaking, prove that:
						</p>
						<span class="rp_theorem_header" id="theorem2">Theorem 2</span>
						<p class="rp_original rp_theorem">
							Given \(n\) players willing to play, less than half of which are <span class="definable" data-define="malicious adversary">malicious</span>, all <span class="definable" data-define="Turing machine game">Tm-games</span> are playable.
						</p>
						<p class="rp_original">
							The above term "willing to play" indicates a technical condition rather than a psychological one. Namely, having successfully completed the <span class="definable">engagement protocol</span>. After completing this protocol, all players can be forced to play any desired game. The engagement protocol consists of two phases.
						</p>
						<ol class="rp_original">
							<li value="1">
								For each player \(i\), a protocol is performed at the end of which no minority of the players can even predict a bit of \(i\)'s private input with chances essentially better than &frac12;. However, it is <i>guaranteed</i> that any subset of cardinality \(&gt;n/2\) can, without the cooperation or even against the actions of other players, easily compute \(i\)'s private input.
							</li>
							<li value="2">
								The community deals to each player a sequence of encrypted "random" bits so that, a) the recipient knows their decryption, b) they appear unpredictable to any minority of the players, but c) they are easily computable by any majority of the players.
							</li>
						</ol>
						<p class="rp_original">
							We stress that, while no one can be forced to complete the <span class="definable">engagement protocol</span> (so to become "willing to play"), no one can decide not to complete it because he received a better idea of what the result of the subsequent game may be. Completing the engagement protocol will not give any player (or any small enough group of players) any knowledge about the others' private inputs.
						</p>
						<p class="rp_original">
							Phase 1 of the <span class="definable">engagement protocol</span> consists of a verifiable secret sharing in the sense of Awerbuch, Chor, Goldwasser, and Micali<a href="#citation14"><sup class="reference" data-citation="14">[14]</sup></a>. However, we contribute a new protocol both tolerating up to \(\frac{n}{2}\) <span class="definable" data-define="malicious adversary">malicious adversaries</span> and using any <span class="definable">trapdoor function</span> whatsoever. Phase 2 of the engagement protocol is the multi-party version of Blum's <span class="definable" data-define="coin flip">coin flipping</span> by telephone<a href="#citation6"><sup class="reference" data-citation="6">[6]</sup></a>. Despite the (deceptive) similarity with the verifiable secret sharing of phase 1, to implement phase 2 we must make use of a yet unpublished theorem (and algorithm) of ACGM.
						</p>
						<p class="rp_original">
							We now give a bird's eye view of how to make any <span class="definable" data-define="Turing machine game">Tm-game</span> \(g\) playable despite <span class="definable" data-define="malicious adversary">malicious adversaries</span>. On input \(M,1^k\), we first run the <span class="definable">engagement protocol</span>, then the <span class="definable" data-define="passive adversary">passive-adversary</span> playable version of the Tm-game. Here, we require all parties to use, as their private inputs, the strings they shared in phase 1 of the engagement protocol and, as a source of randomness, the encrypted random bits each was dealt in phase 2. The key point is that, now, no malicious adversary can deviate from his prescribed program, and thus he becomes a simple passive adversary. In fact, he is required to prove, in <span class="definable" data-define="zero knowledge proof">zero-knowledge</span> (in the sense of the Goldwasser, Micali, and Rackoff<a href="#citation8"><sup class="reference" data-citation="8">[8]</sup></a>), that each message he sends is what he should have sent being honest, given his private input, his random choices, and the messages he received so far. (Here, an essential tool is our recent result that all <span class="definable">NP</span> languages possess zero-knowledge proofs<a href="#citation15"><sup class="reference" data-citation="15">[15]</sup></a>.) If a malicious party, frustrated at not being able to send messages according to a different program, decides to stop, his input and random bits will be reconstructed by the community who will compute his messages when necessary, without skewing the probability distribution of the final outcome.
						</p>
						<p class="rp_original">
							We would like to stress our new use of <span class="definable" data-define="NP-complete">NP-completeness</span>. From being our most effective way to prove lower-bounds, it now becomes our most effective tool to construct correct protocols.
						</p>
					</section>
					<section id="sec6">
						<h2>General Games</h2>
						<p class="rp_original">
							Many actions in life, like negotiating a contract, casting a vote in a ballot, playing cards, bargaining in the market, submitting a STOC abstract, driving a car, or simply living, may be viewed as participating with others in a game with pay-offs/penalties associated with its results. This is not only true for individuals, but also for companies, governments, armies etc. that are engaged in financial, political, and physical struggles. Despite the diversity of these games, all of them can be described in the elegant mathematical framework laid out by Von Neumann and Morgenstern earlier in this century. Game theory, however, exhibits a "gap", in that it neglected to study whether, or how, or under which conditions, games can be <i>implemented</i>. That is, it never addressed the question of whether, given the description of a game, a method exists for physically or mentally playing it. We do fill this gap by showing that, in a complexity theoretic sense, all games can be played.
						</p>
						<p class="rp_original">
							In this extended abstract we will only informally clarify what and how this is. We start by briefly recalling the ingredients used by game theory to model an \(n\)-player game with incomplete information.
						</p>
						<section id="sec6.1">
							<h3>Games</h3>
							<p class="rp_original">
								Essentially, a game consists of a set \(S\) of possible <i>states</i>, representing all possible instantaneous descriptions of the game, a set \(M\) of possible <i>moves</i>, describing all possible ways to change the current state of the game, a set \(\{K_1,K_2,\dots,K_n\}\) of <span class="definable" data-define="knowledge function">knowledge functions</span>, where \(K_i(\sigma)\) represents the partial information about state \(\sigma\) possessed by player \(i\), and a function \(p\), the <span class="definable">pay-off function</span> that, evaluated on the final state, tells the outcome of the game. Without loss of generality, the players make moves in cyclic order and the set of possible moves in any state are the same for all states. Also <span class="definable">WLOG</span>, the game goes on for a fixed number of moves \(m\). With little restriction we do assume that the players make use of recursive strategies for selecting their moves. (The classical model does not rule out selecting moves according to an infinite table.)
							</p>
							<p class="rp_original">
								Let us now see how a game evolves using, <del>in parenthesis</del><ins>as annotations</ins>, poker as an example. The game starts by having "NATURE" select an initial state \(\sigma_1\).
							</p>
							<p class="rp_annotation">
								For poker, \(\sigma_1\) is a randomly selected permutation of the 52 cards; the first \(5n\) cards of the permutation representing the players initial hands and the remaining ones the deck.
							</p>
							<p class="rp_original">
								Player 1 moves first. He does not know \(\sigma_1\)&mdash;nor does anybody else&mdash;he only knows \(K_1(\sigma_1)\), his own hand.
							</p>
							<p class="rp_annotation">
								...the first 5 elements of permutation \(\sigma_1\).
							</p>
							<p class="rp_original">
								Based solely on \(K_1(\sigma_1)\), he will select a move \(\mu\).
							</p>
							<p class="rp_annotation">
								E.g. he changes 3 cards with the first 3 cards of the deck.
							</p>
							<p class="rp_original">
								This move automatically updates the unknown current state to \(\sigma_2\).
							</p>
							<p class="rp_annotation">
								The new state consists of the cards currently possessed by each player, the sequence of cards in the deck, and which cards were discarded by player 1. \(K_1(\sigma_2)\) consists of the new hand of player 1 and the cards he just discarded.
							</p>
							<p class="rp_original">
								Now it is the turn of player 2. He also does not know the current state \(\sigma_2\), he only knows \(K_2(\sigma_2)\). Based solely on this information, he selects his move, which updates the current state, and so on. After the prescribed number of moves, the <span class="definable">pay-off function</span> \(p\) is evaluated at the final state to compute the result of the game.
							</p>
							<p class="rp_annotation">
								In poker the result consists of who has won, how much he has won, and how much everyone else has individually lost.
							</p>
							<p class="rp_original">
								Note that a <span class="definable" data-define="Turing machine game">Tm-game</span> is indeed a game in which the initial state is empty and each player moves only once. State \(\sigma_i\) consists of the sequence of the first \(i\) moves. Each player has no knowledge about the current state and chooses his move to be the string \(x_i\), his own private input. The <span class="definable">pay-off function</span> \(M\) is then run on \(\sigma_n\). (Having probabilistic machines running on the final state, rather than deterministc ones, is a quite natural generalization.)
							</p>
							<p class="rp_original">
								From this brief description it is immediately apparent that, by properly selecting the <span class="definable" data-define="knowledge function">knowledge functions</span>, one can enforce any desired  "privacy" constraints in a game.
							</p>
						</section>
						<section id="sec6.2">
							<h3>Playable Games</h3>
							<p class="rp_original">
								Game theory, besides an elegant formulation, also suggests to the players strategies satisfying some desired property (e.g. optimality). That is, game theory's primary concern is hot TO SELECT MOVES WELL. However, and ironically!, it never addressed the question of how TO PLAY WELL. For a general \(n\)-player game, all we can say is that we need \(n+1\) parties to properly play it; the extra party being the "<span class="definable">trusted party</span>". The trusted party communicates privately with all players. At step \(t\), he knows the current state \(\sigma_t\) of the game. He kindly computes \(\alpha=K_{t\bmod{n}}(\sigma_t)\), communicates \(\alpha\) to player \(t\bmod{n}\), receives from him a move \(\mu\), secretly computes the new state \(S_{t+1}=\mu(S_t)\), and so on. At the end, the trusted party will evaluate the <span class="definable">pay-off function</span> on the final state and declare the outcome of the game. Clearly, playing with the trusted party achieves exaclty the privacy constraints of the game description, and at each player will get the correct outcome.
							</p>
							<p class="rp_original">
								Now, the fact that, in general, an \(n\)-person game requires \(n+1\) people to be played, not only is grotesque, but also diminishes the otherwise wide applicability of game theory! In fact, in real-life situations, we may simply not have any trusted parties, whether persons or public computers. Recently, complaints have been raised about financial transactions in the stock market. The complaints were about the fact that some parties were enjoying knowledge that was considered "extra" before choosing their move, i.e. before buying stocks. Just another game, the stock market, but one in which you may desire trusting no one!
							</p>
							<p class="rp_original">
								We are thus led to consider the notion of a <span class="definable" data-define="purely playable game">(purely) playable</span> game. This is an \(n\)-person game that can be implemented by the \(n\) players without invoking any <span class="definable" data-define="trusted party">trusted parties</span>. In general, however, given the specification of a game with complicated <span class="definable" data-define="knowledge function">knowledge functions</span>, it is not at all easy to decide whether it is playable in some meaningful way. Here, among the "meaningful ways", we also include non-mathematical methods. Yet the decision may still not be easy.
							</p>
							<p class="rp_original">
								Poker, for instance, has simple enough <span class="definable" data-define="knowledge function">knowledge functions</span> (i.e. privacy constraints) that makes it playable in a "physical" way. In it, we use cards with cards with equal "back" and "opaque", tables whose top does not reflect light too much, we shuffle the deck "a lot", and we hand cards "facing down". All this is satisfactory as, in our physical model (the world), we only see along straight lines. However, assume we define NEWPOKER as follows. A player may select his move not only based on his own hand, but also on the knowledge of whether, combining the current hands of all players, one may form a <span class="definable" data-define="royal flush">royal flush</span>. NEWPOKER is certainly a game in the Von Neumann framework, but it is no longer apparent whether any physical realization of the game exists, particularly if some of the players may be cheaters.
							</p>
							<p class="rp_original">
								This is what we perceive lacking in game theory: the attention to the notion of playability. At this point, a variety of good questions naturally arises:
							</p>
							<p class="rp_original" style="text-align:center;font-style:italic">
								Is there a model (physical or mathematical) which makes all games playable?
							</p>
							<p class="rp_original">
								Or at least,
							</p>
							<p class="rp_original" style="text-align:center;font-style:italic">
								Does every game have a model in which it is playable?
							</p>
							<p class="rp_original">
								And if not,
							</p>
							<p class="rp_original" style="text-align:center;font-style:italic">
								Does every game have a model in which it is playable?
							</p>
							<p class="rp_original">
								We show that the first question can be affirmatively answered in a computational complexity model.
							</p>
						</section>
						<section id="sec6.3">
							<h3>A General Result</h3>
							<span class="rp_theorem_header" id="theorem3">Theorem 3</span>
							<p class="rp_original rp_theorem">
								If any <span class="definable">trapdoor function</span> exists, any game is <span class="definable" data-define="purely playable game">playable</span> if more than half of the players are honest.
							</p>
							<p class="rp_original">
								Essentially our result consists of a protocol for simulating the <span class="definable" data-define="trusted third party">trusted party</span> of an ideal game. That is, if more than half of the players follow our protocol, whatever a player (or a set of players of size less than \(\frac{n}{2}\)) knows at any step of the game, he would have also known in an ideal execution of the game with a trusted party. In our context, the knowledge constraints are satisfied in a computational complexity sense. Namely, any player (or collection of dishonest players), in order to compute anything more than his or her due share of the current state, should perform an exponential-time computation. Unfortunately, we cannot in this extended abstract elaborate on the relationship between general games and <span class="definable" data-define="Turing machine games">Tm-games</span>, nor how to pass from solving the latter ones to solve the general case. We'll do this in the final paper.
							</p>
						</section>
						<section id="sec6.4">
							<h3>A Completeness Theorem for Fault-Tolerant Computation</h3>
							<p class="rp_original">
								Our main theorem has direct impact to the field of fault-tolerant computation. This is so because protocols, when properly formalized (which we will do in the final paper), are games with partial information. Thus, as long as the majority of the players is honest, all protocols may be correctly played. Actually, slightly more strongly, the correct way to play a game can be found in a uniform manner. Namely, we exhibit a specific, efficient algorithm that, on input a protocol problem, outputs an efficient, distributed protocol for solving it.
							</p>
							<p class="rp_original">
								It should be noticed that, before this, only a handful of multiparty protocol problems were given a satisfactory solution (e.g. collective <span class="definable" data-define="coin flip">coin flipping</span> and poker over the telephone, secret exchange, voting, and a few others). Moreover the security of some of these solutions crucially depended on the "trapdoorness" of specific functions satisfying some additional, convenient property (e.g. <span class="definable">multiplicativity</span>). By contrast, our completeness theorem is proved based on <i>any</i> <span class="definable">trapdoor function</span> (multiplicative or not, <span class="definable" data-define="associativity">associative</span> or not, etc.). That is, we prove that, if public-key cryptography is possible at all, then all protocol problems are (automatically!) solvable if more than half the players are honest.
							</p>
						</section>
					</section>
					<section id="sec7">
						<h2>Recent Developments</h2>
						<p class="rp_original">
							Recently, Haver and Micali found<span data-broken-link=true></span> a <span class="definable" data-define="Turing machine game">Tm-game</span> solver that is algorithmically much simpler (for instance it does not use Barrington's <span class="definable" data-define="straight-line program">straight-line programs</span> but more difficult to prove correct. Also, Goldreich and Vainish found a simpler solution based on a specific assumption, the computational difficulty of <span class="definable" data-define="quadratic residue">quadratic residuosity</span>.
						</p>
					</section>
					<section id="sec8">
						<h2>Acknowledgements</h2>
						<p class="rp_original">
							We are very grateful to Shimon Even, Dick Karp, Mike Merritt, Albert Meyer, Yoram Moses for having doubted the generality of some of our intermediate solutions and having encouraged us to reach the right level of generality. In particular, Albert Meyer contributed the beautiful notion of a Turing-machine game, and Dick Karp steered us toward games with incomplete information as the best avenue to our completeness theorem for protocols. 
						</p>
						<p class="rp_original">
							We also would like to thank Benny Chor, Mike Fischer and Shafi Goldwasser for helpful discussions concerning the issues of this paper. 
						</p>
						<p class="rp_annotation">
							I would be loathe to take away the credit from these people in the HTML version!
						</p>
					</section>
				</div>
            </div>
			<div class="main_toplevel main_section main_color7">
                <h1>Footnotes</h1>
				
                <ol id="footnotes">
                    <li id="footnote1">
						<a href="#fref1">[^]</a> For instance, there may be only one communication tape. In this case, digital signatures can be used to authenticate the sender. In case that not all machines may read all communication tapes, Byzantine agreement can be used to simulate the fact that all processors agree on what message machine \(i\) has sent to machine \(j\) at time \(t\). The common clock may be replaced by local clocks that don't drift "too much". The quite tight synchrony of the message delivery can be replaced by a feasible upper bound on the time it takes a message to be delivered, and so on.
					</li>
                </ol>
            </div>
            <div class="main_toplevel main_section main_color8">
                <h1>References</h1>
                <ol id="referencelist">
                    <li id="citation1">
						S. Goldwasser, and S. Micali, <i>Probabilistic Encryption</i>, JCSS Vol. 28, No. 2, April 1984.
						<p>
							An earlier version (containing other results) was titled <i>Probabilistic Encryption and How to Play Mental Poker Hiding All Partial Information</i>
						</p>
					</li>
					<li id="citation2">
						A. Yao, <i>Theory and Application of Trapdoor Functions</i>, Proc. of 23rd FOCS, IEEE, Nov., 1982, pp. 80-91.
					</li>
					<li id="citation3">
						S. Goldwasser, S. Micali and C. Rackoff, <i>The Knowledge Complexity of Interactive Proof-Systems</i>, To appear SIAM J. on Computing (manuscript available from authors).
						<p>
							Earlier version in Proc. 17th Annual ACM Symp. on Theory of Computing, pp 291-304.
						</p>
					</li>
					<li id="citation4">
						J. Halpern and M.O. Rabin, <i>A Logic to reason about likehood</i>, Proc of 15th STOC, 1983.
					</li>
					<li id="citation5">
						M. Fischer, S. Micali, C. Rackoff, and D. Witenberg, <i>A Secure Protocol for the Oblivious Transfer</i>, in preparation 1986.
					</li>
					<li id="citation6">
						M. Blum, <i>Coin Flipping by Telephone</i>, IEEE COMPCON 1982, pp. 133-137.
					</li>
					<li id="citation7">
						S. Even, O. Goldreich, and A. Lempel, <i>A Randomized Protocol for Signing Contracts</i>, CACN, vol. 28, No. 6, 1985, pp. 637-647.
					</li>
					<li id="citation8">
						S. Goldwasser, S. Micali, and R. Rivest, <i>A Digital Signature Sceme Secure Against Adaptive, Chosen Cyphertext Attack</i>. To appear in SAIM J. on Computing (available from authors)
						<p>Earlier version, titled "<i>A Paradoxical Solution to the Signature Problem</i>", in Proc. 25th FOCS, 1984, pp.441-448.
					</li>
					<li id="citation9">
						M. Blum and S. Micali, <i>How to Generate Sequences of Cryptographicall String Pseudo-Random Bits</i>, SIAM J. on Computing, Vol. 13, Nov 1984, pp. 850-864
					</li>
					<li id="citation10">
						R. Boppana and R. Hirschfeld, <i>Pseudo-Random Generators and Complexity Classes</i>, To appear in Randomness and Computation, 5th volume of Advances in Computing Research, ed. S. Micali
					</li>
					<li id="citation11">
						L. Leonid, <i>One-Way Functions and Pseudo-Random Generators</i>, Proc. 17th STOC, 1985, pp. 363-365.
					</li>
					<li id="citation12">
						A. Yao, <i>How to Generate and Exchange Secrets</i>, Proc. 27th STOC, 1986, pp. 162-167
					</li>
					<li id="citation13">
						D. Barrington, <i>Bounded-Width Branching Programs Recognize Exactly THose Languages in NC<sup>1</sup></i>, Proc. 18th STOC, 1986 pp 1-5
					</li>
					<li id="citation14">
						B. Chor, S. Goldwasser, S. Micali, and B. Awerbuch, <i>Verifiable Secret Sharing and Achieving Simultaneity in the Presence of Faults</i>, Proc. 26th FOCS, 1985, pp. 383-395.
					</li>
					<li id="citation15">
						O. Goldreich, S. Micali, and A. Wigderson, <i>Proofs that Yield Nothing but their Validity and a Methodology of Cryptographic Design</i>, Proc. of FOCS 1986.
					</li>
					<li id="citation16">
						B. Chor and O Goldreich, <i>RSA/Rabin Bits are 1/2+1/poly(logN) Secure</i>, To appear SIAM J. on Computing.
						<p>Earlier version in PROC. FOCS 1984, pp. 449-463</p>
					</li>
                </ol>
            </div>
            <div class="main_toplevel main_section main_color9">
                <div class="rp_problems">
                    <p><a href="mailto:multipartycomputationorg+81@gmail.com">Problem with this page?</a></p>
                </div>
                <p>Copyright &copy; Nicolas Schank 2014, Brown University</p>
            </div>
        </div>
    </body>
</html>